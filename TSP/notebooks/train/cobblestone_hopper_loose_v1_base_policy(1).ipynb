{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Installation (work on colab)"],"metadata":{"id":"4ZNZOeJhUvi5"}},{"cell_type":"code","source":["!apt-get install -y \\\n","    libgl1-mesa-dev \\\n","    libgl1-mesa-glx \\\n","    libglew-dev \\\n","    libosmesa6-dev \\\n","    software-properties-common\n","\n","!apt-get install -y patchelf\n","!pip install gym\n","\n","!pip install free-mujoco-py\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install imageio==2.4.1\n","!pip install -U colabgymrender\n","!pip install mujoco"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JKvWsG4iUn2K","executionInfo":{"status":"ok","timestamp":1701310050535,"user_tz":300,"elapsed":54038,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}},"outputId":"5cfc4ba6-0aeb-404e-da9b-8b0ca867c1f5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","software-properties-common is already the newest version (0.99.22.8).\n","The following additional packages will be installed:\n","  libegl-dev libgl-dev libgles-dev libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev\n","  libglvnd-dev libglx-dev libopengl-dev libosmesa6\n","The following NEW packages will be installed:\n","  libegl-dev libgl-dev libgl1-mesa-dev libgl1-mesa-glx libgles-dev libgles1 libglew-dev\n","  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev libopengl-dev libosmesa6\n","  libosmesa6-dev\n","0 upgraded, 15 newly installed, 0 to remove and 15 not upgraded.\n","Need to get 3,952 kB of archives.\n","After this operation, 18.7 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5,584 B]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.0.4-0ubuntu1~22.04.1 [6,510 B]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libglew-dev amd64 2.2.0-4 [287 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6 amd64 23.0.4-0ubuntu1~22.04.1 [3,054 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6-dev amd64 23.0.4-0ubuntu1~22.04.1 [8,986 B]\n","Fetched 3,952 kB in 0s (10.1 MB/s)\n","Selecting previously unselected package libglx-dev:amd64.\n","(Reading database ... 120882 files and directories currently installed.)\n","Preparing to unpack .../00-libglx-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglx-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl-dev:amd64.\n","Preparing to unpack .../01-libgl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libegl-dev:amd64.\n","Preparing to unpack .../02-libegl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libegl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-glx:amd64.\n","Preparing to unpack .../03-libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libgles1:amd64.\n","Preparing to unpack .../04-libgles1_1.4.0-1_amd64.deb ...\n","Unpacking libgles1:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles-dev:amd64.\n","Preparing to unpack .../05-libgles-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgles-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-core-dev:amd64.\n","Preparing to unpack .../06-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libopengl-dev:amd64.\n","Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-dev:amd64.\n","Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-dev:amd64.\n","Preparing to unpack .../09-libgl1-mesa-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglu1-mesa-dev:amd64.\n","Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglew-dev:amd64.\n","Preparing to unpack .../12-libglew-dev_2.2.0-4_amd64.deb ...\n","Unpacking libglew-dev:amd64 (2.2.0-4) ...\n","Selecting previously unselected package libosmesa6:amd64.\n","Preparing to unpack .../13-libosmesa6_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libosmesa6:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libosmesa6-dev:amd64.\n","Preparing to unpack .../14-libosmesa6-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libosmesa6-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Setting up libgles1:amd64 (1.4.0-1) ...\n","Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up libglx-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up libopengl-dev:amd64 (1.4.0-1) ...\n","Setting up libgl-dev:amd64 (1.4.0-1) ...\n","Setting up libosmesa6:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up libegl-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Setting up libosmesa6-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up libgles-dev:amd64 (1.4.0-1) ...\n","Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n","Setting up libglew-dev:amd64 (2.2.0-4) ...\n","Setting up libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  patchelf\n","0 upgraded, 1 newly installed, 0 to remove and 15 not upgraded.\n","Need to get 72.1 kB of archives.\n","After this operation, 186 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 patchelf amd64 0.14.3-1 [72.1 kB]\n","Fetched 72.1 kB in 0s (481 kB/s)\n","Selecting previously unselected package patchelf.\n","(Reading database ... 121022 files and directories currently installed.)\n","Preparing to unpack .../patchelf_0.14.3-1_amd64.deb ...\n","Unpacking patchelf (0.14.3-1) ...\n","Setting up patchelf (0.14.3-1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n","Collecting free-mujoco-py\n","  Downloading free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Cython<0.30.0,>=0.29.24 (from free-mujoco-py)\n","  Downloading Cython-0.29.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.16.0)\n","Collecting fasteners==0.15 (from free-mujoco-py)\n","  Downloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n","Collecting glfw<2.0.0,>=1.4.0 (from free-mujoco-py)\n","  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (2.31.6)\n","Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.23.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fasteners==0.15->free-mujoco-py) (1.16.0)\n","Collecting monotonic>=0.1 (from fasteners==0.15->free-mujoco-py)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n","Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (9.4.0)\n","Installing collected packages: monotonic, glfw, fasteners, Cython, free-mujoco-py\n","  Attempting uninstall: Cython\n","    Found existing installation: Cython 3.0.5\n","    Uninstalling Cython-3.0.5:\n","      Successfully uninstalled Cython-3.0.5\n","Successfully installed Cython-0.29.36 fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 monotonic-1.6\n","Collecting imageio==2.4.1\n","  Downloading imageio-2.4.1.tar.gz (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.1) (1.23.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.1) (9.4.0)\n","Building wheels for collected packages: imageio\n","  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imageio: filename=imageio-2.4.1-py3-none-any.whl size=3303885 sha256=1b2e88bb74ff4e764bac500b05f8547b3ff33603d6432c66a5b285321ea30272\n","  Stored in directory: /root/.cache/pip/wheels/96/5d/ce/bdbdb04744dac03906336eb0d01ff1e222061d3419c55c55f9\n","Successfully built imageio\n","Installing collected packages: imageio\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.31.6\n","    Uninstalling imageio-2.31.6:\n","      Successfully uninstalled imageio-2.31.6\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","free-mujoco-py 2.1.6 requires imageio<3.0.0,>=2.9.0, but you have imageio 2.4.1 which is incompatible.\n","moviepy 1.0.3 requires imageio<3.0,>=2.5; python_version >= \"3.4\", but you have imageio 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed imageio-2.4.1\n","Collecting colabgymrender\n","  Downloading colabgymrender-1.1.0.tar.gz (3.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from colabgymrender) (1.0.3)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (4.4.2)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (4.66.1)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (2.31.0)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (0.1.10)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (1.23.5)\n","Collecting imageio<3.0,>=2.5 (from moviepy->colabgymrender)\n","  Downloading imageio-2.33.0-py3-none-any.whl (313 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.3/313.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (0.4.9)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy->colabgymrender) (9.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy->colabgymrender) (67.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (2023.7.22)\n","Building wheels for collected packages: colabgymrender\n","  Building wheel for colabgymrender (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for colabgymrender: filename=colabgymrender-1.1.0-py3-none-any.whl size=3115 sha256=2f55da53333edcb7dd6ba2b95a753bc1958177326809d5f8c9ccbded31f0c4b3\n","  Stored in directory: /root/.cache/pip/wheels/13/62/63/7b3acfb684dd3d665d7fc1d213427b136205a222389767e295\n","Successfully built colabgymrender\n","Installing collected packages: imageio, colabgymrender\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.4.1\n","    Uninstalling imageio-2.4.1:\n","      Successfully uninstalled imageio-2.4.1\n","Successfully installed colabgymrender-1.1.0 imageio-2.33.0\n","Collecting mujoco\n","  Downloading mujoco-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.4.0)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.5.2)\n","Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.23.5)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco) (3.1.7)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (2023.6.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (6.1.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (4.5.0)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (3.17.0)\n","Installing collected packages: mujoco\n","Successfully installed mujoco-3.0.1\n"]}]},{"cell_type":"code","source":["!python --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BsNICgpbv6vT","executionInfo":{"status":"ok","timestamp":1701310050773,"user_tz":300,"elapsed":278,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}},"outputId":"546bb531-5cac-4e1e-88f0-a90442197607"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZhFKM-rDS0yE","executionInfo":{"status":"ok","timestamp":1701310062431,"user_tz":300,"elapsed":11666,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}},"outputId":"cb9b7551-f151-4fbb-8fa6-62d7a17f5e9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pfrl\n","  Downloading pfrl-0.4.0.tar.gz (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/112.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pfrl) (2.1.0+cu118)\n","Requirement already satisfied: gym>=0.9.7 in /usr/local/lib/python3.10/dist-packages (from pfrl) (0.25.2)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from pfrl) (1.23.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pfrl) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from pfrl) (3.13.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.9.7->pfrl) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.9.7->pfrl) (0.0.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->pfrl) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->pfrl) (1.3.0)\n","Building wheels for collected packages: pfrl\n","  Building wheel for pfrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pfrl: filename=pfrl-0.4.0-py3-none-any.whl size=155460 sha256=ebc80888996813720316b5f3f3bd6c3c8f2f0fac7282c584e5c0ed765de89bc9\n","  Stored in directory: /root/.cache/pip/wheels/22/4a/0f/a87cd1ae925086eb3a1b8759f620fcf48e47939fb082946c3b\n","Successfully built pfrl\n","Installing collected packages: pfrl\n","Successfully installed pfrl-0.4.0\n"]}],"source":["!pip install pfrl"]},{"cell_type":"code","source":["!pip install gymnasium"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofM7lHvOUVDc","executionInfo":{"status":"ok","timestamp":1701310076223,"user_tz":300,"elapsed":12296,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}},"outputId":"de422544-b9ff-48e8-eabc-d443cf85639b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"]}]},{"cell_type":"code","source":["import functools\n","import numpy as np\n","\n","import gymnasium as gym\n","import gym.wrappers\n","\n","import pfrl"],"metadata":{"id":"Oh_oFqaITeKZ","executionInfo":{"status":"ok","timestamp":1701310082540,"user_tz":300,"elapsed":6321,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["pip freeze > requirements.txt"],"metadata":{"id":"5hMuefzEwZFU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = \"cuda:0\""],"metadata":{"id":"iUBFcB_3VDK8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"joyBiJ7Sedjh","executionInfo":{"status":"ok","timestamp":1701310099396,"user_tz":300,"elapsed":16864,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}},"outputId":"c3af95fe-17f1-4f1b-ab53-ad6de9fea68d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/pexpect/popen_spawn.py:60: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n","  self._read_thread.setDaemon(True)\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4bA0agQ_APu","executionInfo":{"status":"ok","timestamp":1701226299356,"user_tz":300,"elapsed":7,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"d5d77ca3-c0ec-41b6-d2f7-9fca54093112"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OH5x_jG1_EjT","executionInfo":{"status":"ok","timestamp":1701226347353,"user_tz":300,"elapsed":218,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"8339dc04-2a84-4de9-86d1-56bc16eebf9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mbin\u001b[0m/    \u001b[01;36mcuda\u001b[0m@     \u001b[01;34mcuda-11.8\u001b[0m/  \u001b[01;34mgames\u001b[0m/               \u001b[01;34minclude\u001b[0m/  \u001b[01;34mlib64\u001b[0m/      \u001b[01;36mman\u001b[0m@   \u001b[01;34mshare\u001b[0m/\n","\u001b[01;34mcolab\u001b[0m/  \u001b[01;36mcuda-11\u001b[0m@  \u001b[01;34metc\u001b[0m/        \u001b[01;34m_gcs_config_ops.so\u001b[0m/  \u001b[01;34mlib\u001b[0m/      \u001b[01;34mlicensing\u001b[0m/  \u001b[01;34msbin\u001b[0m/  \u001b[01;34msrc\u001b[0m/\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rRe72RDV7pZ6","executionInfo":{"status":"ok","timestamp":1701226316235,"user_tz":300,"elapsed":231,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"b91a8118-05f0-4935-9b42-0702c5700ea9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot create regular file '/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/assets/envs/': Not a directory\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/codes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"62iQsL5jeqca","executionInfo":{"status":"ok","timestamp":1701310107985,"user_tz":300,"elapsed":371,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}},"outputId":"8438ca4d-8289-4fc7-f6b7-10f80178bc33"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/codes\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"V0gMDExYyHLW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize environment"],"metadata":{"id":"7aEG-BTXXDm9"}},{"cell_type":"code","source":["env_name = \"Hopper-v3\"\n","num_envs = 5\n","seed = 42\n","\n","monitor = False\n","render = False\n","\n","process_seeds = np.arange(num_envs) + seed * num_envs"],"metadata":{"id":"59FrC2_ziiqL","executionInfo":{"status":"ok","timestamp":1701310110218,"user_tz":300,"elapsed":253,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def make_env(process_idx, test):\n","    env = gym.make(env_name, xml_file=\"envs/cobblestone_hopper_loose_v1.xml\")\n","    # Unwrap TimiLimit wrapper\n","    assert isinstance(env, gym.wrappers.TimeLimit)\n","    env = env.env\n","    # Use different random seeds for train and test envs\n","    process_seed = int(process_seeds[process_idx])\n","    env_seed = 2**32 - 1 - process_seed if test else process_seed\n","    env.seed(env_seed)\n","    # Cast observations to float32 because our model uses float32\n","    env = pfrl.wrappers.CastObservationToFloat32(env)\n","    # Normalize action space to [-1, 1]^n\n","    env = pfrl.wrappers.NormalizeActionSpace(env)\n","    if monitor:\n","        env = gym.wrappers.Monitor(env, \"monitor\")\n","    if render:\n","        env = pfrl.wrappers.Render(env)\n","    return env"],"metadata":{"id":"sIrESw-ZUcLs","executionInfo":{"status":"ok","timestamp":1701310112403,"user_tz":300,"elapsed":159,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def make_batch_env(test):\n","    return pfrl.envs.MultiprocessVectorEnv(\n","        [\n","            functools.partial(make_env, idx, test)\n","            for idx, env in enumerate(range(num_envs))\n","        ]\n","    )"],"metadata":{"id":"jR-KaV8_c5HA","executionInfo":{"status":"ok","timestamp":1701310114114,"user_tz":300,"elapsed":155,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["env = make_env(0, test=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pB9V9m9IXAsR","executionInfo":{"status":"ok","timestamp":1701310288698,"user_tz":300,"elapsed":162,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}},"outputId":"93d08c9a-e710-4168-c0c1-1f3846b46b7d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["env.observation_space"],"metadata":{"id":"eTojqsUDXNpe","executionInfo":{"status":"ok","timestamp":1701310299543,"user_tz":300,"elapsed":146,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a3b58a3-980c-4df1-f77c-efed4a47d0db"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(-inf, inf, (11,), float64)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["env.action_space"],"metadata":{"id":"V1db9VKdXQDL","executionInfo":{"status":"ok","timestamp":1701310300864,"user_tz":300,"elapsed":283,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"19a56eb2-7ca1-482f-9a1b-df37705b5240"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(-1.0, 1.0, (3,), float32)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["obs_size = env.observation_space.shape[0]\n","action_size = env.action_space.shape[0]\n","timestep_limit = env.spec.max_episode_steps"],"metadata":{"id":"BNYgd4K-X1RG","executionInfo":{"status":"ok","timestamp":1701310302715,"user_tz":300,"elapsed":183,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tTDCl9klyIle"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## SAC Config (with PFRL)\n","\n","sample codes and parameters in [this file](https://github.com/pfnet/pfrl/blob/master/examples/mujoco/reproduction/soft_actor_critic/train_soft_actor_critic.py)."],"metadata":{"id":"vw5gYvbBVfEj"}},{"cell_type":"code","source":["import torch\n","from torch import distributions, nn\n","\n","from pfrl import experiments, replay_buffers, utils\n","from pfrl.nn.lmbda import Lambda"],"metadata":{"id":"CrghSSqgVmC6","executionInfo":{"status":"ok","timestamp":1701310304423,"user_tz":300,"elapsed":276,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# default params\n","policy_output_scale = 1.0\n","\n","policy_hidden_dim = 256\n","policy_lr = 3e-4\n","\n","q_func_hidden_dim = 256\n","q_func_lr = 3e-4\n","\n","temperature_optimizer_lr = 3e-4\n","\n","buffer_size = 10 ** 6\n","replay_start_size = 10 ** 4\n","batch_size = 256\n","gamma = 0.99\n","\n","# steps = 10 ** 6\n","steps = 6 * 10 ** 5"],"metadata":{"id":"YSXLsGBMYMk2","executionInfo":{"status":"ok","timestamp":1701310307498,"user_tz":300,"elapsed":151,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def burnin_action_func():\n","    \"\"\"Select random actions until model is updated one or more times.\"\"\"\n","    return np.random.uniform(env.action_space.low, env.action_space.high).astype(np.float32)"],"metadata":{"id":"3un8LjlWaQ4B","executionInfo":{"status":"ok","timestamp":1701310309755,"user_tz":300,"elapsed":493,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def squashed_diagonal_gaussian_head(x):\n","    assert x.shape[-1] == action_size * 2\n","    mean, log_scale = torch.chunk(x, 2, dim=1)\n","    log_scale = torch.clamp(log_scale, -20.0, 2.0)\n","    var = torch.exp(log_scale * 2)\n","    base_distribution = distributions.Independent(\n","        distributions.Normal(loc=mean, scale=torch.sqrt(var)), 1\n","    )\n","    # cache_size=1 is required for numerical stability\n","    return distributions.transformed_distribution.TransformedDistribution(\n","        base_distribution, [distributions.transforms.TanhTransform(cache_size=1)]\n","    )"],"metadata":{"id":"BTBY4AGWVX4q","executionInfo":{"status":"ok","timestamp":1701310311119,"user_tz":300,"elapsed":244,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def make_policy_with_optimizer():\n","    policy = nn.Sequential(\n","        nn.Linear(obs_size, policy_hidden_dim),\n","        nn.ReLU(),\n","        nn.Linear(policy_hidden_dim, policy_hidden_dim),\n","        nn.ReLU(),\n","        nn.Linear(policy_hidden_dim, action_size * 2),\n","        Lambda(squashed_diagonal_gaussian_head),\n","    )\n","    torch.nn.init.xavier_uniform_(policy[0].weight)\n","    torch.nn.init.xavier_uniform_(policy[2].weight)\n","    torch.nn.init.xavier_uniform_(policy[4].weight, gain=policy_output_scale)\n","    policy_optimizer = torch.optim.Adam(policy.parameters(), lr=policy_lr)\n","    return policy, policy_optimizer"],"metadata":{"id":"_schTPU8X9CG","executionInfo":{"status":"ok","timestamp":1701310312818,"user_tz":300,"elapsed":177,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def make_q_func_with_optimizer():\n","    q_func = nn.Sequential(\n","        pfrl.nn.ConcatObsAndAction(),\n","        nn.Linear(obs_size + action_size, q_func_hidden_dim),\n","        nn.ReLU(),\n","        nn.Linear(q_func_hidden_dim, q_func_hidden_dim),\n","        nn.ReLU(),\n","        nn.Linear(q_func_hidden_dim, 1),\n","    )\n","    torch.nn.init.xavier_uniform_(q_func[1].weight)\n","    torch.nn.init.xavier_uniform_(q_func[3].weight)\n","    torch.nn.init.xavier_uniform_(q_func[5].weight)\n","    q_func_optimizer = torch.optim.Adam(q_func.parameters(), lr=q_func_lr)\n","    return q_func, q_func_optimizer"],"metadata":{"id":"YfRiZLWzY_C6","executionInfo":{"status":"ok","timestamp":1701310313934,"user_tz":300,"elapsed":5,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["policy, policy_optimizer = make_policy_with_optimizer()\n","q_func1, q_func1_optimizer = make_q_func_with_optimizer()\n","q_func2, q_func2_optimizer = make_q_func_with_optimizer()"],"metadata":{"id":"ODiMpSZ8Ycao","executionInfo":{"status":"ok","timestamp":1701310316920,"user_tz":300,"elapsed":1251,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["rbuffer = replay_buffers.ReplayBuffer(buffer_size)"],"metadata":{"id":"UPdN1jnDZ92X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701310317473,"user_tz":300,"elapsed":163,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}},"outputId":"7acf4d0d-0548-4eed-c29e-cc9db52ce692"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"C5oTxaLoxjjA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train SAC policy online"],"metadata":{"id":"gY1IkWhJdMK8"}},{"cell_type":"code","source":["agent = pfrl.agents.SoftActorCritic(\n","    policy,\n","    q_func1,\n","    q_func2,\n","    policy_optimizer,\n","    q_func1_optimizer,\n","    q_func2_optimizer,\n","    rbuffer,\n","    gamma=gamma,\n","    replay_start_size=replay_start_size,\n","    gpu=0 if torch.cuda.is_available() else -1,\n","    minibatch_size=batch_size,\n","    burnin_action_func=burnin_action_func,\n","    entropy_target=-action_size,\n","    temperature_optimizer_lr=temperature_optimizer_lr,\n",")"],"metadata":{"id":"Ps-9PQuLaKPT","executionInfo":{"status":"ok","timestamp":1701310324608,"user_tz":300,"elapsed":5813,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["experiments.train_agent_batch_with_evaluation(\n","    agent=agent,\n","    env=make_batch_env(test=False),\n","    eval_env=make_batch_env(test=False),\n","    outdir=\"logs/cobblestone_loose_v1\",\n","    steps=steps,\n","    eval_n_steps=None,\n","    eval_n_episodes=10,\n","    eval_interval=5 * 10 ** 3,\n","    log_interval=10 ** 3,\n","    max_episode_len=timestep_limit,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O1KQLV3SbeEX","outputId":"4b813d2f-5a34-405f-c727-ec6d38458ef9","executionInfo":{"status":"ok","timestamp":1701321963122,"user_tz":300,"elapsed":11321055,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:1000 episode:46 last_R: 21.61544016826583 average_R:18.30140508404171\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:2000 episode:88 last_R: 18.055701895136092 average_R:19.43727467434379\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:3000 episode:126 last_R: 83.85675010038703 average_R:22.370290212376467\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:4000 episode:166 last_R: 7.6749768959099125 average_R:24.132277618990408\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:5000 episode:209 last_R: 19.061230674298933 average_R:23.420252466725938\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 139 R: 111.43986597176354\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 142 R: 115.04206916477757\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 139 R: 112.14947692580049\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 139 R: 112.2633052986955\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 135 R: 108.92517490768611\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 146 R: 118.70527633398099\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 136 R: 108.73233715132766\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 143 R: 116.39602818223823\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 136 R: 109.81751869634195\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 133 R: 106.4234543732358\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated -3.4028235e+38 -> 111.98945070058478\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:6000 episode:247 last_R: 18.644287199605696 average_R:23.672531472773574\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:7000 episode:295 last_R: 15.95195932699795 average_R:21.848672004035425\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:8000 episode:337 last_R: 13.419559079568929 average_R:20.999611030472302\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:9000 episode:377 last_R: 19.386516045600164 average_R:21.31782133421888\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","/usr/local/lib/python3.10/dist-packages/pfrl/replay_buffer.py:180: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n","  \"action\": torch.as_tensor(\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:10000 episode:423 last_R: 13.736996528530144 average_R:22.21480184866566\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 0.30996758), ('average_q2', -0.089171775), ('average_q_func1_loss', 2.8815677165985107), ('average_q_func2_loss', 3.7019572257995605), ('n_updates', 1), ('average_entropy', 1.7611058), ('temperature', 0.9997000694274902)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 206 R: 236.6059015814184\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 134 R: 107.48308786985181\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 129 R: 102.93282970877395\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 136 R: 109.39722587437447\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 142 R: 115.61037258250417\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 158 R: 131.18809272215742\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 127 R: 101.3467985138536\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 187 R: 160.4443327359642\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 223 R: 195.99376201445023\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 160 R: 133.00914791776574\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 111.98945070058478 -> 139.40115515211141\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:11000 episode:443 last_R: 95.37078073175377 average_R:24.380724404974803\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 11.950818), ('average_q2', 12.040433), ('average_q_func1_loss', 0.7402474159002304), ('average_q_func2_loss', 0.7338401889801025), ('n_updates', 1001), ('average_entropy', 1.7101979), ('temperature', 0.7564159631729126)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:12000 episode:456 last_R: 178.5835204769372 average_R:32.087401490616756\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 20.5159), ('average_q2', 20.518087), ('average_q_func1_loss', 1.4738838496804236), ('average_q_func2_loss', 1.4662169474363327), ('n_updates', 2001), ('average_entropy', 1.3283871), ('temperature', 0.5940183997154236)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:13000 episode:466 last_R: 208.42338417031834 average_R:49.68224283252173\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 29.492973), ('average_q2', 29.543863), ('average_q_func1_loss', 2.282000270485878), ('average_q_func2_loss', 2.2462023693323134), ('n_updates', 3001), ('average_entropy', 0.6528397), ('temperature', 0.4770852327346802)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:14000 episode:476 last_R: 194.55617694157908 average_R:67.18632368153027\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 37.516884), ('average_q2', 37.62425), ('average_q_func1_loss', 3.2534891802072523), ('average_q_func2_loss', 3.1942887538671494), ('n_updates', 4001), ('average_entropy', 0.4763377), ('temperature', 0.3870944678783417)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:15000 episode:485 last_R: 202.46528088652772 average_R:82.71537333137097\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 45.79043), ('average_q2', 45.68711), ('average_q_func1_loss', 4.45368060708046), ('average_q_func2_loss', 4.331842510700226), ('n_updates', 5001), ('average_entropy', 0.02917948), ('temperature', 0.31583425402641296)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 83 R: 161.94557132423756\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 78 R: 143.2448283428271\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 82 R: 143.27475978366644\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 72 R: 119.19118716237553\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 82 R: 145.58087939576058\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 82 R: 156.96163044149546\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 83 R: 160.49105447168938\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 73 R: 122.71442904258973\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 91 R: 161.58698653255115\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 81 R: 150.2113365130656\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 139.40115515211141 -> 146.52026630102586\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:16000 episode:496 last_R: 210.27541627221316 average_R:101.18416278737888\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 51.701477), ('average_q2', 51.680237), ('average_q_func1_loss', 5.41669930934906), ('average_q_func2_loss', 5.196777287721634), ('n_updates', 6001), ('average_entropy', -0.16309124), ('temperature', 0.25755181908607483)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:17000 episode:506 last_R: 219.1552245795441 average_R:118.75325334329065\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 59.89442), ('average_q2', 60.055386), ('average_q_func1_loss', 6.650331883430481), ('average_q_func2_loss', 6.41116170167923), ('n_updates', 7001), ('average_entropy', -0.4485221), ('temperature', 0.21136067807674408)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:18000 episode:513 last_R: 185.83935836739516 average_R:131.2993535542679\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 64.50588), ('average_q2', 64.58111), ('average_q_func1_loss', 7.569387474060059), ('average_q_func2_loss', 7.181593440771103), ('n_updates', 8001), ('average_entropy', -0.7959983), ('temperature', 0.17524732649326324)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:19000 episode:522 last_R: 189.99531384473943 average_R:146.9386324031594\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 70.36247), ('average_q2', 70.38129), ('average_q_func1_loss', 7.210667552947998), ('average_q_func2_loss', 6.842566796541214), ('n_updates', 9001), ('average_entropy', -0.9325831), ('temperature', 0.14549320936203003)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:20000 episode:532 last_R: 203.8070397444707 average_R:163.1850583764971\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 75.33048), ('average_q2', 75.54939), ('average_q_func1_loss', 7.732079312801361), ('average_q_func2_loss', 7.249326014518738), ('n_updates', 10001), ('average_entropy', -1.1679319), ('temperature', 0.12129270285367966)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 106 R: 191.10397863180754\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 105 R: 190.10662180849582\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 106 R: 189.36992227059056\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 105 R: 191.85628470126915\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 106 R: 193.20781217356478\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 106 R: 189.93556853489736\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 106 R: 192.15277693140777\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 105 R: 187.85244473287327\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 106 R: 189.64897499252365\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 106 R: 188.62846440735274\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 146.52026630102586 -> 190.38628491847825\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:21000 episode:541 last_R: 195.59019105270698 average_R:177.49814755180768\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 78.041275), ('average_q2', 78.30141), ('average_q_func1_loss', 8.168129293918609), ('average_q_func2_loss', 7.8735221576690675), ('n_updates', 11001), ('average_entropy', -1.7558419), ('temperature', 0.1021898090839386)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:22000 episode:551 last_R: 191.94500519330623 average_R:191.44603488402907\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 82.56584), ('average_q2', 82.56845), ('average_q_func1_loss', 8.741115579605102), ('average_q_func2_loss', 8.15385454416275), ('n_updates', 12001), ('average_entropy', -2.4372585), ('temperature', 0.08765137940645218)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:23000 episode:560 last_R: 182.64922624193233 average_R:195.10567050035561\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 85.783325), ('average_q2', 86.02444), ('average_q_func1_loss', 9.784059146642685), ('average_q_func2_loss', 9.415054602622986), ('n_updates', 13001), ('average_entropy', -2.6031864), ('temperature', 0.07731153070926666)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:24000 episode:567 last_R: 187.83509058764403 average_R:195.1211818037951\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 89.78244), ('average_q2', 89.68281), ('average_q_func1_loss', 8.694700927734376), ('average_q_func2_loss', 8.328845355510712), ('n_updates', 14001), ('average_entropy', -2.7821102), ('temperature', 0.07194586098194122)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:25000 episode:577 last_R: 202.0673548769705 average_R:196.5616562002499\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 93.844536), ('average_q2', 93.646065), ('average_q_func1_loss', 9.150508782863618), ('average_q_func2_loss', 8.399624762535096), ('n_updates', 15001), ('average_entropy', -2.518567), ('temperature', 0.06502047926187515)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 115 R: 203.283174757742\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 125 R: 223.76209952624825\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 157 R: 238.87009492653124\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 157 R: 243.69273238301218\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 115 R: 210.18706710581185\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 128 R: 227.91884152316575\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 118 R: 211.67058122578524\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 116 R: 207.82447293350205\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 146 R: 229.72699675527923\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 144 R: 228.7803383357625\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 190.38628491847825 -> 222.57163994728404\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:26000 episode:585 last_R: 238.353529686342 average_R:198.50210871483398\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 94.35911), ('average_q2', 94.21739), ('average_q_func1_loss', 9.722138969898223), ('average_q_func2_loss', 9.005542976856232), ('n_updates', 16001), ('average_entropy', -2.6194215), ('temperature', 0.057956762611866)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:27000 episode:592 last_R: 237.60358389142885 average_R:200.99233886472956\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 97.733315), ('average_q2', 97.8005), ('average_q_func1_loss', 10.180899140834809), ('average_q_func2_loss', 9.20589941740036), ('n_updates', 17001), ('average_entropy', -2.9475904), ('temperature', 0.05267307162284851)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:28000 episode:601 last_R: 310.8723870187982 average_R:204.10347135469954\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 99.6547), ('average_q2', 99.61525), ('average_q_func1_loss', 10.073988902568818), ('average_q_func2_loss', 8.95330654144287), ('n_updates', 18001), ('average_entropy', -2.687878), ('temperature', 0.04827600717544556)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:29000 episode:606 last_R: 313.02639773567046 average_R:207.10493722284727\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 101.66039), ('average_q2', 101.78171), ('average_q_func1_loss', 9.582239136695861), ('average_q_func2_loss', 8.511263811588288), ('n_updates', 19001), ('average_entropy', -2.8801804), ('temperature', 0.045182004570961)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:30000 episode:615 last_R: 240.96620573931497 average_R:209.79884498161638\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 101.18153), ('average_q2', 101.07422), ('average_q_func1_loss', 9.104095113277435), ('average_q_func2_loss', 7.992450640201569), ('n_updates', 20001), ('average_entropy', -3.379538), ('temperature', 0.04633897915482521)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 118 R: 220.88006838194383\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 117 R: 228.0892255025074\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 115 R: 222.73351873532192\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 117 R: 226.36036881120495\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 116 R: 224.3938671159486\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 116 R: 225.2890550478053\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 115 R: 224.2124844193634\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 115 R: 223.48086554215945\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 115 R: 222.8866338856517\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 117 R: 228.5674893932933\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 222.57163994728404 -> 224.68935768351997\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:31000 episode:621 last_R: 229.71610533184915 average_R:211.59399127534618\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 105.455795), ('average_q2', 105.36289), ('average_q_func1_loss', 8.931115317344666), ('average_q_func2_loss', 7.823133821487427), ('n_updates', 21001), ('average_entropy', -3.2120135), ('temperature', 0.050067875534296036)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:32000 episode:630 last_R: 192.48248813992168 average_R:213.32173275459715\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 101.2031), ('average_q2', 101.32304), ('average_q_func1_loss', 7.9891325378417966), ('average_q_func2_loss', 7.13802877664566), ('n_updates', 22001), ('average_entropy', -3.3089454), ('temperature', 0.055133454501628876)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:33000 episode:637 last_R: 221.72166295276705 average_R:214.83625656415515\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 106.673065), ('average_q2', 106.71614), ('average_q_func1_loss', 8.528020234107972), ('average_q_func2_loss', 7.708078726530075), ('n_updates', 23001), ('average_entropy', -3.262671), ('temperature', 0.053831323981285095)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:34000 episode:645 last_R: 248.35531009097497 average_R:216.95213415347126\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 105.959564), ('average_q2', 105.83599), ('average_q_func1_loss', 8.513391089439391), ('average_q_func2_loss', 7.767382524013519), ('n_updates', 24001), ('average_entropy', -2.7324712), ('temperature', 0.050522807985544205)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:35000 episode:654 last_R: 141.7627609299582 average_R:218.74283010457327\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 105.11449), ('average_q2', 104.95011), ('average_q_func1_loss', 7.569844057559967), ('average_q_func2_loss', 6.4619205057621), ('n_updates', 25001), ('average_entropy', -3.4582424), ('temperature', 0.051744040101766586)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 82 R: 147.9398498312066\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 106 R: 230.41206290153315\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 73 R: 117.34961392206263\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 83 R: 153.9358613340645\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 85 R: 161.45763365388743\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 87 R: 173.8117524546533\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 74 R: 118.6774852142685\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 86 R: 162.2088706369695\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 86 R: 170.6795753406217\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 85 R: 156.9290493467097\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:36000 episode:665 last_R: 120.58289856916541 average_R:216.0572422921406\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 107.09356), ('average_q2', 107.234184), ('average_q_func1_loss', 8.295664668083191), ('average_q_func2_loss', 7.2488175082206725), ('n_updates', 26001), ('average_entropy', -3.071557), ('temperature', 0.050711508840322495)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:37000 episode:675 last_R: 231.5329328267083 average_R:211.5720809614303\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 107.867325), ('average_q2', 107.851906), ('average_q_func1_loss', 7.194156277179718), ('average_q_func2_loss', 6.4552438974380495), ('n_updates', 27001), ('average_entropy', -2.9703696), ('temperature', 0.04969574138522148)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:38000 episode:686 last_R: 202.55085168763668 average_R:206.7649613633365\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 107.02693), ('average_q2', 107.097984), ('average_q_func1_loss', 7.383582775592804), ('average_q_func2_loss', 6.834942271709442), ('n_updates', 28001), ('average_entropy', -3.0238214), ('temperature', 0.05026543140411377)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:39000 episode:694 last_R: 218.08516764244754 average_R:206.30677683577574\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 104.67502), ('average_q2', 104.70338), ('average_q_func1_loss', 6.902197365760803), ('average_q_func2_loss', 6.393397004604339), ('n_updates', 29001), ('average_entropy', -2.9630988), ('temperature', 0.05231926962733269)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:40000 episode:703 last_R: 229.4211710521461 average_R:206.5977640184006\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 105.42448), ('average_q2', 105.11164), ('average_q_func1_loss', 7.542997193336487), ('average_q_func2_loss', 6.822030749320984), ('n_updates', 30001), ('average_entropy', -3.0915332), ('temperature', 0.05395682156085968)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 116 R: 191.8936430070185\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 117 R: 198.25302975219907\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 117 R: 195.9664433316167\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 115 R: 195.06223437760949\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 117 R: 198.81104751659805\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 117 R: 196.0776867499609\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 117 R: 198.35439132093526\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 115 R: 195.3984645922318\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 117 R: 197.382808539444\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 117 R: 201.09492982708508\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:41000 episode:711 last_R: 198.46190718538273 average_R:203.9458428427517\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 107.10645), ('average_q2', 107.05789), ('average_q_func1_loss', 7.080208764076233), ('average_q_func2_loss', 6.420946469306946), ('n_updates', 31001), ('average_entropy', -2.9525292), ('temperature', 0.05321407690644264)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:42000 episode:719 last_R: 201.75768449991503 average_R:201.4050733991999\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 106.927704), ('average_q2', 106.7205), ('average_q_func1_loss', 6.4371901679039), ('average_q_func2_loss', 5.8067664575576785), ('n_updates', 32001), ('average_entropy', -3.1268632), ('temperature', 0.052052535116672516)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:43000 episode:728 last_R: 210.37363107815824 average_R:200.54483898889706\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 106.679825), ('average_q2', 106.6275), ('average_q_func1_loss', 6.922405714988709), ('average_q_func2_loss', 6.2587336373329165), ('n_updates', 33001), ('average_entropy', -3.0669537), ('temperature', 0.05039823055267334)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:44000 episode:744 last_R: 25.1804035544014 average_R:179.1553648703026\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 106.4987), ('average_q2', 106.5369), ('average_q_func1_loss', 7.926003317832947), ('average_q_func2_loss', 7.094464726448059), ('n_updates', 34001), ('average_entropy', -3.0630605), ('temperature', 0.048018861562013626)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:45000 episode:755 last_R: 51.72368535303547 average_R:161.6215987775321\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 106.25495), ('average_q2', 106.24013), ('average_q_func1_loss', 6.741753259897232), ('average_q_func2_loss', 6.287002379894257), ('n_updates', 35001), ('average_entropy', -2.8682325), ('temperature', 0.044154804199934006)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 166 R: 268.4648492085156\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 205 R: 303.0489365955863\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 169 R: 268.0249546107992\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 211 R: 316.6764826690212\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 170 R: 275.5291163429482\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 175 R: 287.3250692503958\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 196 R: 278.05853087360117\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 208 R: 287.50514070011985\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 200 R: 292.9541874183086\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 174 R: 284.6523197696947\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 224.68935768351997 -> 286.22395874389906\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:46000 episode:759 last_R: 182.24911501894636 average_R:162.52173639518446\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 107.46936), ('average_q2', 107.420425), ('average_q_func1_loss', 5.954817950725555), ('average_q_func2_loss', 5.492640535831452), ('n_updates', 36001), ('average_entropy', -3.276886), ('temperature', 0.044190455228090286)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:47000 episode:765 last_R: 288.3147756562764 average_R:169.1126496627641\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 108.83225), ('average_q2', 108.96292), ('average_q_func1_loss', 6.942988171577453), ('average_q_func2_loss', 6.55619166135788), ('n_updates', 37001), ('average_entropy', -3.2877586), ('temperature', 0.04944910481572151)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:48000 episode:768 last_R: 375.30270103100264 average_R:174.45632188055382\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 107.08913), ('average_q2', 107.314), ('average_q_func1_loss', 8.528650062084198), ('average_q_func2_loss', 7.606833928823471), ('n_updates', 38001), ('average_entropy', -3.066937), ('temperature', 0.05365410074591637)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:49000 episode:773 last_R: 480.30812867776166 average_R:187.51658985043406\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 110.22443), ('average_q2', 110.05906), ('average_q_func1_loss', 7.408287527561188), ('average_q_func2_loss', 6.766769421100617), ('n_updates', 39001), ('average_entropy', -2.9633658), ('temperature', 0.05549319088459015)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:50000 episode:780 last_R: 251.8289269236482 average_R:193.87233881218773\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 108.88179), ('average_q2', 108.97459), ('average_q_func1_loss', 6.378646149635315), ('average_q_func2_loss', 5.651487646102905), ('n_updates', 40001), ('average_entropy', -3.1349432), ('temperature', 0.05296959728002548)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 208 R: 319.74057538607565\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 190 R: 288.2864276909848\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 188 R: 379.43153864129556\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 221 R: 447.2646088100048\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 188 R: 310.69346786973983\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 200 R: 299.9979781258199\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 203 R: 398.40019120170695\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 227 R: 451.4797430636612\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 204 R: 311.365498196062\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 200 R: 413.99808401420165\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 286.22395874389906 -> 362.0658112999552\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:51000 episode:785 last_R: 273.14725132990765 average_R:197.53195475085693\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 108.821785), ('average_q2', 108.74653), ('average_q_func1_loss', 6.918760069608688), ('average_q_func2_loss', 6.500675485134125), ('n_updates', 41001), ('average_entropy', -2.8889756), ('temperature', 0.052780602127313614)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:52000 episode:790 last_R: 275.14823211347056 average_R:200.3781056954682\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 108.89611), ('average_q2', 108.68754), ('average_q_func1_loss', 5.994344010353088), ('average_q_func2_loss', 5.661520354747772), ('n_updates', 42001), ('average_entropy', -2.7227817), ('temperature', 0.050545379519462585)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:53000 episode:794 last_R: 443.36939661823277 average_R:205.73782639810065\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 107.34454), ('average_q2', 107.573204), ('average_q_func1_loss', 6.129783147573471), ('average_q_func2_loss', 5.505154659748078), ('n_updates', 43001), ('average_entropy', -3.0239494), ('temperature', 0.053429532796144485)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:54000 episode:796 last_R: 540.246328912356 average_R:211.0910750151057\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 110.14914), ('average_q2', 110.256065), ('average_q_func1_loss', 6.527198874950409), ('average_q_func2_loss', 6.141062923669815), ('n_updates', 44001), ('average_entropy', -2.8992984), ('temperature', 0.052622999995946884)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:55000 episode:798 last_R: 664.7832015436649 average_R:219.396306417522\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 110.19309), ('average_q2', 110.05947), ('average_q_func1_loss', 5.637869732379913), ('average_q_func2_loss', 5.403436017036438), ('n_updates', 45001), ('average_entropy', -2.7802205), ('temperature', 0.05209958925843239)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 678 R: 842.0561949283895\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 697 R: 861.4888773434228\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 708 R: 872.6986453352117\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 681 R: 845.2754198754357\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 707 R: 871.7599639661256\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 682 R: 846.7756025562453\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 676 R: 840.4343477744994\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 682 R: 847.1749009164633\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 698 R: 862.8112144320577\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 679 R: 843.1533367842118\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 362.0658112999552 -> 853.3628503912063\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:56000 episode:798 last_R: 664.7832015436649 average_R:219.396306417522\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 110.49896), ('average_q2', 110.652565), ('average_q_func1_loss', 6.543223608732223), ('average_q_func2_loss', 6.178942718505859), ('n_updates', 46001), ('average_entropy', -2.875797), ('temperature', 0.052444104105234146)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:57000 episode:798 last_R: 664.7832015436649 average_R:219.396306417522\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 109.60508), ('average_q2', 109.64981), ('average_q_func1_loss', 5.890645627975464), ('average_q_func2_loss', 5.214237525463104), ('n_updates', 47001), ('average_entropy', -3.1840918), ('temperature', 0.05441391095519066)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:58000 episode:799 last_R: 1034.0485218278739 average_R:227.4248707078389\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 110.82112), ('average_q2', 110.77535), ('average_q_func1_loss', 5.83123429775238), ('average_q_func2_loss', 5.107893126010895), ('n_updates', 48001), ('average_entropy', -3.0900626), ('temperature', 0.05021187663078308)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:59000 episode:801 last_R: 1023.9007194018923 average_R:243.95580426454887\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 116.051765), ('average_q2', 116.07769), ('average_q_func1_loss', 5.037859377861023), ('average_q_func2_loss', 4.802486033439636), ('n_updates', 49001), ('average_entropy', -2.9674034), ('temperature', 0.048006635159254074)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:60000 episode:803 last_R: 1023.4497489358257 average_R:259.82653949665314\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 115.08695), ('average_q2', 115.04245), ('average_q_func1_loss', 5.379106283187866), ('average_q_func2_loss', 4.8123835587501524), ('n_updates', 50001), ('average_entropy', -3.0176296), ('temperature', 0.04643449932336807)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 240 R: 405.18197674537913\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 228 R: 393.93773396313117\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 250 R: 413.8940286995062\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 236 R: 398.3130599785901\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 297 R: 460.5583483410828\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 210 R: 317.05774905369407\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 129 R: 239.0278926191536\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 1000 R: 1021.3571734080451\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 169 R: 334.80494497381494\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 981 R: 1144.8547760981266\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:61000 episode:805 last_R: 450.07142030126715 average_R:264.5886011771805\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 113.47087), ('average_q2', 113.51977), ('average_q_func1_loss', 5.059606671333313), ('average_q_func2_loss', 5.073657739162445), ('n_updates', 51001), ('average_entropy', -3.059636), ('temperature', 0.04666341841220856)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:62000 episode:806 last_R: 473.5475010689308 average_R:267.28856070665535\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 116.88611), ('average_q2', 116.58767), ('average_q_func1_loss', 5.082375937700272), ('average_q_func2_loss', 4.447500941753387), ('n_updates', 52001), ('average_entropy', -3.1150455), ('temperature', 0.04825049638748169)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:63000 episode:812 last_R: 213.6893592005683 average_R:283.8894326605538\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 116.55988), ('average_q2', 116.75735), ('average_q_func1_loss', 4.187810828685761), ('average_q_func2_loss', 3.9737479519844054), ('n_updates', 53001), ('average_entropy', -3.2686472), ('temperature', 0.04912003502249718)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:64000 episode:817 last_R: 233.5531213321597 average_R:294.4860779385757\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 118.026405), ('average_q2', 118.29397), ('average_q_func1_loss', 4.657343881130219), ('average_q_func2_loss', 4.80726444363594), ('n_updates', 54001), ('average_entropy', -2.9601731), ('temperature', 0.048316411674022675)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:65000 episode:825 last_R: 216.64692267156633 average_R:298.7815550385045\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 119.502525), ('average_q2', 119.465065), ('average_q_func1_loss', 4.4169233751296995), ('average_q_func2_loss', 4.352455830574035), ('n_updates', 55001), ('average_entropy', -2.9171479), ('temperature', 0.04866786673665047)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 123 R: 221.59084701984398\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 121 R: 217.1488929100557\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 121 R: 217.8321493819611\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 119 R: 207.39189230493903\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 121 R: 216.81335606795946\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 122 R: 219.3492539990193\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 123 R: 221.7270199783932\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 119 R: 209.4955788989589\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 119 R: 211.2695512180378\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 120 R: 215.89510479685973\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:66000 episode:833 last_R: 216.8372821322322 average_R:303.5617128337538\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 118.38329), ('average_q2', 118.43349), ('average_q_func1_loss', 4.854999743700027), ('average_q_func2_loss', 4.523719453811646), ('n_updates', 56001), ('average_entropy', -2.8500037), ('temperature', 0.0500740148127079)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:67000 episode:843 last_R: 219.71610666341783 average_R:318.10461564662796\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 117.46687), ('average_q2', 117.46727), ('average_q_func1_loss', 4.639101158380509), ('average_q_func2_loss', 4.469696800708771), ('n_updates', 57001), ('average_entropy', -3.1085145), ('temperature', 0.04982711002230644)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:68000 episode:851 last_R: 222.0970317703042 average_R:330.82227207972477\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 117.226234), ('average_q2', 117.14735), ('average_q_func1_loss', 5.38049168586731), ('average_q_func2_loss', 4.942420024871826), ('n_updates', 58001), ('average_entropy', -2.9318447), ('temperature', 0.051581840962171555)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:69000 episode:858 last_R: 219.887833172465 average_R:339.4050905072477\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 116.13111), ('average_q2', 116.40173), ('average_q_func1_loss', 4.602857834100723), ('average_q_func2_loss', 4.355369375944138), ('n_updates', 59001), ('average_entropy', -3.320603), ('temperature', 0.05338013172149658)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:70000 episode:868 last_R: 226.28112798470195 average_R:333.1333686520956\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 116.67842), ('average_q2', 116.91643), ('average_q_func1_loss', 4.441220309734344), ('average_q_func2_loss', 4.158282560110092), ('n_updates', 60001), ('average_entropy', -3.0262678), ('temperature', 0.05000748857855797)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 122 R: 222.73868132980505\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 125 R: 226.0467913362276\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 122 R: 223.82893150223475\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 121 R: 222.81468394616678\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 123 R: 222.37105890605193\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 121 R: 223.74474321745734\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 123 R: 221.78540373816273\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 122 R: 223.01452343227666\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 122 R: 222.57644299935282\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 121 R: 221.44628663481018\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:71000 episode:875 last_R: 236.89444985394738 average_R:322.32416945023215\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 119.79947), ('average_q2', 119.64627), ('average_q_func1_loss', 4.27339901804924), ('average_q_func2_loss', 4.209249602556229), ('n_updates', 61001), ('average_entropy', -2.925485), ('temperature', 0.04795742407441139)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:72000 episode:883 last_R: 229.83054685964703 average_R:320.73126880778284\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 120.021126), ('average_q2', 120.06386), ('average_q_func1_loss', 4.777991374731064), ('average_q_func2_loss', 4.614929876327515), ('n_updates', 62001), ('average_entropy', -2.832231), ('temperature', 0.04770741984248161)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:73000 episode:890 last_R: 207.14739395840937 average_R:318.3914299065967\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 118.58798), ('average_q2', 118.63833), ('average_q_func1_loss', 4.2363247966766355), ('average_q_func2_loss', 4.283066693544388), ('n_updates', 63001), ('average_entropy', -2.9616668), ('temperature', 0.045473046600818634)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:74000 episode:898 last_R: 229.9452751593523 average_R:300.0266467688982\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 121.624794), ('average_q2', 121.69496), ('average_q_func1_loss', 3.901480141878128), ('average_q_func2_loss', 3.7992900478839875), ('n_updates', 64001), ('average_entropy', -3.2130473), ('temperature', 0.046843159943819046)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:75000 episode:906 last_R: 218.27840537831943 average_R:253.87631504528312\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.39538), ('average_q2', 122.47491), ('average_q_func1_loss', 4.35991058588028), ('average_q_func2_loss', 3.882915802001953), ('n_updates', 65001), ('average_entropy', -2.9533632), ('temperature', 0.05427759140729904)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 124 R: 226.7180027369587\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 128 R: 234.90326072867092\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 132 R: 250.13647480309652\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 123 R: 225.83482444243455\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 124 R: 222.73812514201308\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 127 R: 230.56735965705423\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 124 R: 227.6623234280271\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 123 R: 224.8177053917371\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 126 R: 234.7622400380202\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 127 R: 237.27279477084673\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:76000 episode:914 last_R: 236.4395156060975 average_R:228.54713171393175\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 121.47311), ('average_q2', 121.542564), ('average_q_func1_loss', 5.500581315755844), ('average_q_func2_loss', 4.877802413702011), ('n_updates', 66001), ('average_entropy', -2.6416993), ('temperature', 0.0531490184366703)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:77000 episode:922 last_R: 219.7990753803102 average_R:228.90192664270515\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.62458), ('average_q2', 122.4315), ('average_q_func1_loss', 3.9054550278186797), ('average_q_func2_loss', 3.5962370872497558), ('n_updates', 67001), ('average_entropy', -2.8490794), ('temperature', 0.04868881404399872)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:78000 episode:926 last_R: 231.2345907031512 average_R:228.77999929125335\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.78455), ('average_q2', 122.85061), ('average_q_func1_loss', 4.819320590496063), ('average_q_func2_loss', 4.693460800647736), ('n_updates', 68001), ('average_entropy', -2.8437524), ('temperature', 0.048051971942186356)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:79000 episode:931 last_R: 514.0149826190267 average_R:240.52511185770086\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 121.58637), ('average_q2', 121.61717), ('average_q_func1_loss', 4.950306932926178), ('average_q_func2_loss', 4.470376832485199), ('n_updates', 69001), ('average_entropy', -2.8278174), ('temperature', 0.04417377710342407)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:80000 episode:936 last_R: 254.26540972398087 average_R:244.54105242259797\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.94754), ('average_q2', 122.809074), ('average_q_func1_loss', 4.442569383382797), ('average_q_func2_loss', 4.510919210910797), ('n_updates', 70001), ('average_entropy', -3.160675), ('temperature', 0.045098207890987396)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 264 R: 429.6639709566106\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 217 R: 382.97302945222833\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 216 R: 379.1259497115828\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 156 R: 270.49338300872387\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 162 R: 278.06664454067567\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 222 R: 384.75953118902044\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 214 R: 377.17479651447667\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 223 R: 383.4477473690253\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 181 R: 339.70927678200013\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 221 R: 384.28209492079515\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:81000 episode:942 last_R: 271.44670814033776 average_R:250.35285107252315\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 121.72982), ('average_q2', 121.9013), ('average_q_func1_loss', 6.975240044593811), ('average_q_func2_loss', 6.292326176166535), ('n_updates', 71001), ('average_entropy', -2.6776705), ('temperature', 0.047240134328603745)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:82000 episode:944 last_R: 448.2991536746473 average_R:253.80648891334593\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 124.38712), ('average_q2', 124.39347), ('average_q_func1_loss', 5.548124823570252), ('average_q_func2_loss', 5.205278520584106), ('n_updates', 72001), ('average_entropy', -3.0103018), ('temperature', 0.04849090054631233)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:83000 episode:949 last_R: 350.427064512884 average_R:267.3575640897879\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 124.418724), ('average_q2', 124.336784), ('average_q_func1_loss', 5.250877499580383), ('average_q_func2_loss', 5.018794485330582), ('n_updates', 73001), ('average_entropy', -2.7930532), ('temperature', 0.048263244330883026)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:84000 episode:956 last_R: 331.92448665327385 average_R:272.7573385016601\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.30108), ('average_q2', 123.1613), ('average_q_func1_loss', 4.581905399560928), ('average_q_func2_loss', 4.250369197130203), ('n_updates', 74001), ('average_entropy', -3.0397172), ('temperature', 0.048338476568460464)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:85000 episode:964 last_R: 312.0467540172889 average_R:278.3762903133169\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.32404), ('average_q2', 123.31908), ('average_q_func1_loss', 4.104547635316849), ('average_q_func2_loss', 3.9126254105567932), ('n_updates', 75001), ('average_entropy', -3.017258), ('temperature', 0.04823146015405655)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 152 R: 316.04880155895967\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 150 R: 312.58609510457006\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 146 R: 301.4578972832876\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 121 R: 202.2008590753249\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 122 R: 203.96567575039694\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 150 R: 312.6041173280142\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 151 R: 313.45445122310724\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 140 R: 274.86190037498653\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 147 R: 307.411054400889\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 122 R: 213.8133559394748\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:86000 episode:972 last_R: 195.67225973885866 average_R:276.33172742394413\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.23843), ('average_q2', 125.19533), ('average_q_func1_loss', 4.522823715209961), ('average_q_func2_loss', 4.372619752883911), ('n_updates', 76001), ('average_entropy', -3.0009809), ('temperature', 0.04803662374615669)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:87000 episode:979 last_R: 225.31445126945448 average_R:277.0736668302063\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.3148), ('average_q2', 127.41522), ('average_q_func1_loss', 4.490619897842407), ('average_q_func2_loss', 4.2954457104206085), ('n_updates', 77001), ('average_entropy', -3.1952975), ('temperature', 0.05152316391468048)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:88000 episode:986 last_R: 301.21708018913023 average_R:280.9087492732413\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.02326), ('average_q2', 125.662445), ('average_q_func1_loss', 4.745074093341827), ('average_q_func2_loss', 4.4198061001300815), ('n_updates', 78001), ('average_entropy', -3.0536714), ('temperature', 0.05403535068035126)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:89000 episode:993 last_R: 231.7676753710353 average_R:283.03291105748235\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.099), ('average_q2', 128.10785), ('average_q_func1_loss', 4.582460350990296), ('average_q_func2_loss', 4.618864846229553), ('n_updates', 79001), ('average_entropy', -3.0970666), ('temperature', 0.05407838895916939)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:90000 episode:1001 last_R: 317.2883471666005 average_R:284.9127256174417\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.88836), ('average_q2', 128.04475), ('average_q_func1_loss', 5.560436770915985), ('average_q_func2_loss', 5.887580766677856), ('n_updates', 80001), ('average_entropy', -2.9009032), ('temperature', 0.053148359060287476)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 115 R: 213.49071360283244\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 113 R: 227.88704548884468\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 113 R: 213.43054958840034\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 121 R: 218.69746503199195\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 114 R: 225.11787462589692\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 116 R: 227.5942241423033\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 114 R: 227.4198561059266\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 114 R: 224.2556408207884\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 115 R: 240.69900573115947\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 112 R: 214.1869571889324\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:91000 episode:1007 last_R: 232.30101505441067 average_R:290.99165143739197\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.967415), ('average_q2', 124.10961), ('average_q_func1_loss', 6.159143607616425), ('average_q_func2_loss', 5.8820389556884765), ('n_updates', 81001), ('average_entropy', -2.980447), ('temperature', 0.05074898898601532)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:92000 episode:1013 last_R: 296.58487107734277 average_R:296.1935838605883\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.00902), ('average_q2', 127.848305), ('average_q_func1_loss', 4.416195794343948), ('average_q_func2_loss', 4.2809631896018985), ('n_updates', 82001), ('average_entropy', -2.8430586), ('temperature', 0.04804561287164688)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:93000 episode:1019 last_R: 217.76653123982217 average_R:296.37083408430084\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.625725), ('average_q2', 127.53254), ('average_q_func1_loss', 4.068443483114242), ('average_q_func2_loss', 4.139069302082062), ('n_updates', 83001), ('average_entropy', -3.11712), ('temperature', 0.04841501638293266)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:94000 episode:1027 last_R: 222.25290819121213 average_R:297.91944931554036\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 131.23026), ('average_q2', 131.06973), ('average_q_func1_loss', 4.766947551965713), ('average_q_func2_loss', 4.322178844213486), ('n_updates', 84001), ('average_entropy', -2.9180517), ('temperature', 0.04884786158800125)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:95000 episode:1033 last_R: 351.7876798528124 average_R:289.58328831992355\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.10737), ('average_q2', 127.09536), ('average_q_func1_loss', 4.354608625173569), ('average_q_func2_loss', 4.029893399477005), ('n_updates', 85001), ('average_entropy', -3.029633), ('temperature', 0.04834223538637161)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 125 R: 197.47807774599931\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 158 R: 316.14811400092884\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 163 R: 327.9406638716577\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 125 R: 193.87161578640016\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 160 R: 321.9736297723901\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 167 R: 347.980725504973\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 163 R: 327.7526074208994\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 160 R: 321.00363346723833\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 157 R: 305.6463651784279\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 118 R: 178.5930107096563\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:96000 episode:1041 last_R: 216.90070198517503 average_R:283.12728911403946\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.68726), ('average_q2', 128.59169), ('average_q_func1_loss', 5.384301512241364), ('average_q_func2_loss', 5.174683387279511), ('n_updates', 86001), ('average_entropy', -2.9951806), ('temperature', 0.04902273789048195)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:97000 episode:1048 last_R: 288.600885815055 average_R:268.9450673581746\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.14604), ('average_q2', 127.270515), ('average_q_func1_loss', 5.069185073375702), ('average_q_func2_loss', 5.236278183460236), ('n_updates', 87001), ('average_entropy', -2.8782587), ('temperature', 0.04954524710774422)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:98000 episode:1053 last_R: 173.7702237801369 average_R:267.1485534689506\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.4558), ('average_q2', 127.49712), ('average_q_func1_loss', 4.408487777709961), ('average_q_func2_loss', 4.285938398838043), ('n_updates', 88001), ('average_entropy', -3.050998), ('temperature', 0.049888648092746735)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:99000 episode:1059 last_R: 334.5314967056214 average_R:269.35745668718573\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.50372), ('average_q2', 127.71492), ('average_q_func1_loss', 4.343109793663025), ('average_q_func2_loss', 4.102371541261673), ('n_updates', 89001), ('average_entropy', -3.1283894), ('temperature', 0.04978087171912193)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:100000 episode:1064 last_R: 336.3614903163025 average_R:275.61458670793496\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.28236), ('average_q2', 128.06677), ('average_q_func1_loss', 4.699758810997009), ('average_q_func2_loss', 4.2572235274314885), ('n_updates', 90001), ('average_entropy', -3.0387995), ('temperature', 0.051050469279289246)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 210 R: 427.38320464079663\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 180 R: 339.7666410880379\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 138 R: 234.70560408438214\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 177 R: 343.7389315650277\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 176 R: 342.58322547711697\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 202 R: 416.13088214908583\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 206 R: 422.06953533109726\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 176 R: 336.21887220109204\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 140 R: 238.94586036121032\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 180 R: 347.7303683375106\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:101000 episode:1068 last_R: 334.99738431721244 average_R:283.05763716887327\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.99638), ('average_q2', 129.03233), ('average_q_func1_loss', 5.350235247612), ('average_q_func2_loss', 5.629161409139633), ('n_updates', 91001), ('average_entropy', -2.8301334), ('temperature', 0.0523676760494709)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:102000 episode:1073 last_R: 345.8628712103002 average_R:292.4295679495599\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.11108), ('average_q2', 126.56639), ('average_q_func1_loss', 4.574963998794556), ('average_q_func2_loss', 4.517729777097702), ('n_updates', 92001), ('average_entropy', -2.9479837), ('temperature', 0.0549934059381485)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:103000 episode:1078 last_R: 374.8352741055577 average_R:301.5300759542428\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.260925), ('average_q2', 127.31958), ('average_q_func1_loss', 4.361644049882889), ('average_q_func2_loss', 4.101871596574783), ('n_updates', 93001), ('average_entropy', -3.1025455), ('temperature', 0.05438270792365074)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:104000 episode:1083 last_R: 361.03747126011876 average_R:309.7823498432048\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.81036), ('average_q2', 132.69498), ('average_q_func1_loss', 4.237683762311935), ('average_q_func2_loss', 3.874005527496338), ('n_updates', 94001), ('average_entropy', -2.8767521), ('temperature', 0.052479639649391174)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:105000 episode:1090 last_R: 356.29210279858586 average_R:315.1638634303641\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.19989), ('average_q2', 129.12291), ('average_q_func1_loss', 4.828684659004211), ('average_q_func2_loss', 4.585013166666031), ('n_updates', 95001), ('average_entropy', -2.9429712), ('temperature', 0.04994731396436691)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 184 R: 339.25769119172946\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 204 R: 464.58062705121347\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 175 R: 338.8116540297703\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 227 R: 533.7384446242224\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 174 R: 339.87550185311176\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 175 R: 343.53948089631103\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 172 R: 339.92073641615224\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 181 R: 327.1586466525788\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 179 R: 346.5519897373329\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 186 R: 376.53223248274804\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:106000 episode:1094 last_R: 365.44770894069364 average_R:319.68286237226613\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 131.71481), ('average_q2', 131.7331), ('average_q_func1_loss', 4.455977034568787), ('average_q_func2_loss', 4.494596875905991), ('n_updates', 96001), ('average_entropy', -2.8377447), ('temperature', 0.04922635853290558)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:107000 episode:1100 last_R: 321.8170914984632 average_R:330.41142810972445\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.33566), ('average_q2', 129.18143), ('average_q_func1_loss', 4.744276237487793), ('average_q_func2_loss', 4.678861247301102), ('n_updates', 97001), ('average_entropy', -3.0910542), ('temperature', 0.051312655210494995)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:108000 episode:1106 last_R: 217.00142451578094 average_R:324.00997632015384\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.25279), ('average_q2', 128.6188), ('average_q_func1_loss', 4.463019530773163), ('average_q_func2_loss', 4.505725764036178), ('n_updates', 98001), ('average_entropy', -2.8417993), ('temperature', 0.05194329097867012)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:109000 episode:1114 last_R: 229.2683331192257 average_R:320.1003905623827\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.36305), ('average_q2', 130.08186), ('average_q_func1_loss', 4.397585455179215), ('average_q_func2_loss', 4.149792048931122), ('n_updates', 99001), ('average_entropy', -3.1709213), ('temperature', 0.05003591999411583)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:110000 episode:1118 last_R: 428.4676479321661 average_R:325.95306548833\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 131.66762), ('average_q2', 131.536), ('average_q_func1_loss', 5.033063015937805), ('average_q_func2_loss', 4.799991016387939), ('n_updates', 100001), ('average_entropy', -2.9276426), ('temperature', 0.04730815440416336)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 133 R: 255.07368071464313\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 132 R: 240.10069530148817\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 135 R: 256.38030608907155\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 133 R: 250.47367451456398\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 136 R: 259.28655556585846\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 132 R: 243.60763579539488\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 133 R: 243.87049218004924\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 134 R: 257.6595772076224\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 147 R: 266.9169845096812\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 130 R: 232.17947115731127\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:111000 episode:1122 last_R: 319.62169050115904 average_R:337.04712662401766\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.68044), ('average_q2', 128.62852), ('average_q_func1_loss', 4.413318845033646), ('average_q_func2_loss', 3.992189404964447), ('n_updates', 101001), ('average_entropy', -2.805061), ('temperature', 0.04711884260177612)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:112000 episode:1128 last_R: 334.56130051415926 average_R:353.5328923983619\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.44421), ('average_q2', 130.75822), ('average_q_func1_loss', 5.293492636680603), ('average_q_func2_loss', 4.890287978649139), ('n_updates', 102001), ('average_entropy', -3.0140972), ('temperature', 0.04744257777929306)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:113000 episode:1133 last_R: 371.6909307474442 average_R:352.56837414855573\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.91722), ('average_q2', 129.59303), ('average_q_func1_loss', 4.331639152765274), ('average_q_func2_loss', 3.909886964559555), ('n_updates', 103001), ('average_entropy', -3.0865405), ('temperature', 0.0505027137696743)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:114000 episode:1137 last_R: 399.2031111638847 average_R:360.11827486853906\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.26425), ('average_q2', 130.31668), ('average_q_func1_loss', 4.9661831021308895), ('average_q_func2_loss', 4.179792023897171), ('n_updates', 104001), ('average_entropy', -2.9862494), ('temperature', 0.049734536558389664)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:115000 episode:1142 last_R: 204.0573373108697 average_R:366.66556189442736\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.47206), ('average_q2', 128.34908), ('average_q_func1_loss', 4.416478953361511), ('average_q_func2_loss', 4.174887182712555), ('n_updates', 105001), ('average_entropy', -3.0018268), ('temperature', 0.04796016588807106)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 192 R: 359.00793060640734\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 183 R: 341.3968163068261\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 207 R: 373.9316620439326\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 204 R: 372.5679592203373\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 263 R: 480.4131520814944\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 213 R: 429.3995722617739\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 195 R: 361.70561108757215\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 196 R: 411.01683573827296\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 190 R: 357.17832795021934\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 197 R: 362.6288542854154\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:116000 episode:1147 last_R: 457.14549844613515 average_R:375.1894042482687\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.34488), ('average_q2', 130.23502), ('average_q_func1_loss', 4.3380547142028805), ('average_q_func2_loss', 4.155893719196319), ('n_updates', 106001), ('average_entropy', -3.0992553), ('temperature', 0.046064168214797974)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:117000 episode:1152 last_R: 528.6284376492456 average_R:383.2879531469256\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.30058), ('average_q2', 128.18938), ('average_q_func1_loss', 4.505921096801758), ('average_q_func2_loss', 3.7376425182819366), ('n_updates', 107001), ('average_entropy', -3.0565326), ('temperature', 0.04558159038424492)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:118000 episode:1157 last_R: 440.8034387383608 average_R:384.0601446921539\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.81648), ('average_q2', 130.95726), ('average_q_func1_loss', 6.966610865592957), ('average_q_func2_loss', 6.881803281307221), ('n_updates', 108001), ('average_entropy', -3.184109), ('temperature', 0.04686462879180908)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:119000 episode:1162 last_R: 623.3755157972824 average_R:388.4757760610618\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.72699), ('average_q2', 130.8064), ('average_q_func1_loss', 3.8411220514774325), ('average_q_func2_loss', 4.03150685429573), ('n_updates', 109001), ('average_entropy', -2.8718798), ('temperature', 0.047014977782964706)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:120000 episode:1167 last_R: 337.0121570094558 average_R:387.91295158952687\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.9246), ('average_q2', 130.70097), ('average_q_func1_loss', 6.6011483144760135), ('average_q_func2_loss', 5.463444979190826), ('n_updates', 110001), ('average_entropy', -2.9103076), ('temperature', 0.04682384431362152)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 192 R: 446.76236273680234\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 197 R: 419.123503427122\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 198 R: 398.48242812411473\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 188 R: 457.3285538283617\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 152 R: 317.6754413330266\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 192 R: 443.91822408418113\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 269 R: 640.0364082466823\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 197 R: 442.28849943520316\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 145 R: 255.4021433028688\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 193 R: 442.9665788310127\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:121000 episode:1172 last_R: 447.94036595788 average_R:391.51076357516854\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.14236), ('average_q2', 128.0583), ('average_q_func1_loss', 5.195315647125244), ('average_q_func2_loss', 5.110369231700897), ('n_updates', 111001), ('average_entropy', -2.8784964), ('temperature', 0.04652909189462662)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:122000 episode:1177 last_R: 431.36738915967754 average_R:391.39971569985045\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.46964), ('average_q2', 128.48302), ('average_q_func1_loss', 4.259150502681732), ('average_q_func2_loss', 4.139249970912934), ('n_updates', 112001), ('average_entropy', -3.096224), ('temperature', 0.04750455915927887)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:123000 episode:1183 last_R: 308.90906363831067 average_R:386.2931253654947\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.31927), ('average_q2', 126.63065), ('average_q_func1_loss', 3.9537936437129972), ('average_q_func2_loss', 3.9965779685974123), ('n_updates', 113001), ('average_entropy', -2.9241824), ('temperature', 0.047695741057395935)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:124000 episode:1189 last_R: 258.5427199751637 average_R:381.0317395026804\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 131.42722), ('average_q2', 131.34763), ('average_q_func1_loss', 4.002667828798294), ('average_q_func2_loss', 4.059387083053589), ('n_updates', 114001), ('average_entropy', -2.9876635), ('temperature', 0.04605517536401749)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:125000 episode:1195 last_R: 337.06826243628393 average_R:378.0459345271\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.8165), ('average_q2', 127.10684), ('average_q_func1_loss', 4.055371179580688), ('average_q_func2_loss', 3.8542316591739656), ('n_updates', 115001), ('average_entropy', -3.2291622), ('temperature', 0.047062356024980545)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 136 R: 210.00125137408423\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 180 R: 352.12373104946283\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 216 R: 486.7184244136429\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 144 R: 229.64936519066507\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 144 R: 229.1359367274397\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 228 R: 516.4082671871712\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 179 R: 345.41868649282884\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 176 R: 309.6177399479935\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 135 R: 206.16913388633606\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 220 R: 493.763251196536\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:126000 episode:1200 last_R: 244.37775212547632 average_R:378.27259265962886\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.9319), ('average_q2', 129.03107), ('average_q_func1_loss', 4.003959062099457), ('average_q_func2_loss', 3.677912700176239), ('n_updates', 116001), ('average_entropy', -2.812341), ('temperature', 0.04743599519133568)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:127000 episode:1207 last_R: 325.53204812558107 average_R:385.09000681477085\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.50563), ('average_q2', 129.4017), ('average_q_func1_loss', 5.374859018325806), ('average_q_func2_loss', 4.9717190861701965), ('n_updates', 117001), ('average_entropy', -3.1511462), ('temperature', 0.0460791289806366)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:128000 episode:1213 last_R: 259.0697294175226 average_R:388.05542975812887\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.92838), ('average_q2', 128.70415), ('average_q_func1_loss', 4.3756431341171265), ('average_q_func2_loss', 4.06841704249382), ('n_updates', 118001), ('average_entropy', -3.1372428), ('temperature', 0.04687786102294922)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:129000 episode:1218 last_R: 461.0179833155076 average_R:384.7885848956692\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.09768), ('average_q2', 125.01215), ('average_q_func1_loss', 3.4877140033245086), ('average_q_func2_loss', 3.273357471227646), ('n_updates', 119001), ('average_entropy', -3.060169), ('temperature', 0.0463283509016037)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:130000 episode:1223 last_R: 595.1776279602967 average_R:379.1265729476479\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.50167), ('average_q2', 125.563934), ('average_q_func1_loss', 4.049701068401337), ('average_q_func2_loss', 3.9696738469600676), ('n_updates', 120001), ('average_entropy', -2.887026), ('temperature', 0.04442356154322624)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 150 R: 268.5606112715463\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 163 R: 336.24061722913444\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 145 R: 252.884646598347\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 167 R: 337.59232258253337\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 166 R: 336.7744261325821\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 166 R: 328.3579873817177\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 149 R: 265.4656530566377\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 167 R: 337.706537807651\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 166 R: 336.23711963358363\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 185 R: 387.9178712512452\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:131000 episode:1228 last_R: 329.05388974229766 average_R:371.20775597703573\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.572556), ('average_q2', 125.550255), ('average_q_func1_loss', 4.371784564256668), ('average_q_func2_loss', 3.8912666392326356), ('n_updates', 121001), ('average_entropy', -2.880517), ('temperature', 0.04396400228142738)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:132000 episode:1233 last_R: 619.6081675603721 average_R:379.6575586651607\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.177086), ('average_q2', 125.469955), ('average_q_func1_loss', 3.8086725080013277), ('average_q_func2_loss', 3.664053009748459), ('n_updates', 122001), ('average_entropy', -3.0142608), ('temperature', 0.045420169830322266)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:133000 episode:1238 last_R: 335.26777965012684 average_R:375.5301474539878\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.068146), ('average_q2', 126.84451), ('average_q_func1_loss', 3.8507528162002562), ('average_q_func2_loss', 4.248962297439575), ('n_updates', 123001), ('average_entropy', -3.0048404), ('temperature', 0.04298717528581619)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:134000 episode:1242 last_R: 568.6797686657951 average_R:381.67429873444235\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.76487), ('average_q2', 127.833176), ('average_q_func1_loss', 3.539532722234726), ('average_q_func2_loss', 4.011110725402832), ('n_updates', 124001), ('average_entropy', -3.0153964), ('temperature', 0.0427849180996418)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:135000 episode:1247 last_R: 679.9853037959073 average_R:390.1368517670253\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.4065), ('average_q2', 125.19229), ('average_q_func1_loss', 3.897102597951889), ('average_q_func2_loss', 3.6162486517429353), ('n_updates', 125001), ('average_entropy', -2.946626), ('temperature', 0.0415128730237484)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 172 R: 343.5098226485035\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 230 R: 561.7090729128195\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 169 R: 330.26426933701885\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 170 R: 324.0835740388212\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 223 R: 499.5200390502437\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 198 R: 418.6438042948375\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 223 R: 495.87324414000454\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 218 R: 437.1475469082547\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 193 R: 407.169897532923\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 168 R: 329.2715642213572\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:136000 episode:1252 last_R: 443.6616606001604 average_R:388.0840427798437\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.83544), ('average_q2', 127.71149), ('average_q_func1_loss', 3.658714429140091), ('average_q_func2_loss', 4.472087339162827), ('n_updates', 126001), ('average_entropy', -3.0429757), ('temperature', 0.040665432810783386)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:137000 episode:1254 last_R: 833.0128417296582 average_R:396.20303904735175\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.51653), ('average_q2', 126.420685), ('average_q_func1_loss', 4.438116004467011), ('average_q_func2_loss', 4.2526551675796505), ('n_updates', 127001), ('average_entropy', -3.0547884), ('temperature', 0.041019510477781296)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:138000 episode:1257 last_R: 785.0119818033826 average_R:410.5646708255655\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.37467), ('average_q2', 123.19742), ('average_q_func1_loss', 3.862471761703491), ('average_q_func2_loss', 3.6760878443717955), ('n_updates', 128001), ('average_entropy', -2.9082847), ('temperature', 0.04116469621658325)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:139000 episode:1262 last_R: 558.6252497752804 average_R:418.4105170739874\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.63956), ('average_q2', 128.81863), ('average_q_func1_loss', 3.9668518078327177), ('average_q_func2_loss', 4.141562298536301), ('n_updates', 129001), ('average_entropy', -2.9654), ('temperature', 0.04008355364203453)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:140000 episode:1265 last_R: 452.60581592029376 average_R:416.7919170939451\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.432014), ('average_q2', 127.47394), ('average_q_func1_loss', 3.7104461181163786), ('average_q_func2_loss', 3.581926542520523), ('n_updates', 130001), ('average_entropy', -3.0084643), ('temperature', 0.04044833779335022)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 267 R: 498.2813250329093\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 320 R: 702.6010427664795\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 313 R: 630.3162560962772\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 326 R: 777.0951260734611\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 371 R: 637.3224390194697\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 329 R: 665.8955417198923\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 402 R: 718.410038766021\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 384 R: 941.8872892241501\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 271 R: 506.48497337780174\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 297 R: 513.2856854057266\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:141000 episode:1268 last_R: 420.539592528957 average_R:419.48249800639474\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.35375), ('average_q2', 125.15857), ('average_q_func1_loss', 3.372850949764252), ('average_q_func2_loss', 3.758331878185272), ('n_updates', 131001), ('average_entropy', -2.9727724), ('temperature', 0.039772678166627884)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:142000 episode:1272 last_R: 703.0084645310326 average_R:432.1877838062208\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 124.65025), ('average_q2', 124.79397), ('average_q_func1_loss', 3.506914106607437), ('average_q_func2_loss', 3.450585502386093), ('n_updates', 132001), ('average_entropy', -3.0979314), ('temperature', 0.04079052060842514)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:143000 episode:1275 last_R: 652.8979292409191 average_R:434.2402563296823\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.73938), ('average_q2', 128.70828), ('average_q_func1_loss', 4.31458279132843), ('average_q_func2_loss', 3.793765904903412), ('n_updates', 133001), ('average_entropy', -2.9119062), ('temperature', 0.040462493896484375)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:144000 episode:1280 last_R: 545.9719943642598 average_R:444.47127728349034\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.98724), ('average_q2', 129.17737), ('average_q_func1_loss', 4.470339621305466), ('average_q_func2_loss', 4.866275529861451), ('n_updates', 134001), ('average_entropy', -3.046237), ('temperature', 0.03974545747041702)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:145000 episode:1283 last_R: 537.7227431631803 average_R:448.53046123696294\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.983345), ('average_q2', 125.926926), ('average_q_func1_loss', 3.9969363713264467), ('average_q_func2_loss', 3.696513079404831), ('n_updates', 135001), ('average_entropy', -3.0229716), ('temperature', 0.03966042026877403)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 327 R: 871.4597264762336\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 315 R: 728.3379106616691\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 319 R: 765.8409621963586\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 372 R: 637.0759498390603\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 290 R: 644.3858674445511\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 280 R: 622.2555220415057\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 330 R: 822.5318683569305\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 275 R: 542.681091823228\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 324 R: 589.2570885057561\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 250 R: 516.3053235928849\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:146000 episode:1285 last_R: 814.5907419104367 average_R:458.6685510539691\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.87584), ('average_q2', 127.79261), ('average_q_func1_loss', 5.010009846687317), ('average_q_func2_loss', 4.932249542474747), ('n_updates', 136001), ('average_entropy', -2.959283), ('temperature', 0.03883500024676323)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:147000 episode:1290 last_R: 426.5329467003606 average_R:476.58908559164956\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.91985), ('average_q2', 125.60577), ('average_q_func1_loss', 5.473252217769623), ('average_q_func2_loss', 5.121547427177429), ('n_updates', 137001), ('average_entropy', -2.902046), ('temperature', 0.03967221453785896)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:148000 episode:1292 last_R: 561.4930223509989 average_R:479.9616658060919\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.18558), ('average_q2', 129.27757), ('average_q_func1_loss', 4.409559121131897), ('average_q_func2_loss', 4.012786827087402), ('n_updates', 138001), ('average_entropy', -3.1461442), ('temperature', 0.03854101523756981)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:149000 episode:1296 last_R: 708.6914124248475 average_R:500.32667501017175\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.66762), ('average_q2', 129.87894), ('average_q_func1_loss', 3.7946205878257753), ('average_q_func2_loss', 3.8141966497898103), ('n_updates', 139001), ('average_entropy', -3.0159154), ('temperature', 0.0393860824406147)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:150000 episode:1300 last_R: 796.952070283875 average_R:504.6127419384982\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.72598), ('average_q2', 130.98972), ('average_q_func1_loss', 4.1540412366390225), ('average_q_func2_loss', 3.938396692276001), ('n_updates', 140001), ('average_entropy', -2.8327227), ('temperature', 0.039840035140514374)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 290 R: 709.121317307181\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 281 R: 589.0423201639122\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 288 R: 710.8718595145324\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 309 R: 564.4272854322985\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 452 R: 1097.5997237798085\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 306 R: 731.1119064146563\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 416 R: 968.2322126597853\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 448 R: 985.2139906059243\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 342 R: 856.0688490889488\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 289 R: 703.9804369229219\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:151000 episode:1304 last_R: 760.3633974931352 average_R:516.9256435946581\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.48338), ('average_q2', 129.48322), ('average_q_func1_loss', 3.7484170591831205), ('average_q_func2_loss', 3.888979090452194), ('n_updates', 141001), ('average_entropy', -3.001678), ('temperature', 0.040386248379945755)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:152000 episode:1308 last_R: 343.7926618341441 average_R:526.2469625763943\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.0305), ('average_q2', 128.79721), ('average_q_func1_loss', 3.4050549948215485), ('average_q_func2_loss', 3.1997551679611207), ('n_updates', 142001), ('average_entropy', -3.1940134), ('temperature', 0.04015093296766281)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:153000 episode:1309 last_R: 471.4470281254743 average_R:528.8023209929172\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.59576), ('average_q2', 132.62866), ('average_q_func1_loss', 3.799868153333664), ('average_q_func2_loss', 4.100051107406617), ('n_updates', 143001), ('average_entropy', -2.9417584), ('temperature', 0.039942629635334015)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:154000 episode:1313 last_R: 719.5377248938527 average_R:545.9024826509227\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 131.19435), ('average_q2', 131.15034), ('average_q_func1_loss', 3.986349813938141), ('average_q_func2_loss', 3.796914340257645), ('n_updates', 144001), ('average_entropy', -3.1296837), ('temperature', 0.04126312956213951)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:155000 episode:1317 last_R: 592.0907670134198 average_R:563.7717239987986\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.02484), ('average_q2', 129.76285), ('average_q_func1_loss', 5.149871472120285), ('average_q_func2_loss', 4.27690709233284), ('n_updates', 145001), ('average_entropy', -3.0744874), ('temperature', 0.039848897606134415)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 322 R: 706.2308312352801\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 209 R: 420.31702296941074\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 313 R: 669.3498391089918\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 272 R: 697.0887355326943\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 163 R: 274.87728533416725\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 180 R: 358.93856065344625\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 408 R: 774.9013733101642\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 318 R: 708.7856725758813\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 277 R: 543.6308675981486\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 184 R: 364.8305930643636\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:156000 episode:1321 last_R: 597.6828551193005 average_R:572.8023851627593\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.12267), ('average_q2', 129.07466), ('average_q_func1_loss', 4.245527272224426), ('average_q_func2_loss', 3.9827235031127928), ('n_updates', 146001), ('average_entropy', -3.181997), ('temperature', 0.03930385783314705)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:157000 episode:1323 last_R: 598.4152932117387 average_R:574.7902177663171\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.21703), ('average_q2', 136.03351), ('average_q_func1_loss', 3.4883936405181886), ('average_q_func2_loss', 3.5200103485584258), ('n_updates', 147001), ('average_entropy', -3.0326228), ('temperature', 0.03970174491405487)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:158000 episode:1328 last_R: 457.7926706659912 average_R:593.1784532983709\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.40231), ('average_q2', 134.54236), ('average_q_func1_loss', 3.5978124940395357), ('average_q_func2_loss', 3.835923351049423), ('n_updates', 148001), ('average_entropy', -2.9220054), ('temperature', 0.03935098275542259)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:159000 episode:1328 last_R: 457.7926706659912 average_R:593.1784532983709\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.48466), ('average_q2', 134.5251), ('average_q_func1_loss', 3.765921580791473), ('average_q_func2_loss', 4.448687545061111), ('n_updates', 149001), ('average_entropy', -3.1764326), ('temperature', 0.03900614008307457)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:160000 episode:1333 last_R: 763.1250659597991 average_R:602.2987798902801\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 133.5158), ('average_q2', 133.39319), ('average_q_func1_loss', 4.886731301546097), ('average_q_func2_loss', 3.875376740694046), ('n_updates', 150001), ('average_entropy', -2.994773), ('temperature', 0.03907419368624687)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 204 R: 369.74828399840726\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 202 R: 380.0884227742549\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 278 R: 553.9828075517073\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 117 R: 185.05096211686154\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 116 R: 184.051088083885\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 619 R: 1417.748231827416\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 338 R: 685.6448660806594\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 117 R: 185.9756634072921\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 432 R: 846.0107332724983\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 288 R: 594.1516953021311\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:161000 episode:1337 last_R: 728.0942115852176 average_R:612.2861005406287\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 133.17894), ('average_q2', 133.1363), ('average_q_func1_loss', 4.3205041146278385), ('average_q_func2_loss', 4.039073075056076), ('n_updates', 151001), ('average_entropy', -3.0203621), ('temperature', 0.040308207273483276)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:162000 episode:1339 last_R: 571.2719063379012 average_R:622.62655349529\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 133.16492), ('average_q2', 133.0846), ('average_q_func1_loss', 3.7694367599487304), ('average_q_func2_loss', 4.024208787679672), ('n_updates', 152001), ('average_entropy', -2.9933052), ('temperature', 0.041141677647829056)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:163000 episode:1343 last_R: 789.901531920442 average_R:625.8206913905782\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.1088), ('average_q2', 134.24669), ('average_q_func1_loss', 3.542698533535004), ('average_q_func2_loss', 3.573993453979492), ('n_updates', 153001), ('average_entropy', -2.941477), ('temperature', 0.04310907796025276)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:164000 episode:1345 last_R: 388.2296550638843 average_R:634.0400799385919\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.72675), ('average_q2', 135.6872), ('average_q_func1_loss', 4.129427500963211), ('average_q_func2_loss', 4.119296879768371), ('n_updates', 154001), ('average_entropy', -3.111874), ('temperature', 0.043204259127378464)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:165000 episode:1349 last_R: 601.9725178793738 average_R:637.060980585585\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.45981), ('average_q2', 135.80037), ('average_q_func1_loss', 4.728984376192093), ('average_q_func2_loss', 4.789812616109848), ('n_updates', 155001), ('average_entropy', -2.7925327), ('temperature', 0.04145568236708641)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 197 R: 419.91754831116486\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 471 R: 1087.3446812826876\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 284 R: 649.7811448852653\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 338 R: 885.3398513375108\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 267 R: 617.2472218021963\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 242 R: 508.82814619073855\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 339 R: 912.569049627645\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 229 R: 419.9902208678061\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 250 R: 565.2693809337023\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 360 R: 927.7138492245087\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:166000 episode:1353 last_R: 530.693223220255 average_R:646.0365515396846\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.7832), ('average_q2', 137.74413), ('average_q_func1_loss', 3.951127861738205), ('average_q_func2_loss', 3.9306385779380797), ('n_updates', 156001), ('average_entropy', -3.0466049), ('temperature', 0.04285768046975136)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:167000 episode:1357 last_R: 529.7228645584172 average_R:635.4217934678074\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.34297), ('average_q2', 137.37419), ('average_q_func1_loss', 4.425586314201355), ('average_q_func2_loss', 3.5650911462306976), ('n_updates', 157001), ('average_entropy', -3.1255727), ('temperature', 0.043722402304410934)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:168000 episode:1362 last_R: 561.33739358568 average_R:630.2179510914738\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.4706), ('average_q2', 135.34041), ('average_q_func1_loss', 4.087250384092331), ('average_q_func2_loss', 3.9296580135822294), ('n_updates', 158001), ('average_entropy', -2.8772593), ('temperature', 0.04349462687969208)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:169000 episode:1367 last_R: 673.4139111079008 average_R:640.8840443250142\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.82368), ('average_q2', 134.95558), ('average_q_func1_loss', 4.2564940512180325), ('average_q_func2_loss', 4.3681750178337095), ('n_updates', 159001), ('average_entropy', -3.0291076), ('temperature', 0.042489275336265564)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:170000 episode:1371 last_R: 511.1564983957315 average_R:633.9416621333677\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.29466), ('average_q2', 132.12305), ('average_q_func1_loss', 4.1436090397834775), ('average_q_func2_loss', 4.432747761011123), ('n_updates', 160001), ('average_entropy', -3.0699532), ('temperature', 0.041109848767519)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 317 R: 762.1853060305675\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 426 R: 939.2027879291871\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 578 R: 1380.7566271870867\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 336 R: 750.1507144469949\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 380 R: 852.3782126366574\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 202 R: 394.27122386485877\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 295 R: 688.8505221030291\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 251 R: 531.4692052860706\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 302 R: 684.6392693831008\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 303 R: 722.5288403792166\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:171000 episode:1373 last_R: 373.5440747546859 average_R:633.290355456771\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.97919), ('average_q2', 135.18404), ('average_q_func1_loss', 5.337380175590515), ('average_q_func2_loss', 4.296363447904587), ('n_updates', 161001), ('average_entropy', -3.3883731), ('temperature', 0.04053371027112007)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:172000 episode:1377 last_R: 670.8702311753909 average_R:633.0738217987279\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.46196), ('average_q2', 137.47574), ('average_q_func1_loss', 4.3038513886928556), ('average_q_func2_loss', 4.555117074251175), ('n_updates', 162001), ('average_entropy', -3.0173624), ('temperature', 0.04169956222176552)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:173000 episode:1382 last_R: 247.28270469054766 average_R:628.0596854880838\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.57503), ('average_q2', 132.63782), ('average_q_func1_loss', 4.573251039981842), ('average_q_func2_loss', 4.376146588325501), ('n_updates', 163001), ('average_entropy', -3.007485), ('temperature', 0.04147337004542351)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:174000 episode:1385 last_R: 525.7589174384038 average_R:625.3467037470529\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.14578), ('average_q2', 138.25266), ('average_q_func1_loss', 3.8340971493721008), ('average_q_func2_loss', 4.242626446485519), ('n_updates', 164001), ('average_entropy', -2.9543884), ('temperature', 0.040678124874830246)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:175000 episode:1389 last_R: 1057.1865952870667 average_R:626.3604441005696\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.3425), ('average_q2', 137.45187), ('average_q_func1_loss', 4.008537385463715), ('average_q_func2_loss', 3.8664562237262725), ('n_updates', 165001), ('average_entropy', -2.8776553), ('temperature', 0.04122294485569)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 255 R: 550.6270426835723\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 234 R: 541.8951693097083\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 226 R: 485.8213770241006\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 193 R: 397.9825928118213\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 372 R: 946.5970532717812\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 169 R: 327.9480386140189\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 180 R: 349.8216733387314\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 303 R: 759.0429979312603\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 183 R: 349.96931021706143\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 248 R: 569.0692923926201\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:176000 episode:1393 last_R: 479.35417974874264 average_R:627.7281777467567\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.83539), ('average_q2', 137.78062), ('average_q_func1_loss', 4.679618365764618), ('average_q_func2_loss', 4.820220556259155), ('n_updates', 166001), ('average_entropy', -3.0553198), ('temperature', 0.04049830883741379)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:177000 episode:1398 last_R: 512.7106710791523 average_R:617.8707643027413\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.34518), ('average_q2', 137.25925), ('average_q_func1_loss', 3.7924867987632753), ('average_q_func2_loss', 3.3529839980602265), ('n_updates', 167001), ('average_entropy', -2.9637637), ('temperature', 0.039605725556612015)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:178000 episode:1401 last_R: 500.6853761235982 average_R:615.5545458490758\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.92516), ('average_q2', 136.8031), ('average_q_func1_loss', 3.863657622337341), ('average_q_func2_loss', 4.159884463548661), ('n_updates', 168001), ('average_entropy', -3.0548835), ('temperature', 0.03837030008435249)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:179000 episode:1405 last_R: 782.2633323626444 average_R:606.2428261980024\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.2506), ('average_q2', 138.98114), ('average_q_func1_loss', 3.840159275531769), ('average_q_func2_loss', 3.930342024564743), ('n_updates', 169001), ('average_entropy', -3.182648), ('temperature', 0.03847441077232361)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:180000 episode:1410 last_R: 652.1200413509863 average_R:606.7825583708104\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.11264), ('average_q2', 137.29948), ('average_q_func1_loss', 4.449502197504043), ('average_q_func2_loss', 4.3883873963356015), ('n_updates', 170001), ('average_entropy', -2.98425), ('temperature', 0.03798291087150574)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 341 R: 817.046602188173\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 275 R: 663.9190460152932\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 280 R: 690.8990871624874\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 358 R: 794.6807956293231\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 346 R: 840.6334673005349\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 309 R: 692.2269443993473\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 216 R: 385.92784374343995\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 277 R: 687.8731144500042\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 285 R: 717.8747767329114\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 269 R: 571.710727304228\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:181000 episode:1412 last_R: 505.1876867943307 average_R:601.261790918254\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.11264), ('average_q2', 139.22954), ('average_q_func1_loss', 5.3305093908309935), ('average_q_func2_loss', 4.471091408729553), ('n_updates', 171001), ('average_entropy', -2.7561002), ('temperature', 0.03944118693470955)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:182000 episode:1416 last_R: 690.1107810189673 average_R:599.9538941415786\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.89569), ('average_q2', 137.15479), ('average_q_func1_loss', 4.853701710700989), ('average_q_func2_loss', 4.253816843032837), ('n_updates', 172001), ('average_entropy', -3.0182672), ('temperature', 0.03964974731206894)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:183000 episode:1419 last_R: 701.2623191826161 average_R:596.3699086998338\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.48077), ('average_q2', 138.57132), ('average_q_func1_loss', 3.929174745082855), ('average_q_func2_loss', 3.7800142085552215), ('n_updates', 173001), ('average_entropy', -2.8892155), ('temperature', 0.03867575153708458)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:184000 episode:1423 last_R: 733.1314772861724 average_R:596.4718538251844\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.57246), ('average_q2', 137.42393), ('average_q_func1_loss', 4.241274209022522), ('average_q_func2_loss', 4.106973428726196), ('n_updates', 174001), ('average_entropy', -3.0672348), ('temperature', 0.03903960436582565)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:185000 episode:1427 last_R: 1000.7858270964622 average_R:590.9852631002093\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.13066), ('average_q2', 139.0871), ('average_q_func1_loss', 3.9007810282707216), ('average_q_func2_loss', 3.7256868255138396), ('n_updates', 175001), ('average_entropy', -2.9909914), ('temperature', 0.03913675248622894)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 504 R: 1227.0313283459107\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 155 R: 267.05512577379443\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 485 R: 1215.9053883700603\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 150 R: 256.84222776268354\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 97 R: 145.24977219604972\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 98 R: 149.0858940504793\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 94 R: 141.277460797049\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 90 R: 137.27558481758365\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 93 R: 140.902156217188\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 137 R: 232.3378392680665\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:186000 episode:1432 last_R: 258.25441353257014 average_R:587.6108920871906\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.32486), ('average_q2', 138.66653), ('average_q_func1_loss', 3.9841678631305695), ('average_q_func2_loss', 3.5361003375053404), ('n_updates', 176001), ('average_entropy', -3.0585341), ('temperature', 0.040122732520103455)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:187000 episode:1434 last_R: 647.761691872388 average_R:582.6413684849192\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.82365), ('average_q2', 138.90631), ('average_q_func1_loss', 4.799118995666504), ('average_q_func2_loss', 5.653991000652313), ('n_updates', 177001), ('average_entropy', -2.9635134), ('temperature', 0.0399521067738533)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:188000 episode:1437 last_R: 761.2433518871771 average_R:590.0075148351846\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.934), ('average_q2', 140.23184), ('average_q_func1_loss', 4.63047888636589), ('average_q_func2_loss', 4.153978366851806), ('n_updates', 178001), ('average_entropy', -3.0404902), ('temperature', 0.04045068845152855)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:189000 episode:1441 last_R: 849.9533119300115 average_R:584.2814049356593\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.74957), ('average_q2', 135.82121), ('average_q_func1_loss', 4.493585792779922), ('average_q_func2_loss', 4.129123519659043), ('n_updates', 179001), ('average_entropy', -3.2076378), ('temperature', 0.04148515686392784)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:190000 episode:1445 last_R: 611.1311934302715 average_R:582.281845240523\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.68578), ('average_q2', 138.89192), ('average_q_func1_loss', 4.1713090264797215), ('average_q_func2_loss', 3.8357397544384004), ('n_updates', 180001), ('average_entropy', -2.9474394), ('temperature', 0.041888974606990814)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 211 R: 380.2954328685492\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 304 R: 728.4655880287207\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 242 R: 546.2439033791824\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 282 R: 693.9324343318003\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 349 R: 846.1695582292741\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 263 R: 628.2482336062241\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 287 R: 703.3396713404076\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 321 R: 728.6023689325084\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 222 R: 404.0079939125269\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 245 R: 553.4575225854978\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:191000 episode:1448 last_R: 508.17921715020407 average_R:577.4557241246351\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.53445), ('average_q2', 139.65672), ('average_q_func1_loss', 4.722422164678574), ('average_q_func2_loss', 4.413037650585174), ('n_updates', 181001), ('average_entropy', -2.9201167), ('temperature', 0.040497664362192154)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:192000 episode:1453 last_R: 619.8591478088026 average_R:574.9739311362259\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.57475), ('average_q2', 140.50642), ('average_q_func1_loss', 4.3099320542812345), ('average_q_func2_loss', 4.465323239564896), ('n_updates', 182001), ('average_entropy', -2.9729671), ('temperature', 0.040792595595121384)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:193000 episode:1454 last_R: 170.74966849937144 average_R:568.2999370810143\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.97897), ('average_q2', 141.10147), ('average_q_func1_loss', 3.8633521616458895), ('average_q_func2_loss', 3.538406742811203), ('n_updates', 183001), ('average_entropy', -3.0230484), ('temperature', 0.04043152928352356)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:194000 episode:1459 last_R: 947.9200422170347 average_R:586.5454299836691\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.27776), ('average_q2', 138.43402), ('average_q_func1_loss', 5.96928302526474), ('average_q_func2_loss', 5.287146208286285), ('n_updates', 184001), ('average_entropy', -3.1218174), ('temperature', 0.040150731801986694)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:195000 episode:1460 last_R: 627.4660919179324 average_R:587.6099776048018\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.23163), ('average_q2', 140.26744), ('average_q_func1_loss', 5.126499309539795), ('average_q_func2_loss', 4.717438836097717), ('n_updates', 185001), ('average_entropy', -3.0411444), ('temperature', 0.04212697595357895)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 369 R: 835.7976091152693\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 448 R: 1111.8481324436293\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 741 R: 1951.0744735199116\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 373 R: 806.5385921766035\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 375 R: 816.388047716361\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 134 R: 221.87820342657693\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 279 R: 547.6459558035889\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 352 R: 703.0756321035066\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 320 R: 639.4031039454652\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 194 R: 354.7103936726099\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:196000 episode:1464 last_R: 516.8178828673259 average_R:595.7753269848458\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.47394), ('average_q2', 140.61389), ('average_q_func1_loss', 4.916762382984161), ('average_q_func2_loss', 3.950238745212555), ('n_updates', 186001), ('average_entropy', -2.9564123), ('temperature', 0.04066982865333557)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:197000 episode:1467 last_R: 619.3690877912527 average_R:596.9061295332858\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.6592), ('average_q2', 138.65022), ('average_q_func1_loss', 4.707547525167465), ('average_q_func2_loss', 4.506037594079971), ('n_updates', 187001), ('average_entropy', -2.816152), ('temperature', 0.03961314260959625)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:198000 episode:1470 last_R: 669.4758381508328 average_R:600.0230819625118\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.53426), ('average_q2', 139.6202), ('average_q_func1_loss', 4.283775215148926), ('average_q_func2_loss', 3.9811741876602174), ('n_updates', 188001), ('average_entropy', -2.7679033), ('temperature', 0.03858475387096405)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:199000 episode:1473 last_R: 1007.6306802172236 average_R:607.71452540778\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.08685), ('average_q2', 138.78995), ('average_q_func1_loss', 4.056210761070251), ('average_q_func2_loss', 4.003126378059387), ('n_updates', 189001), ('average_entropy', -3.0971024), ('temperature', 0.038063712418079376)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:200000 episode:1477 last_R: 736.1375323069873 average_R:611.4787097190806\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.018), ('average_q2', 144.2203), ('average_q_func1_loss', 4.732076562643051), ('average_q_func2_loss', 3.997827670574188), ('n_updates', 190001), ('average_entropy', -2.877335), ('temperature', 0.03895178437232971)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 357 R: 826.4885657448093\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 330 R: 711.2840958203025\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 314 R: 718.0748759816562\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 306 R: 745.4897156784943\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 442 R: 1140.2165490718496\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 327 R: 721.9237620341528\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 296 R: 621.4863339495053\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 304 R: 652.9750134448935\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 280 R: 680.9862123713255\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 265 R: 522.2408876272481\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:201000 episode:1482 last_R: 370.33599763008704 average_R:615.8282200986479\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.5427), ('average_q2', 140.40178), ('average_q_func1_loss', 4.327159736156464), ('average_q_func2_loss', 4.280933928489685), ('n_updates', 191001), ('average_entropy', -2.9087563), ('temperature', 0.0402696467936039)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:202000 episode:1486 last_R: 740.4349938555398 average_R:617.6943320450741\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.50903), ('average_q2', 139.5744), ('average_q_func1_loss', 4.7964972448348995), ('average_q_func2_loss', 4.10515038728714), ('n_updates', 192001), ('average_entropy', -3.1212363), ('temperature', 0.041090670973062515)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:203000 episode:1489 last_R: 948.0324828695094 average_R:611.2220160727777\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.2239), ('average_q2', 143.29082), ('average_q_func1_loss', 5.167338126897812), ('average_q_func2_loss', 4.6062899386882785), ('n_updates', 193001), ('average_entropy', -2.895191), ('temperature', 0.039790038019418716)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:204000 episode:1493 last_R: 675.6625501751075 average_R:615.6195990441327\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.29259), ('average_q2', 144.48962), ('average_q_func1_loss', 4.455656638145447), ('average_q_func2_loss', 4.614498074054718), ('n_updates', 194001), ('average_entropy', -2.9944093), ('temperature', 0.03984967991709709)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:205000 episode:1497 last_R: 534.6510816302293 average_R:619.565271737006\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.18181), ('average_q2', 140.26672), ('average_q_func1_loss', 3.531741646528244), ('average_q_func2_loss', 3.899861401319504), ('n_updates', 195001), ('average_entropy', -2.9721007), ('temperature', 0.039328522980213165)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 247 R: 551.0425572963571\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 249 R: 546.6613575848746\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 246 R: 545.1290196193531\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 244 R: 545.6637838406436\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 248 R: 549.5603075505372\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 250 R: 534.7561735417394\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 247 R: 542.7992660100815\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 246 R: 548.1387145989033\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 255 R: 577.997252223869\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 249 R: 550.9881713089533\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:206000 episode:1499 last_R: 504.04034345404517 average_R:622.0609550829945\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.45512), ('average_q2', 144.77379), ('average_q_func1_loss', 3.8883645761013033), ('average_q_func2_loss', 3.7902312123775483), ('n_updates', 196001), ('average_entropy', -3.221436), ('temperature', 0.038649220019578934)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:207000 episode:1504 last_R: 473.5825924890168 average_R:635.3812408924279\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.35245), ('average_q2', 142.34918), ('average_q_func1_loss', 4.606304317712784), ('average_q_func2_loss', 4.0656435012817385), ('n_updates', 197001), ('average_entropy', -3.1880722), ('temperature', 0.03833407908678055)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:208000 episode:1508 last_R: 620.2037738137295 average_R:633.2482593723958\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.00037), ('average_q2', 140.95009), ('average_q_func1_loss', 4.280695278644561), ('average_q_func2_loss', 4.152746511697769), ('n_updates', 198001), ('average_entropy', -3.0597565), ('temperature', 0.03850765526294708)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:209000 episode:1513 last_R: 628.679153227602 average_R:634.7076367565467\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.18207), ('average_q2', 143.19992), ('average_q_func1_loss', 3.873068675994873), ('average_q_func2_loss', 3.575856256484985), ('n_updates', 199001), ('average_entropy', -3.0171814), ('temperature', 0.03869682922959328)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:210000 episode:1516 last_R: 410.27868909616643 average_R:625.3853548051671\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.53523), ('average_q2', 144.85953), ('average_q_func1_loss', 3.756942036151886), ('average_q_func2_loss', 3.769896490573883), ('n_updates', 200001), ('average_entropy', -3.1269288), ('temperature', 0.03777989745140076)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 278 R: 647.4530066521319\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 371 R: 913.5075179615507\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 282 R: 766.1133848816585\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 333 R: 792.6304246760018\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 246 R: 567.4196650135018\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 287 R: 693.2872506990317\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 188 R: 379.8641993798423\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 261 R: 547.7937792411917\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 296 R: 817.6791195490076\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 343 R: 823.3623981755817\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:211000 episode:1521 last_R: 611.3734598163165 average_R:627.2799241818493\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.27928), ('average_q2', 143.56465), ('average_q_func1_loss', 4.301580284833908), ('average_q_func2_loss', 3.7212235140800476), ('n_updates', 201001), ('average_entropy', -3.1356955), ('temperature', 0.03867587819695473)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:212000 episode:1525 last_R: 518.4781035569271 average_R:628.369014793224\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.62659), ('average_q2', 145.68748), ('average_q_func1_loss', 4.280489928722382), ('average_q_func2_loss', 4.029286216497422), ('n_updates', 202001), ('average_entropy', -3.031516), ('temperature', 0.03843066096305847)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:213000 episode:1529 last_R: 538.8489462330351 average_R:624.2131888467991\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.62538), ('average_q2', 143.7126), ('average_q_func1_loss', 4.825125991106034), ('average_q_func2_loss', 4.160993622541428), ('n_updates', 203001), ('average_entropy', -3.0190933), ('temperature', 0.037858977913856506)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:214000 episode:1532 last_R: 506.89300729740745 average_R:627.252748081224\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.09474), ('average_q2', 143.18288), ('average_q_func1_loss', 4.250769072771073), ('average_q_func2_loss', 4.006063101291656), ('n_updates', 204001), ('average_entropy', -3.0697687), ('temperature', 0.03706108778715134)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:215000 episode:1536 last_R: 402.9506358751877 average_R:624.1112310157251\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.31372), ('average_q2', 144.28423), ('average_q_func1_loss', 4.78781834602356), ('average_q_func2_loss', 4.552818199396134), ('n_updates', 205001), ('average_entropy', -3.1770918), ('temperature', 0.03781251236796379)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 314 R: 737.6844647388929\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 189 R: 458.1103391838216\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 291 R: 744.264270924201\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 278 R: 700.6211853558768\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 151 R: 314.34278749906525\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 308 R: 579.0557253741663\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 279 R: 706.9187985843454\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 230 R: 545.3970413654554\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 322 R: 814.8877752001181\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 232 R: 555.4358925580055\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:216000 episode:1539 last_R: 843.2526412051972 average_R:621.8034469623233\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.6112), ('average_q2', 143.3402), ('average_q_func1_loss', 3.8423924481868745), ('average_q_func2_loss', 3.4586549961566924), ('n_updates', 206001), ('average_entropy', -2.8877652), ('temperature', 0.037618327885866165)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:217000 episode:1542 last_R: 699.5745295239058 average_R:629.5472607297993\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.02538), ('average_q2', 141.85037), ('average_q_func1_loss', 4.055715336799621), ('average_q_func2_loss', 3.68716570019722), ('n_updates', 207001), ('average_entropy', -2.9739857), ('temperature', 0.038133420050144196)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:218000 episode:1545 last_R: 741.6371111567843 average_R:628.1036923571035\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.79832), ('average_q2', 145.40967), ('average_q_func1_loss', 4.644609203338623), ('average_q_func2_loss', 4.17211668252945), ('n_updates', 208001), ('average_entropy', -2.9368212), ('temperature', 0.0379185788333416)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:219000 episode:1549 last_R: 693.7489496514822 average_R:637.4415781233431\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.10298), ('average_q2', 147.32568), ('average_q_func1_loss', 3.997484629154205), ('average_q_func2_loss', 3.676045359373093), ('n_updates', 209001), ('average_entropy', -3.0766244), ('temperature', 0.03821121156215668)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:220000 episode:1555 last_R: 872.4864043415129 average_R:637.3115578459177\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.93517), ('average_q2', 145.14822), ('average_q_func1_loss', 3.9286119747161865), ('average_q_func2_loss', 3.970077773332596), ('n_updates', 210001), ('average_entropy', -3.2669291), ('temperature', 0.03844602778553963)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 307 R: 767.3609698541908\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 240 R: 531.6187423032616\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 237 R: 509.6976630288811\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 280 R: 648.7399891537325\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 281 R: 666.0558279268586\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 310 R: 795.4057409984615\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 285 R: 651.8128807312213\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 246 R: 553.4741547614195\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 236 R: 507.7428218814886\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 224 R: 469.25538459362144\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:221000 episode:1559 last_R: 625.9657522590109 average_R:623.47720428685\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.04163), ('average_q2', 144.11763), ('average_q_func1_loss', 4.606277927160263), ('average_q_func2_loss', 3.5402003014087677), ('n_updates', 211001), ('average_entropy', -3.0217822), ('temperature', 0.03834599256515503)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:222000 episode:1560 last_R: 591.1917296843383 average_R:623.1144606645141\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.45468), ('average_q2', 144.1536), ('average_q_func1_loss', 4.069724907875061), ('average_q_func2_loss', 3.8300676703453065), ('n_updates', 212001), ('average_entropy', -2.9182007), ('temperature', 0.037909649312496185)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:223000 episode:1565 last_R: 992.855227186238 average_R:623.0379101813509\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.23521), ('average_q2', 144.28891), ('average_q_func1_loss', 3.7897408306598663), ('average_q_func2_loss', 3.6474963557720184), ('n_updates', 213001), ('average_entropy', -3.0344598), ('temperature', 0.03851127251982689)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:224000 episode:1568 last_R: 671.523137078167 average_R:623.1685470550334\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.55972), ('average_q2', 146.48294), ('average_q_func1_loss', 3.7507322180271148), ('average_q_func2_loss', 3.5854035353660585), ('n_updates', 214001), ('average_entropy', -2.9838927), ('temperature', 0.03830912709236145)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:225000 episode:1572 last_R: 740.6377972961617 average_R:626.0080452831473\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.06544), ('average_q2', 144.9078), ('average_q_func1_loss', 4.900830870866775), ('average_q_func2_loss', 4.293763991594314), ('n_updates', 215001), ('average_entropy', -3.1093893), ('temperature', 0.03937806189060211)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 201 R: 484.83602394342824\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 262 R: 651.3777005538504\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 337 R: 790.4312067120909\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 341 R: 816.732018474224\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 346 R: 797.7835168140662\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 340 R: 759.4380846384157\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 351 R: 814.4214652723641\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 210 R: 500.13236319601765\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 228 R: 553.2051008742554\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 302 R: 751.0062771054438\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:226000 episode:1577 last_R: 702.9841863210161 average_R:622.626894257734\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.80652), ('average_q2', 148.71608), ('average_q_func1_loss', 3.997973848581314), ('average_q_func2_loss', 3.9303492534160616), ('n_updates', 216001), ('average_entropy', -3.0159333), ('temperature', 0.039654411375522614)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:227000 episode:1580 last_R: 327.8684191556043 average_R:619.5274147965732\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.74619), ('average_q2', 144.81128), ('average_q_func1_loss', 3.7495993316173553), ('average_q_func2_loss', 3.5830073559284212), ('n_updates', 217001), ('average_entropy', -3.205749), ('temperature', 0.040733397006988525)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:228000 episode:1584 last_R: 474.2922409074753 average_R:626.4575541742952\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.43468), ('average_q2', 145.54503), ('average_q_func1_loss', 4.873078194856643), ('average_q_func2_loss', 4.296981055736541), ('n_updates', 218001), ('average_entropy', -2.668995), ('temperature', 0.04012759402394295)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:229000 episode:1586 last_R: 358.18625288440785 average_R:623.0186629508843\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.61424), ('average_q2', 150.56029), ('average_q_func1_loss', 3.8926098310947417), ('average_q_func2_loss', 3.7408766555786133), ('n_updates', 219001), ('average_entropy', -3.0544863), ('temperature', 0.039709024131298065)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:230000 episode:1591 last_R: 421.6211111941477 average_R:622.4897707614214\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.40225), ('average_q2', 147.41922), ('average_q_func1_loss', 5.011112734079361), ('average_q_func2_loss', 4.024296263456344), ('n_updates', 220001), ('average_entropy', -3.1855047), ('temperature', 0.03942256048321724)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 167 R: 320.6152062095752\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 219 R: 589.5610724063117\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 229 R: 574.5408717291454\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 159 R: 351.4739469041913\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 222 R: 471.7527583650394\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 283 R: 706.5928010614505\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 209 R: 448.14782469194813\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 384 R: 963.300734495929\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 234 R: 565.1143523344333\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 325 R: 775.0217755907022\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:231000 episode:1593 last_R: 558.6857083320606 average_R:618.9854137607992\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.05307), ('average_q2', 145.57864), ('average_q_func1_loss', 5.455456211566925), ('average_q_func2_loss', 4.680130367279053), ('n_updates', 221001), ('average_entropy', -3.0543883), ('temperature', 0.0389992780983448)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:232000 episode:1597 last_R: 822.0479831794162 average_R:630.770939670204\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.2659), ('average_q2', 149.00487), ('average_q_func1_loss', 3.927980320453644), ('average_q_func2_loss', 3.686038165092468), ('n_updates', 222001), ('average_entropy', -2.803749), ('temperature', 0.03848530724644661)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:233000 episode:1600 last_R: 890.2252687854926 average_R:630.346271486186\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.48674), ('average_q2', 147.73193), ('average_q_func1_loss', 4.413249698877334), ('average_q_func2_loss', 4.098137227296829), ('n_updates', 223001), ('average_entropy', -2.7937195), ('temperature', 0.03872309625148773)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:234000 episode:1605 last_R: 323.8380185013496 average_R:628.7428257628712\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.76128), ('average_q2', 148.7884), ('average_q_func1_loss', 5.060013490915298), ('average_q_func2_loss', 4.882719483375549), ('n_updates', 224001), ('average_entropy', -2.9417388), ('temperature', 0.04070558026432991)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:235000 episode:1607 last_R: 519.8523592316187 average_R:628.2695791165543\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.35207), ('average_q2', 147.54369), ('average_q_func1_loss', 4.4558722448349), ('average_q_func2_loss', 3.856012661457062), ('n_updates', 225001), ('average_entropy', -2.922999), ('temperature', 0.04051511362195015)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 291 R: 681.7540815495697\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 247 R: 621.5312160998491\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 344 R: 850.965500459214\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 276 R: 658.1610405554636\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 280 R: 697.866348635335\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 221 R: 480.6842027959103\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 336 R: 861.9270767724511\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 236 R: 501.7618465065709\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 335 R: 843.6946319911411\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 294 R: 723.9581248492825\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:236000 episode:1609 last_R: 944.8727629657064 average_R:632.6681281761406\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.7302), ('average_q2', 149.90475), ('average_q_func1_loss', 4.030867290496826), ('average_q_func2_loss', 4.149975181818008), ('n_updates', 226001), ('average_entropy', -2.9912455), ('temperature', 0.040317192673683167)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:237000 episode:1613 last_R: 1041.8887918863281 average_R:652.7300991967942\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.1066), ('average_q2', 147.35335), ('average_q_func1_loss', 3.703377952575684), ('average_q_func2_loss', 3.339588761329651), ('n_updates', 227001), ('average_entropy', -3.0058086), ('temperature', 0.04213108494877815)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:238000 episode:1616 last_R: 739.7392505079225 average_R:655.0480098715328\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.69582), ('average_q2', 148.76997), ('average_q_func1_loss', 4.349379665851593), ('average_q_func2_loss', 4.157494398355484), ('n_updates', 228001), ('average_entropy', -3.0944796), ('temperature', 0.04089142754673958)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:239000 episode:1620 last_R: 765.8772710861531 average_R:664.4773074013674\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.2711), ('average_q2', 148.69589), ('average_q_func1_loss', 4.751580151319504), ('average_q_func2_loss', 3.673892492055893), ('n_updates', 229001), ('average_entropy', -2.8932672), ('temperature', 0.040709055960178375)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:240000 episode:1623 last_R: 1527.9220916970908 average_R:673.8943381438465\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.36266), ('average_q2', 150.34969), ('average_q_func1_loss', 5.206658687591553), ('average_q_func2_loss', 4.452956287860871), ('n_updates', 230001), ('average_entropy', -3.1827295), ('temperature', 0.03979441151022911)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 362 R: 818.2945615588717\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 335 R: 802.013436115377\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 381 R: 842.6514244493849\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 321 R: 590.9143214927545\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 191 R: 459.50026814044986\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 267 R: 535.3459320929254\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 241 R: 453.22637919344083\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 321 R: 692.0704034409742\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 208 R: 483.9128882513582\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 250 R: 469.1596994369145\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:241000 episode:1625 last_R: 815.9241690146961 average_R:675.6034166692076\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.8153), ('average_q2', 148.74171), ('average_q_func1_loss', 5.361882246732712), ('average_q_func2_loss', 5.407936152219772), ('n_updates', 231001), ('average_entropy', -2.9374437), ('temperature', 0.04018985480070114)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:242000 episode:1630 last_R: 861.5626922032309 average_R:679.7019089875816\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.1092), ('average_q2', 149.06853), ('average_q_func1_loss', 4.237529406547546), ('average_q_func2_loss', 3.8532255125045776), ('n_updates', 232001), ('average_entropy', -2.9054563), ('temperature', 0.03925091400742531)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:243000 episode:1634 last_R: 687.943390159313 average_R:676.8338128468536\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.87488), ('average_q2', 149.85141), ('average_q_func1_loss', 3.753828078508377), ('average_q_func2_loss', 3.319598916769028), ('n_updates', 233001), ('average_entropy', -2.9615216), ('temperature', 0.038908250629901886)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:244000 episode:1636 last_R: 542.0788523746797 average_R:678.2804935904999\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.35834), ('average_q2', 149.43481), ('average_q_func1_loss', 4.025908548831939), ('average_q_func2_loss', 3.936183743476868), ('n_updates', 234001), ('average_entropy', -3.0585458), ('temperature', 0.03907710686326027)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:245000 episode:1640 last_R: 524.9802719269258 average_R:679.6639008549931\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.82103), ('average_q2', 148.57439), ('average_q_func1_loss', 4.855163756608963), ('average_q_func2_loss', 3.941081794500351), ('n_updates', 235001), ('average_entropy', -3.1037686), ('temperature', 0.03901536762714386)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 289 R: 584.5928672986054\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 234 R: 530.7491530452896\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 363 R: 820.9667887137377\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 343 R: 831.4838632293227\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 441 R: 1033.00427538316\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 235 R: 530.2080687908136\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 231 R: 523.7113058765761\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 324 R: 754.0478194743217\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 346 R: 634.693941814974\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 345 R: 840.1330175188872\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:246000 episode:1643 last_R: 958.8687532903788 average_R:685.3129627780736\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.77687), ('average_q2', 149.57191), ('average_q_func1_loss', 4.170060801506042), ('average_q_func2_loss', 4.746324025392532), ('n_updates', 236001), ('average_entropy', -3.0705042), ('temperature', 0.03941044956445694)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:247000 episode:1646 last_R: 746.3088191648505 average_R:682.2570097730879\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 153.63846), ('average_q2', 153.77919), ('average_q_func1_loss', 4.752680702209473), ('average_q_func2_loss', 4.003892862796784), ('n_updates', 237001), ('average_entropy', -2.826604), ('temperature', 0.039473921060562134)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:248000 episode:1650 last_R: 555.3396663040074 average_R:691.6423760492538\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.74374), ('average_q2', 149.77768), ('average_q_func1_loss', 3.859782181978226), ('average_q_func2_loss', 3.4159242510795593), ('n_updates', 238001), ('average_entropy', -3.127141), ('temperature', 0.0392322801053524)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:249000 episode:1652 last_R: 755.0598527635699 average_R:691.9886528108196\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.96373), ('average_q2', 149.99834), ('average_q_func1_loss', 3.6622344291210176), ('average_q_func2_loss', 3.548848630189896), ('n_updates', 239001), ('average_entropy', -3.1153495), ('temperature', 0.03808822110295296)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:250000 episode:1655 last_R: 773.1949855544702 average_R:694.92285704162\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.21826), ('average_q2', 150.25412), ('average_q_func1_loss', 3.9506277203559876), ('average_q_func2_loss', 3.5121162021160126), ('n_updates', 240001), ('average_entropy', -3.0948892), ('temperature', 0.039087433367967606)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 299 R: 747.0378451594221\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 386 R: 848.7442577274156\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 272 R: 685.1705655162219\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 350 R: 860.9628266731594\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 278 R: 685.7966021507093\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 371 R: 926.4105954337516\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 308 R: 796.6049855019418\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 310 R: 684.8010773023253\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 417 R: 1015.6536939717427\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 287 R: 601.5068771007022\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:251000 episode:1660 last_R: 969.2867888012765 average_R:701.4803150863909\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 152.22044), ('average_q2', 152.73637), ('average_q_func1_loss', 4.563736621141434), ('average_q_func2_loss', 3.911313707828522), ('n_updates', 241001), ('average_entropy', -3.118759), ('temperature', 0.03934190422296524)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:252000 episode:1664 last_R: 685.0751633555063 average_R:696.9311207867121\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.37775), ('average_q2', 147.51784), ('average_q_func1_loss', 3.788566782474518), ('average_q_func2_loss', 3.456684989929199), ('n_updates', 242001), ('average_entropy', -2.9269662), ('temperature', 0.03943932056427002)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:253000 episode:1668 last_R: 424.25518916889496 average_R:695.4291913141226\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.67934), ('average_q2', 150.58319), ('average_q_func1_loss', 3.625980190038681), ('average_q_func2_loss', 3.5001372754573823), ('n_updates', 243001), ('average_entropy', -3.0639453), ('temperature', 0.03913302719593048)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:254000 episode:1672 last_R: 372.59198861955474 average_R:688.2531156133145\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.85591), ('average_q2', 150.56538), ('average_q_func1_loss', 4.549062778949738), ('average_q_func2_loss', 3.9953792881965637), ('n_updates', 244001), ('average_entropy', -3.1252537), ('temperature', 0.038799770176410675)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:255000 episode:1675 last_R: 735.6102009501466 average_R:686.4515569226006\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.73799), ('average_q2', 151.98116), ('average_q_func1_loss', 4.5462106621265415), ('average_q_func2_loss', 4.2144965088367465), ('n_updates', 245001), ('average_entropy', -3.1372075), ('temperature', 0.03936179354786873)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 274 R: 709.7608032173156\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 227 R: 589.9641532626556\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 227 R: 601.5843492631973\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 235 R: 608.7722128266334\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 239 R: 621.6876070526584\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 300 R: 722.8212783871145\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 240 R: 629.2503772309929\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 235 R: 601.3875815974123\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 285 R: 733.0802321945374\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 292 R: 732.3652255725785\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:256000 episode:1678 last_R: 639.348454544167 average_R:694.1549714667337\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.78998), ('average_q2', 148.83073), ('average_q_func1_loss', 4.001244906187058), ('average_q_func2_loss', 3.7113645482063293), ('n_updates', 246001), ('average_entropy', -3.034272), ('temperature', 0.038326386362314224)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:257000 episode:1682 last_R: 541.0595586320791 average_R:699.2295695504946\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 152.52635), ('average_q2', 152.32576), ('average_q_func1_loss', 3.9401028609275817), ('average_q_func2_loss', 3.8956981456279753), ('n_updates', 247001), ('average_entropy', -2.8112855), ('temperature', 0.03927256539463997)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:258000 episode:1687 last_R: 1406.443596711813 average_R:709.2322695408069\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.45079), ('average_q2', 151.18484), ('average_q_func1_loss', 4.692424099445343), ('average_q_func2_loss', 3.861007066965103), ('n_updates', 248001), ('average_entropy', -3.1068096), ('temperature', 0.03864051401615143)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:259000 episode:1688 last_R: 658.1039903728332 average_R:709.8811090104437\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.5249), ('average_q2', 151.88437), ('average_q_func1_loss', 3.9659454345703127), ('average_q_func2_loss', 3.5874366331100465), ('n_updates', 249001), ('average_entropy', -3.0794683), ('temperature', 0.03916114196181297)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:260000 episode:1692 last_R: 728.7948301730976 average_R:712.6900464984158\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.62494), ('average_q2', 151.46378), ('average_q_func1_loss', 4.7090696108341215), ('average_q_func2_loss', 3.950421402454376), ('n_updates', 250001), ('average_entropy', -2.922812), ('temperature', 0.03889891132712364)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 284 R: 742.139244134194\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 301 R: 726.8433239349164\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 308 R: 782.9783535473239\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 365 R: 919.4136393532203\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 360 R: 918.3319306228697\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 295 R: 768.4860085034337\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 312 R: 815.2060871213604\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 326 R: 818.7234395348693\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 321 R: 825.9909656862199\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 319 R: 813.7309091360876\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:261000 episode:1697 last_R: 608.8847216319282 average_R:701.4908634735367\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 153.8984), ('average_q2', 154.13728), ('average_q_func1_loss', 3.9807940483093263), ('average_q_func2_loss', 3.641567989587784), ('n_updates', 251001), ('average_entropy', -2.8421807), ('temperature', 0.03806186094880104)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:262000 episode:1701 last_R: 540.1336555293768 average_R:700.3840138329634\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.51668), ('average_q2', 149.12244), ('average_q_func1_loss', 4.015098946094513), ('average_q_func2_loss', 3.655178577899933), ('n_updates', 252001), ('average_entropy', -2.9638429), ('temperature', 0.03903345391154289)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:263000 episode:1703 last_R: 473.4744757914757 average_R:699.2641911389811\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 152.41539), ('average_q2', 152.74666), ('average_q_func1_loss', 4.077855213880539), ('average_q_func2_loss', 3.8248554134368895), ('n_updates', 253001), ('average_entropy', -3.074812), ('temperature', 0.03920052573084831)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:264000 episode:1709 last_R: 167.84725265913195 average_R:694.5645186089\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.15912), ('average_q2', 150.86668), ('average_q_func1_loss', 3.4868450486660003), ('average_q_func2_loss', 3.340822248458862), ('n_updates', 254001), ('average_entropy', -3.1696124), ('temperature', 0.03951559215784073)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:265000 episode:1713 last_R: 469.2374705831693 average_R:672.6940992112656\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.66054), ('average_q2', 148.61845), ('average_q_func1_loss', 4.365562182664871), ('average_q_func2_loss', 3.8422410917282104), ('n_updates', 255001), ('average_entropy', -3.11442), ('temperature', 0.038173332810401917)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 303 R: 693.3161707834379\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 351 R: 836.3975448551289\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 250 R: 547.5233518154597\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 224 R: 440.3160999096272\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 270 R: 564.1785304943803\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 258 R: 473.14677164605234\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 239 R: 455.8403850786688\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 279 R: 494.73135720944424\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 285 R: 501.29236041262914\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 269 R: 583.6423050329236\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:266000 episode:1717 last_R: 496.8956776316858 average_R:671.8183623670898\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.08253), ('average_q2', 150.07524), ('average_q_func1_loss', 4.159539244174957), ('average_q_func2_loss', 3.929717597961426), ('n_updates', 256001), ('average_entropy', -2.821764), ('temperature', 0.03774541616439819)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:267000 episode:1721 last_R: 478.48542663801624 average_R:670.3262845901232\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.54556), ('average_q2', 149.5526), ('average_q_func1_loss', 4.850753345489502), ('average_q_func2_loss', 4.3468620908260345), ('n_updates', 257001), ('average_entropy', -2.9392567), ('temperature', 0.03911907598376274)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:268000 episode:1725 last_R: 678.7519818175556 average_R:659.5190539118377\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.6343), ('average_q2', 150.00447), ('average_q_func1_loss', 4.090282092094421), ('average_q_func2_loss', 3.739740334749222), ('n_updates', 258001), ('average_entropy', -3.0289047), ('temperature', 0.03995395079255104)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:269000 episode:1731 last_R: 573.5725801108894 average_R:650.778063863324\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 153.7122), ('average_q2', 153.66423), ('average_q_func1_loss', 4.080310986042023), ('average_q_func2_loss', 3.688048919439316), ('n_updates', 259001), ('average_entropy', -2.90517), ('temperature', 0.037529051303863525)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:270000 episode:1734 last_R: 419.5871434047205 average_R:643.6697261056164\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 153.36697), ('average_q2', 153.09337), ('average_q_func1_loss', 3.4001005136966707), ('average_q_func2_loss', 3.402152281999588), ('n_updates', 260001), ('average_entropy', -3.2319922), ('temperature', 0.037141308188438416)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 212 R: 427.1044150726385\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 590 R: 1195.0963919319388\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 156 R: 239.5569701700318\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 154 R: 243.522471466508\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 357 R: 774.1776410027584\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 360 R: 731.1216491475467\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 475 R: 792.8602855077777\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 255 R: 472.9646937325591\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 425 R: 912.2275902055467\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 433 R: 1050.9639002715928\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:271000 episode:1738 last_R: 461.3762102137632 average_R:636.5048198173246\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.55257), ('average_q2', 151.44962), ('average_q_func1_loss', 3.82712193608284), ('average_q_func2_loss', 3.4820338642597197), ('n_updates', 261001), ('average_entropy', -3.013285), ('temperature', 0.03858194872736931)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:272000 episode:1740 last_R: 655.4263545134326 average_R:636.0442326626288\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.56393), ('average_q2', 151.74329), ('average_q_func1_loss', 4.72033072590828), ('average_q_func2_loss', 4.436813939809799), ('n_updates', 262001), ('average_entropy', -3.1170557), ('temperature', 0.03915045037865639)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:273000 episode:1744 last_R: 921.0709459040652 average_R:635.9913795729278\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.85982), ('average_q2', 148.94904), ('average_q_func1_loss', 5.0543881762027745), ('average_q_func2_loss', 4.742245236635208), ('n_updates', 263001), ('average_entropy', -2.7739673), ('temperature', 0.03918680548667908)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:274000 episode:1747 last_R: 542.3029344765534 average_R:635.5693342672677\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 153.03906), ('average_q2', 152.92632), ('average_q_func1_loss', 4.911029361486435), ('average_q_func2_loss', 3.978969213962555), ('n_updates', 264001), ('average_entropy', -2.8771605), ('temperature', 0.03750831261277199)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:275000 episode:1749 last_R: 778.9185194490314 average_R:630.796413302489\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 152.93976), ('average_q2', 152.76361), ('average_q_func1_loss', 4.136323540210724), ('average_q_func2_loss', 3.853338816165924), ('n_updates', 265001), ('average_entropy', -3.0087612), ('temperature', 0.03694150969386101)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 379 R: 932.6233655930971\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 399 R: 820.1714621317749\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 217 R: 445.58968812187385\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 185 R: 352.22160984394947\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 254 R: 529.5774746674442\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 169 R: 298.57124421818554\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 450 R: 1033.0940436155872\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 185 R: 341.67000671663396\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 184 R: 333.3198202099764\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 180 R: 330.89607643659303\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:276000 episode:1752 last_R: 1068.8249892835454 average_R:638.0369266205602\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.77182), ('average_q2', 151.79051), ('average_q_func1_loss', 4.037445952892304), ('average_q_func2_loss', 3.459437495470047), ('n_updates', 266001), ('average_entropy', -3.1296484), ('temperature', 0.036826811730861664)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:277000 episode:1755 last_R: 668.3155927720213 average_R:640.3339831911097\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 153.82571), ('average_q2', 153.7133), ('average_q_func1_loss', 4.39288691163063), ('average_q_func2_loss', 3.8206517469882963), ('n_updates', 267001), ('average_entropy', -3.1936944), ('temperature', 0.03740550950169563)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:278000 episode:1758 last_R: 883.0460933057494 average_R:646.9345196967613\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 152.5792), ('average_q2', 152.694), ('average_q_func1_loss', 5.214695954322815), ('average_q_func2_loss', 4.622502527236938), ('n_updates', 268001), ('average_entropy', -3.0932841), ('temperature', 0.038976263254880905)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:279000 episode:1762 last_R: 818.791474809476 average_R:650.3137822841484\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 153.60869), ('average_q2', 153.3441), ('average_q_func1_loss', 4.031049429178238), ('average_q_func2_loss', 3.6827944493293763), ('n_updates', 269001), ('average_entropy', -2.9597921), ('temperature', 0.03830047324299812)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:280000 episode:1765 last_R: 623.2429418446251 average_R:644.8875138569497\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.48386), ('average_q2', 151.50305), ('average_q_func1_loss', 3.572449930906296), ('average_q_func2_loss', 3.886289020776749), ('n_updates', 270001), ('average_entropy', -2.9813538), ('temperature', 0.038222458213567734)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 189 R: 457.8052198123276\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 157 R: 319.0865711247599\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 253 R: 655.1859839526487\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 155 R: 305.59772400096614\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 220 R: 580.0141626232923\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 220 R: 575.495783910026\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 177 R: 414.3807679673024\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 186 R: 452.790982702348\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 175 R: 403.97468846458844\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 287 R: 747.1479000876645\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:281000 episode:1769 last_R: 819.5003921531994 average_R:658.1010339635261\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.57475), ('average_q2', 151.70084), ('average_q_func1_loss', 3.796234937906265), ('average_q_func2_loss', 3.639057916402817), ('n_updates', 271001), ('average_entropy', -3.024594), ('temperature', 0.03791392222046852)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:282000 episode:1772 last_R: 873.0630794983015 average_R:660.5128207174856\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.83888), ('average_q2', 151.96184), ('average_q_func1_loss', 3.69926832318306), ('average_q_func2_loss', 3.6489958226680757), ('n_updates', 272001), ('average_entropy', -3.0242124), ('temperature', 0.03849866986274719)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:283000 episode:1775 last_R: 760.4147005459165 average_R:663.5440815274451\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 153.08772), ('average_q2', 153.07521), ('average_q_func1_loss', 4.351670099496841), ('average_q_func2_loss', 3.9995644557476044), ('n_updates', 273001), ('average_entropy', -3.0934255), ('temperature', 0.037823617458343506)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:284000 episode:1778 last_R: 672.6255960409557 average_R:665.6698758492947\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.6669), ('average_q2', 158.77916), ('average_q_func1_loss', 4.646554907560349), ('average_q_func2_loss', 4.002631959915161), ('n_updates', 274001), ('average_entropy', -2.9960074), ('temperature', 0.03762457147240639)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:285000 episode:1782 last_R: 836.8018371079297 average_R:670.5947189675152\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 154.24649), ('average_q2', 153.98596), ('average_q_func1_loss', 4.112218142747879), ('average_q_func2_loss', 3.762379466295242), ('n_updates', 275001), ('average_entropy', -2.8848565), ('temperature', 0.037600770592689514)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 436 R: 1158.9615674737179\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 386 R: 909.1744947977784\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 1000 R: 2568.8664090183324\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 325 R: 846.2664302847136\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 367 R: 965.4289734776949\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 323 R: 765.0155545001373\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 287 R: 761.6325331984259\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 239 R: 470.0456069211194\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 266 R: 586.8691769239529\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 407 R: 1066.5832189968603\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 853.3628503912063 -> 1009.8843965592733\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:286000 episode:1784 last_R: 1004.1873164648084 average_R:674.5212968269216\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 154.46815), ('average_q2', 154.53285), ('average_q_func1_loss', 3.754057641029358), ('average_q_func2_loss', 3.92856344461441), ('n_updates', 276001), ('average_entropy', -2.9204378), ('temperature', 0.037533264607191086)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:287000 episode:1788 last_R: 557.5586359123307 average_R:666.1016664036957\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 154.557), ('average_q2', 154.67966), ('average_q_func1_loss', 4.9273454022407535), ('average_q_func2_loss', 3.978032879829407), ('n_updates', 277001), ('average_entropy', -2.9978673), ('temperature', 0.036931902170181274)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:288000 episode:1792 last_R: 859.4017628082288 average_R:667.9166391672131\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 156.39319), ('average_q2', 156.6919), ('average_q_func1_loss', 4.598057472705841), ('average_q_func2_loss', 4.1087629044055936), ('n_updates', 278001), ('average_entropy', -2.8581283), ('temperature', 0.03763347864151001)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:289000 episode:1797 last_R: 446.74063590745277 average_R:664.5878130302838\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 152.1212), ('average_q2', 152.165), ('average_q_func1_loss', 3.967986936569214), ('average_q_func2_loss', 3.6936092150211333), ('n_updates', 279001), ('average_entropy', -3.25577), ('temperature', 0.0376906581223011)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:290000 episode:1798 last_R: 565.9565712967036 average_R:663.7318205142973\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 154.63634), ('average_q2', 154.55296), ('average_q_func1_loss', 4.386210513114929), ('average_q_func2_loss', 3.7029127931594847), ('n_updates', 280001), ('average_entropy', -3.0505977), ('temperature', 0.03893293812870979)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 375 R: 942.7547456542565\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 332 R: 890.2931964064995\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 250 R: 546.9988260864951\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 339 R: 757.2526329790712\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 261 R: 514.7287563227555\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 460 R: 1126.4828103663162\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 361 R: 784.4219501767786\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 400 R: 1049.6989968275773\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 411 R: 984.7710612258724\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 344 R: 695.1296488726725\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:291000 episode:1801 last_R: 830.1919842834991 average_R:667.3399084587794\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.3255), ('average_q2', 151.41635), ('average_q_func1_loss', 3.8521604812145234), ('average_q_func2_loss', 3.7323918187618257), ('n_updates', 281001), ('average_entropy', -3.2283425), ('temperature', 0.03678655996918678)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:292000 episode:1805 last_R: 541.7750121297915 average_R:675.8915435468219\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 152.5018), ('average_q2', 152.6009), ('average_q_func1_loss', 4.1307879829406735), ('average_q_func2_loss', 4.0097827875614165), ('n_updates', 282001), ('average_entropy', -3.0220606), ('temperature', 0.037834495306015015)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:293000 episode:1807 last_R: 684.8165140765952 average_R:681.9032492626351\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 153.77554), ('average_q2', 153.69054), ('average_q_func1_loss', 4.157041776180267), ('average_q_func2_loss', 3.9591920351982117), ('n_updates', 283001), ('average_entropy', -2.8986568), ('temperature', 0.03804812207818031)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:294000 episode:1811 last_R: 1301.6188590753923 average_R:702.7910196199582\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 156.71544), ('average_q2', 156.35696), ('average_q_func1_loss', 4.224011632204056), ('average_q_func2_loss', 4.0378598523139955), ('n_updates', 284001), ('average_entropy', -2.9572403), ('temperature', 0.0386359728872776)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:295000 episode:1813 last_R: 454.7776610072226 average_R:704.8835573664666\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 153.73582), ('average_q2', 153.92953), ('average_q_func1_loss', 4.6467323648929595), ('average_q_func2_loss', 3.921242171525955), ('n_updates', 285001), ('average_entropy', -2.737125), ('temperature', 0.038081567734479904)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 307 R: 840.9748114608874\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 381 R: 998.4226388441128\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 361 R: 964.1663358153784\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 301 R: 666.0381614051053\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 303 R: 766.7927230086135\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 271 R: 615.3291046212471\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 332 R: 804.5028337936355\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 287 R: 724.9172134725326\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 328 R: 876.5659906873606\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 350 R: 889.0745408420612\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:296000 episode:1816 last_R: 724.6054627355109 average_R:708.6432947001344\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 155.56009), ('average_q2', 155.67712), ('average_q_func1_loss', 4.339569532871247), ('average_q_func2_loss', 4.136982489824295), ('n_updates', 286001), ('average_entropy', -3.0471926), ('temperature', 0.0375017449259758)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:297000 episode:1819 last_R: 999.5532974474669 average_R:712.7333779815325\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 154.77084), ('average_q2', 154.77162), ('average_q_func1_loss', 4.227455838918686), ('average_q_func2_loss', 3.732795480489731), ('n_updates', 287001), ('average_entropy', -3.0489166), ('temperature', 0.036996182054281235)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:298000 episode:1823 last_R: 675.2244684772428 average_R:718.2842135668566\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 157.08116), ('average_q2', 157.07329), ('average_q_func1_loss', 4.2337805831432345), ('average_q_func2_loss', 3.838088356256485), ('n_updates', 288001), ('average_entropy', -2.9261892), ('temperature', 0.037308916449546814)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:299000 episode:1826 last_R: 979.4145525715743 average_R:722.9551614328353\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 155.68819), ('average_q2', 155.56415), ('average_q_func1_loss', 4.062810765504837), ('average_q_func2_loss', 4.223790727853775), ('n_updates', 289001), ('average_entropy', -2.9952755), ('temperature', 0.03823158144950867)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:300000 episode:1830 last_R: 1085.4665283635245 average_R:738.9448748052531\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.5038), ('average_q2', 158.36818), ('average_q_func1_loss', 3.6096847915649413), ('average_q_func2_loss', 3.5555363583564756), ('n_updates', 290001), ('average_entropy', -3.0984411), ('temperature', 0.03814211115241051)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 277 R: 708.8423133326702\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 287 R: 704.3342188642328\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 305 R: 821.7817951963507\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 297 R: 721.9474341304223\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 302 R: 765.9638186457846\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 314 R: 811.0601665442761\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 242 R: 544.9143588368331\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 222 R: 530.9282374132538\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 317 R: 907.4308554270799\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 228 R: 537.4893453430878\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:301000 episode:1831 last_R: 910.5931558354489 average_R:742.3150805624985\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 156.51535), ('average_q2', 156.55977), ('average_q_func1_loss', 4.27391575217247), ('average_q_func2_loss', 4.332564399242401), ('n_updates', 291001), ('average_entropy', -2.8212667), ('temperature', 0.03904208168387413)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:302000 episode:1837 last_R: 314.98705270991985 average_R:757.6677816554577\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 154.19553), ('average_q2', 154.15784), ('average_q_func1_loss', 4.499912760257721), ('average_q_func2_loss', 4.236506338119507), ('n_updates', 292001), ('average_entropy', -2.9552882), ('temperature', 0.03913138061761856)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:303000 episode:1839 last_R: 757.297480293957 average_R:763.483595990213\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 153.61307), ('average_q2', 153.35976), ('average_q_func1_loss', 4.283773040771484), ('average_q_func2_loss', 4.053451191186905), ('n_updates', 293001), ('average_entropy', -3.0804322), ('temperature', 0.03923049196600914)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:304000 episode:1842 last_R: 995.3235078288869 average_R:768.9992651077515\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 154.99738), ('average_q2', 155.20674), ('average_q_func1_loss', 4.449001140594483), ('average_q_func2_loss', 4.189032682180405), ('n_updates', 294001), ('average_entropy', -2.8164062), ('temperature', 0.03878791257739067)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:305000 episode:1845 last_R: 689.50452872162 average_R:762.5693461068417\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 153.80588), ('average_q2', 153.82321), ('average_q_func1_loss', 3.9379362738132477), ('average_q_func2_loss', 3.9387354862689974), ('n_updates', 295001), ('average_entropy', -3.0613465), ('temperature', 0.0386970192193985)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 545 R: 1479.813031628283\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 342 R: 1026.786421658896\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 300 R: 744.9913616797645\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 328 R: 850.9867208488968\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 220 R: 487.5281826998714\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 333 R: 697.1369437133459\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 328 R: 880.3577270202968\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 327 R: 823.987760314329\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 358 R: 918.4780597404996\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 424 R: 1137.9502695323808\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:306000 episode:1848 last_R: 694.8866629423172 average_R:762.9799703098806\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.89243), ('average_q2', 159.2275), ('average_q_func1_loss', 3.420620399713516), ('average_q_func2_loss', 3.2538918328285216), ('n_updates', 296001), ('average_entropy', -2.9053893), ('temperature', 0.03676797077059746)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:307000 episode:1851 last_R: 746.0450799635792 average_R:766.3433364840963\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 155.6626), ('average_q2', 155.57388), ('average_q_func1_loss', 3.6705361044406892), ('average_q_func2_loss', 3.7548688316345213), ('n_updates', 297001), ('average_entropy', -3.1439395), ('temperature', 0.03738822042942047)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:308000 episode:1853 last_R: 816.0562566636639 average_R:760.6697138976191\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 152.26894), ('average_q2', 152.20576), ('average_q_func1_loss', 4.385047869682312), ('average_q_func2_loss', 3.955783898830414), ('n_updates', 298001), ('average_entropy', -3.2906127), ('temperature', 0.037964534014463425)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:309000 episode:1856 last_R: 782.9314936073954 average_R:767.8727200487546\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 157.64212), ('average_q2', 157.83812), ('average_q_func1_loss', 3.5336371445655823), ('average_q_func2_loss', 3.330582330226898), ('n_updates', 299001), ('average_entropy', -3.0572636), ('temperature', 0.03869593143463135)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:310000 episode:1860 last_R: 599.3717001030936 average_R:763.6323015400839\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 154.81319), ('average_q2', 154.89613), ('average_q_func1_loss', 4.775585556030274), ('average_q_func2_loss', 5.086853977441788), ('n_updates', 300001), ('average_entropy', -2.7562635), ('temperature', 0.037754714488983154)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 291 R: 747.4914327430838\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 348 R: 910.698697468653\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 323 R: 801.320084276924\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 309 R: 725.7073336198355\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 341 R: 896.4731295099434\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 306 R: 798.7378292751621\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 210 R: 457.1780649864988\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 371 R: 994.544514238902\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 333 R: 818.5300993705566\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 324 R: 794.4377947709065\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:311000 episode:1862 last_R: 542.9098852577799 average_R:759.6280917136569\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 156.86694), ('average_q2', 156.50862), ('average_q_func1_loss', 4.122360748052597), ('average_q_func2_loss', 3.898240011930466), ('n_updates', 301001), ('average_entropy', -3.0516446), ('temperature', 0.036153387278318405)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:312000 episode:1866 last_R: 537.7424491289127 average_R:763.4575796743484\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 156.84491), ('average_q2', 156.65213), ('average_q_func1_loss', 4.542756433486939), ('average_q_func2_loss', 3.9395488238334657), ('n_updates', 302001), ('average_entropy', -3.0005348), ('temperature', 0.03725053742527962)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:313000 episode:1868 last_R: 874.1836713318776 average_R:770.2778284658036\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 154.87071), ('average_q2', 154.58826), ('average_q_func1_loss', 4.259133945703507), ('average_q_func2_loss', 4.370442937612534), ('n_updates', 303001), ('average_entropy', -2.7683468), ('temperature', 0.03721889108419418)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:314000 episode:1872 last_R: 911.4745710284748 average_R:777.3653445440544\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 157.27469), ('average_q2', 157.54477), ('average_q_func1_loss', 4.586853890419007), ('average_q_func2_loss', 4.449303030967712), ('n_updates', 304001), ('average_entropy', -3.1267188), ('temperature', 0.037001464515924454)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:315000 episode:1873 last_R: 953.0186511014626 average_R:779.475812891068\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 156.31856), ('average_q2', 156.11789), ('average_q_func1_loss', 3.7916873717308044), ('average_q_func2_loss', 3.835451513528824), ('n_updates', 305001), ('average_entropy', -3.1892755), ('temperature', 0.03801558166742325)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 261 R: 532.2908855725605\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 472 R: 1234.9644854949663\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 253 R: 521.9361048847833\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 319 R: 676.2532442195908\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 259 R: 528.6344973981378\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 263 R: 532.2226817857195\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 453 R: 1103.1437906845829\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 313 R: 628.7916683089803\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 325 R: 823.7822050481935\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 421 R: 1069.3268869687588\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:316000 episode:1877 last_R: 855.4479844164516 average_R:780.0452216081886\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.00906), ('average_q2', 157.94356), ('average_q_func1_loss', 3.9044604301452637), ('average_q_func2_loss', 3.8098544335365294), ('n_updates', 306001), ('average_entropy', -3.0439594), ('temperature', 0.0371079221367836)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:317000 episode:1880 last_R: 563.7238042265241 average_R:773.351834136976\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 157.04352), ('average_q2', 156.97278), ('average_q_func1_loss', 4.2461321163177494), ('average_q_func2_loss', 3.7722187912464142), ('n_updates', 307001), ('average_entropy', -3.0588126), ('temperature', 0.03590558096766472)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:318000 episode:1882 last_R: 919.223959801185 average_R:779.0546622586575\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 155.00894), ('average_q2', 155.01834), ('average_q_func1_loss', 4.274905880689621), ('average_q_func2_loss', 3.8126324224472046), ('n_updates', 308001), ('average_entropy', -2.9166768), ('temperature', 0.03625733032822609)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:319000 episode:1888 last_R: 327.2805207372043 average_R:778.2568216919557\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 155.28519), ('average_q2', 155.29266), ('average_q_func1_loss', 4.5675502622127535), ('average_q_func2_loss', 4.214702532291413), ('n_updates', 309001), ('average_entropy', -2.9917352), ('temperature', 0.03677899017930031)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:320000 episode:1888 last_R: 327.2805207372043 average_R:778.2568216919557\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 157.43138), ('average_q2', 157.5204), ('average_q_func1_loss', 3.9069869756698608), ('average_q_func2_loss', 3.6000895369052888), ('n_updates', 310001), ('average_entropy', -2.9439383), ('temperature', 0.0374477319419384)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 315 R: 820.421335712035\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 404 R: 1061.7843585956768\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 238 R: 494.3360440250109\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 302 R: 730.1997077988761\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 348 R: 916.600467194185\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 356 R: 825.4894848334171\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 332 R: 863.0969904514195\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 347 R: 882.4198198963113\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 756 R: 1589.3236276619625\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 315 R: 827.3190807194895\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:321000 episode:1893 last_R: 806.2671516916221 average_R:786.3552776150636\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.06816), ('average_q2', 157.98003), ('average_q_func1_loss', 4.083519464731216), ('average_q_func2_loss', 3.7670178318023684), ('n_updates', 311001), ('average_entropy', -3.083225), ('temperature', 0.03753615543246269)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:322000 episode:1898 last_R: 859.4512408830599 average_R:794.2979418599366\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 159.45125), ('average_q2', 159.38121), ('average_q_func1_loss', 3.8172206223011016), ('average_q_func2_loss', 3.70861044049263), ('n_updates', 312001), ('average_entropy', -2.9851606), ('temperature', 0.037129975855350494)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:323000 episode:1898 last_R: 859.4512408830599 average_R:794.2979418599366\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 157.8492), ('average_q2', 157.87657), ('average_q_func1_loss', 3.785493861436844), ('average_q_func2_loss', 3.7601187324523924), ('n_updates', 313001), ('average_entropy', -3.183126), ('temperature', 0.03743980452418327)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:324000 episode:1903 last_R: 1024.1455213849881 average_R:801.4056843314684\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.9484), ('average_q2', 159.15569), ('average_q_func1_loss', 3.9042602825164794), ('average_q_func2_loss', 3.950985280275345), ('n_updates', 314001), ('average_entropy', -3.0002675), ('temperature', 0.03736120089888573)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:325000 episode:1905 last_R: 542.6774116190121 average_R:793.6568572217705\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 156.61292), ('average_q2', 156.74347), ('average_q_func1_loss', 4.009208689928055), ('average_q_func2_loss', 3.85881986618042), ('n_updates', 315001), ('average_entropy', -2.90819), ('temperature', 0.03658072650432587)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 326 R: 808.4017118741498\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 392 R: 878.9141901538519\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 369 R: 929.5682027066144\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 375 R: 922.8597082004902\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 375 R: 939.138412548654\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 364 R: 799.4877245758013\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 353 R: 922.6439279875715\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 326 R: 824.0596390385122\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 378 R: 816.5813757565177\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 329 R: 762.6261163368109\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:326000 episode:1909 last_R: 831.930194294257 average_R:786.1439612078995\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 156.97935), ('average_q2', 157.01318), ('average_q_func1_loss', 4.018235415220261), ('average_q_func2_loss', 3.866529064178467), ('n_updates', 316001), ('average_entropy', -2.9780972), ('temperature', 0.03670172765851021)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:327000 episode:1913 last_R: 878.6521508369354 average_R:784.3351562603616\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 154.87834), ('average_q2', 154.63216), ('average_q_func1_loss', 4.509087879657745), ('average_q_func2_loss', 4.773765704631805), ('n_updates', 317001), ('average_entropy', -3.1170099), ('temperature', 0.03775728866457939)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:328000 episode:1915 last_R: 975.3459086759061 average_R:789.1047081247747\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 157.55832), ('average_q2', 157.47264), ('average_q_func1_loss', 4.419356693029403), ('average_q_func2_loss', 4.39057222366333), ('n_updates', 318001), ('average_entropy', -2.8995445), ('temperature', 0.03793110325932503)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:329000 episode:1918 last_R: 967.0451268336782 average_R:795.8789170776539\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.78291), ('average_q2', 159.17358), ('average_q_func1_loss', 3.6559813737869264), ('average_q_func2_loss', 3.3722622632980346), ('n_updates', 319001), ('average_entropy', -2.9673812), ('temperature', 0.03797121345996857)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:330000 episode:1922 last_R: 973.0488364958226 average_R:793.4578084094126\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 157.46085), ('average_q2', 157.38867), ('average_q_func1_loss', 3.6604754757881164), ('average_q_func2_loss', 3.6124892354011537), ('n_updates', 320001), ('average_entropy', -2.8646915), ('temperature', 0.03751214221119881)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 228 R: 486.9506241454724\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 167 R: 405.9565772030181\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 312 R: 819.2499979626549\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 252 R: 686.2975058891001\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 286 R: 743.2786801396763\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 351 R: 931.2418062595616\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 385 R: 1056.438551323956\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 370 R: 901.6315273416947\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 275 R: 751.2674198897543\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 292 R: 699.2882230158726\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:331000 episode:1925 last_R: 510.47487649969344 average_R:790.8875105803716\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 160.38185), ('average_q2', 160.38388), ('average_q_func1_loss', 4.197794803380966), ('average_q_func2_loss', 3.8985221648216246), ('n_updates', 321001), ('average_entropy', -2.9530926), ('temperature', 0.0359971821308136)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:332000 episode:1927 last_R: 485.6534070150661 average_R:784.911537135137\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 156.1912), ('average_q2', 156.18842), ('average_q_func1_loss', 4.516616855859756), ('average_q_func2_loss', 4.450340901613235), ('n_updates', 322001), ('average_entropy', -2.9010491), ('temperature', 0.03752968832850456)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:333000 episode:1931 last_R: 651.0585374867218 average_R:788.7635971590393\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 157.3477), ('average_q2', 157.06711), ('average_q_func1_loss', 3.7710761535167694), ('average_q_func2_loss', 3.8681434392929077), ('n_updates', 323001), ('average_entropy', -3.2249868), ('temperature', 0.03680406138300896)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:334000 episode:1935 last_R: 766.0221386651731 average_R:784.1629417942703\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 155.80844), ('average_q2', 156.09624), ('average_q_func1_loss', 3.9397609353065492), ('average_q_func2_loss', 3.8863051879405974), ('n_updates', 324001), ('average_entropy', -2.9444537), ('temperature', 0.03712175413966179)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:335000 episode:1939 last_R: 624.5227319291902 average_R:787.4687271490073\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 157.53636), ('average_q2', 157.64198), ('average_q_func1_loss', 4.187159974575042), ('average_q_func2_loss', 4.1650746166706085), ('n_updates', 325001), ('average_entropy', -2.902618), ('temperature', 0.036317937076091766)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 121 R: 230.6621995783266\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 127 R: 245.22703228439156\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 109 R: 188.95764897581134\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 254 R: 730.7930232952864\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 203 R: 455.70220751555644\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 121 R: 225.6723575244267\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 114 R: 220.57027502740198\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 152 R: 320.8255109168602\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 153 R: 327.515447668294\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 119 R: 214.87693430964177\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:336000 episode:1944 last_R: 308.64224327012494 average_R:770.4381880770322\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 156.1485), ('average_q2', 155.92001), ('average_q_func1_loss', 4.015099614858627), ('average_q_func2_loss', 3.749902482032776), ('n_updates', 326001), ('average_entropy', -3.0416408), ('temperature', 0.038477811962366104)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:337000 episode:1946 last_R: 576.2665279955036 average_R:772.0687311466036\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 161.8694), ('average_q2', 161.89656), ('average_q_func1_loss', 4.5218897306919095), ('average_q_func2_loss', 3.815921176671982), ('n_updates', 327001), ('average_entropy', -2.8458285), ('temperature', 0.03738422691822052)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:338000 episode:1949 last_R: 889.1947982979137 average_R:780.7316245431211\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.97733), ('average_q2', 159.18832), ('average_q_func1_loss', 3.7359240865707397), ('average_q_func2_loss', 3.7642759430408477), ('n_updates', 328001), ('average_entropy', -2.908355), ('temperature', 0.037069421261548996)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:339000 episode:1952 last_R: 711.6672325035661 average_R:781.5058389611144\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 159.21863), ('average_q2', 159.30254), ('average_q_func1_loss', 4.552321622371673), ('average_q_func2_loss', 3.8630616652965544), ('n_updates', 329001), ('average_entropy', -2.9304566), ('temperature', 0.03706808015704155)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:340000 episode:1956 last_R: 606.4937885014243 average_R:771.6793316193217\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 155.96112), ('average_q2', 155.97984), ('average_q_func1_loss', 3.891862984895706), ('average_q_func2_loss', 3.666619658470154), ('n_updates', 330001), ('average_entropy', -3.0704327), ('temperature', 0.037407949566841125)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 289 R: 771.7217615770088\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 513 R: 1276.2750855914471\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 122 R: 221.47821579185515\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 287 R: 741.2242025549344\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 279 R: 729.0616445183052\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 263 R: 618.2993845507764\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 281 R: 719.4721907195392\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 218 R: 473.3057654937849\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 284 R: 651.5149121505358\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 150 R: 320.92835011044264\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:341000 episode:1959 last_R: 734.5343885177509 average_R:774.9720698589225\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 160.10368), ('average_q2', 159.91727), ('average_q_func1_loss', 4.661125190258026), ('average_q_func2_loss', 4.138510435819626), ('n_updates', 331001), ('average_entropy', -3.036839), ('temperature', 0.03633822128176689)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:342000 episode:1963 last_R: 294.7042543673312 average_R:766.9473132009883\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 159.31416), ('average_q2', 159.2637), ('average_q_func1_loss', 4.394975512027741), ('average_q_func2_loss', 3.8380461955070495), ('n_updates', 332001), ('average_entropy', -3.1607625), ('temperature', 0.03674045950174332)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:343000 episode:1965 last_R: 1225.7223629920418 average_R:771.4863606718983\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 159.1397), ('average_q2', 159.30038), ('average_q_func1_loss', 4.166335674524308), ('average_q_func2_loss', 3.8330780589580535), ('n_updates', 333001), ('average_entropy', -2.9957829), ('temperature', 0.036998096853494644)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:344000 episode:1970 last_R: 495.0123153508178 average_R:762.4157264526879\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 160.30167), ('average_q2', 160.29584), ('average_q_func1_loss', 4.040124878883362), ('average_q_func2_loss', 3.7682575511932375), ('n_updates', 334001), ('average_entropy', -2.8538864), ('temperature', 0.03655623644590378)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:345000 episode:1972 last_R: 579.6584545907972 average_R:757.35393618137\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.20099), ('average_q2', 158.22534), ('average_q_func1_loss', 3.8373230481147766), ('average_q_func2_loss', 3.7035799372196196), ('n_updates', 335001), ('average_entropy', -2.8410044), ('temperature', 0.03604591265320778)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 376 R: 1032.6337328310435\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 322 R: 793.3893885576357\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 303 R: 787.9311721668747\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 299 R: 806.6139934979925\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 273 R: 714.3970022669448\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 431 R: 1191.9448326347353\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 267 R: 705.6846648766948\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 264 R: 708.8928383392866\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 273 R: 745.7112095868926\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 319 R: 836.187017196236\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:346000 episode:1976 last_R: 705.6211434720173 average_R:754.6129720180486\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.25706), ('average_q2', 158.25594), ('average_q_func1_loss', 3.837940809726715), ('average_q_func2_loss', 3.783045879602432), ('n_updates', 336001), ('average_entropy', -3.1713398), ('temperature', 0.0361800380051136)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:347000 episode:1978 last_R: 809.8026566676593 average_R:752.4918502935993\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 159.83846), ('average_q2', 159.54698), ('average_q_func1_loss', 4.609865193367004), ('average_q_func2_loss', 4.5386233127117155), ('n_updates', 337001), ('average_entropy', -3.0945783), ('temperature', 0.03602314367890358)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:348000 episode:1982 last_R: 1062.0948458422072 average_R:749.5313280065618\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 157.77303), ('average_q2', 157.91954), ('average_q_func1_loss', 5.448087109327316), ('average_q_func2_loss', 4.8190951871871945), ('n_updates', 338001), ('average_entropy', -3.0478446), ('temperature', 0.03654155507683754)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:349000 episode:1986 last_R: 854.4788805937512 average_R:749.920338104031\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 154.39214), ('average_q2', 154.54294), ('average_q_func1_loss', 3.732321308851242), ('average_q_func2_loss', 3.579645439386368), ('n_updates', 339001), ('average_entropy', -3.0873835), ('temperature', 0.03694044426083565)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:350000 episode:1990 last_R: 518.7797960078133 average_R:744.7504686129812\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.19185), ('average_q2', 158.46341), ('average_q_func1_loss', 4.175133601427079), ('average_q_func2_loss', 4.163406791687012), ('n_updates', 340001), ('average_entropy', -2.9058945), ('temperature', 0.03748727962374687)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 373 R: 1049.6309478660116\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 286 R: 620.6149373187668\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 1000 R: 2154.5450739733105\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 385 R: 888.3955632298329\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 365 R: 989.2924396205769\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 256 R: 697.8275486467354\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 344 R: 917.254747743831\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 336 R: 852.6814423926018\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 211 R: 499.2631116218386\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 305 R: 799.9700148745947\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:351000 episode:1994 last_R: 300.6581688599629 average_R:735.6966791513747\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 160.58047), ('average_q2', 160.5742), ('average_q_func1_loss', 5.142106213569641), ('average_q_func2_loss', 4.96024213552475), ('n_updates', 341001), ('average_entropy', -3.2355263), ('temperature', 0.037900909781455994)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:352000 episode:1999 last_R: 738.6712472528797 average_R:737.9867907777091\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 160.46521), ('average_q2', 160.44456), ('average_q_func1_loss', 4.328526265621186), ('average_q_func2_loss', 4.123991609811783), ('n_updates', 342001), ('average_entropy', -3.1728873), ('temperature', 0.0369376502931118)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:353000 episode:2001 last_R: 238.03622013779145 average_R:730.4238141285423\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.05907), ('average_q2', 158.03696), ('average_q_func1_loss', 4.3979906916618345), ('average_q_func2_loss', 4.211994236707687), ('n_updates', 343001), ('average_entropy', -3.1532261), ('temperature', 0.037624895572662354)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:354000 episode:2005 last_R: 233.20099302101698 average_R:732.2429027753683\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 162.17682), ('average_q2', 162.51532), ('average_q_func1_loss', 3.9900848650932312), ('average_q_func2_loss', 3.6839230263233187), ('n_updates', 344001), ('average_entropy', -3.036011), ('temperature', 0.03697989508509636)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:355000 episode:2008 last_R: 1041.908846130551 average_R:733.5452726498032\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 160.99815), ('average_q2', 161.04613), ('average_q_func1_loss', 4.060009917020798), ('average_q_func2_loss', 3.998256183862686), ('n_updates', 345001), ('average_entropy', -3.002413), ('temperature', 0.03733591362833977)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 159 R: 312.5587923830533\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 133 R: 251.99255031073372\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 131 R: 240.34616724076977\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 226 R: 594.0184166721759\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 281 R: 767.7970344087618\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 220 R: 572.5826070853611\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 131 R: 238.34008653724172\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 152 R: 288.05071168163323\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 298 R: 830.3958842028558\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 200 R: 428.7242463849253\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:356000 episode:2013 last_R: 945.4058323064722 average_R:728.2099141776152\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 161.99088), ('average_q2', 162.05302), ('average_q_func1_loss', 5.457258269786835), ('average_q_func2_loss', 4.973690955638886), ('n_updates', 346001), ('average_entropy', -2.741168), ('temperature', 0.03710991516709328)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:357000 episode:2013 last_R: 945.4058323064722 average_R:728.2099141776152\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 159.64247), ('average_q2', 159.56522), ('average_q_func1_loss', 4.64242985367775), ('average_q_func2_loss', 3.767389978170395), ('n_updates', 347001), ('average_entropy', -2.9953017), ('temperature', 0.03794199228286743)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:358000 episode:2017 last_R: 683.8591949939943 average_R:725.511951424924\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.83362), ('average_q2', 158.86926), ('average_q_func1_loss', 4.213460134267807), ('average_q_func2_loss', 4.0686389088630674), ('n_updates', 348001), ('average_entropy', -2.9299), ('temperature', 0.03836369514465332)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:359000 episode:2022 last_R: 874.8101858581666 average_R:719.0067876080991\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.07726), ('average_q2', 158.04979), ('average_q_func1_loss', 4.6341439366340635), ('average_q_func2_loss', 4.457429368495941), ('n_updates', 349001), ('average_entropy', -3.2603042), ('temperature', 0.03836815059185028)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:360000 episode:2023 last_R: 779.9846272980332 average_R:717.5623677872715\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 160.70277), ('average_q2', 160.47865), ('average_q_func1_loss', 4.404013803005219), ('average_q_func2_loss', 4.308069581985474), ('n_updates', 350001), ('average_entropy', -2.9944723), ('temperature', 0.03845857083797455)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 322 R: 853.6938794322331\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 321 R: 827.4403277042258\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 390 R: 1080.3225509141696\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 344 R: 845.5696714628333\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 367 R: 923.6399245661087\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 361 R: 945.8948296882716\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 432 R: 1233.4403070385094\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 325 R: 843.941925131671\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 394 R: 1167.8509673164072\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 402 R: 1204.4571388512559\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:361000 episode:2027 last_R: 837.1681698353312 average_R:735.2449085191005\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 162.1519), ('average_q2', 162.32242), ('average_q_func1_loss', 5.672210199832916), ('average_q_func2_loss', 5.21661128282547), ('n_updates', 351001), ('average_entropy', -2.9757915), ('temperature', 0.03676087409257889)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:362000 episode:2030 last_R: 855.9805579121715 average_R:730.2347474934733\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 160.15437), ('average_q2', 160.08669), ('average_q_func1_loss', 4.210399614572525), ('average_q_func2_loss', 4.242230687141419), ('n_updates', 352001), ('average_entropy', -3.0864127), ('temperature', 0.03600115701556206)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:363000 episode:2033 last_R: 418.55534719379585 average_R:733.3188313979095\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 160.26472), ('average_q2', 160.48836), ('average_q_func1_loss', 3.7594600200653074), ('average_q_func2_loss', 3.640921642780304), ('n_updates', 353001), ('average_entropy', -3.1285923), ('temperature', 0.0361018180847168)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:364000 episode:2036 last_R: 851.5225246488776 average_R:734.995647840689\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 161.1847), ('average_q2', 160.90901), ('average_q_func1_loss', 4.286467844247818), ('average_q_func2_loss', 4.188383233547211), ('n_updates', 354001), ('average_entropy', -2.864516), ('temperature', 0.03775939345359802)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:365000 episode:2038 last_R: 668.6919121574147 average_R:736.2981356819428\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 161.93881), ('average_q2', 162.08423), ('average_q_func1_loss', 4.224367300271988), ('average_q_func2_loss', 4.645860319137573), ('n_updates', 355001), ('average_entropy', -3.0195608), ('temperature', 0.03822493925690651)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 283 R: 740.3068866002188\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 286 R: 760.0544007884846\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 264 R: 702.9233159790069\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 288 R: 740.2274388950885\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 286 R: 755.069571837884\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 295 R: 769.2424823893653\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 284 R: 746.0872258700742\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 281 R: 744.0617336003137\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 291 R: 765.5865571014514\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 273 R: 732.6312535673417\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:366000 episode:2043 last_R: 1180.1312335619439 average_R:761.7084127133523\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 161.39853), ('average_q2', 161.58002), ('average_q_func1_loss', 4.062512434720993), ('average_q_func2_loss', 3.545072239637375), ('n_updates', 356001), ('average_entropy', -2.8593228), ('temperature', 0.03787835314869881)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:367000 episode:2043 last_R: 1180.1312335619439 average_R:761.7084127133523\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.44746), ('average_q2', 158.6651), ('average_q_func1_loss', 5.229678566455841), ('average_q_func2_loss', 5.102992968559265), ('n_updates', 357001), ('average_entropy', -3.1237237), ('temperature', 0.03865402191877365)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:368000 episode:2048 last_R: 985.4191519594854 average_R:768.5239629769308\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 161.52269), ('average_q2', 161.22581), ('average_q_func1_loss', 4.475057903528214), ('average_q_func2_loss', 4.180350347757339), ('n_updates', 358001), ('average_entropy', -2.9263258), ('temperature', 0.036541081964969635)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:369000 episode:2050 last_R: 567.9612605044938 average_R:765.3606688440576\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 160.1251), ('average_q2', 160.51135), ('average_q_func1_loss', 4.604209687709808), ('average_q_func2_loss', 4.714817214012146), ('n_updates', 359001), ('average_entropy', -2.9771416), ('temperature', 0.03801853582262993)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:370000 episode:2053 last_R: 853.2796591224946 average_R:763.0829387142176\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 163.04222), ('average_q2', 163.12558), ('average_q_func1_loss', 4.397421581745148), ('average_q_func2_loss', 4.132884224653244), ('n_updates', 360001), ('average_entropy', -2.7615006), ('temperature', 0.03920146822929382)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 289 R: 735.8278499628799\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 357 R: 914.6818369272166\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 325 R: 870.5563511432251\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 318 R: 920.5958585424191\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 330 R: 940.3497495145907\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 314 R: 834.9365366624886\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 345 R: 892.7623024064468\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 298 R: 728.5317391157549\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 358 R: 923.2548469411284\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 357 R: 1001.8405703518059\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:371000 episode:2057 last_R: 941.5585290167598 average_R:770.7552396084216\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 160.35594), ('average_q2', 160.56056), ('average_q_func1_loss', 4.905064789056778), ('average_q_func2_loss', 4.542479382753372), ('n_updates', 361001), ('average_entropy', -2.8896155), ('temperature', 0.03653327748179436)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:372000 episode:2058 last_R: 1109.2241186977699 average_R:770.9620535734069\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 159.23419), ('average_q2', 159.2546), ('average_q_func1_loss', 3.6899060130119326), ('average_q_func2_loss', 3.6271985733509062), ('n_updates', 362001), ('average_entropy', -2.9433372), ('temperature', 0.03734331950545311)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:373000 episode:2060 last_R: 849.3519255656948 average_R:775.5101634028298\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 162.54887), ('average_q2', 162.40388), ('average_q_func1_loss', 5.3148067247867585), ('average_q_func2_loss', 5.182623767852784), ('n_updates', 363001), ('average_entropy', -3.2530727), ('temperature', 0.036996129900217056)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:374000 episode:2063 last_R: 822.1060568337911 average_R:792.5757288568266\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.34618), ('average_q2', 158.53941), ('average_q_func1_loss', 4.192706602811813), ('average_q_func2_loss', 4.199801971912384), ('n_updates', 364001), ('average_entropy', -2.7714849), ('temperature', 0.03697768971323967)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:375000 episode:2068 last_R: 775.9900477938146 average_R:794.8933933595488\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 158.99178), ('average_q2', 159.10112), ('average_q_func1_loss', 4.061415746212005), ('average_q_func2_loss', 3.8112755000591276), ('n_updates', 365001), ('average_entropy', -3.0501904), ('temperature', 0.03712614253163338)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 321 R: 881.6135911501779\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 344 R: 858.6200719372346\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 274 R: 669.5857629370586\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 362 R: 966.6130337604384\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 302 R: 785.0372507205911\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 318 R: 880.0182669182396\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 337 R: 821.325883918855\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 283 R: 616.7969009861478\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 377 R: 1026.1275178329383\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 348 R: 943.1047241963594\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:376000 episode:2071 last_R: 443.0810539689341 average_R:792.5899085530685\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 161.74953), ('average_q2', 161.6273), ('average_q_func1_loss', 3.918005337715149), ('average_q_func2_loss', 3.853650323152542), ('n_updates', 366001), ('average_entropy', -2.9571433), ('temperature', 0.036497000604867935)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:377000 episode:2073 last_R: 919.7074452263427 average_R:795.1002111696716\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 162.21938), ('average_q2', 162.18048), ('average_q_func1_loss', 5.147385050058364), ('average_q_func2_loss', 5.010695894956589), ('n_updates', 367001), ('average_entropy', -3.0636666), ('temperature', 0.03878772631287575)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:378000 episode:2077 last_R: 930.7019665251787 average_R:802.8119146428096\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 161.43022), ('average_q2', 161.64487), ('average_q_func1_loss', 5.360209127664566), ('average_q_func2_loss', 4.759334540367126), ('n_updates', 368001), ('average_entropy', -3.0568173), ('temperature', 0.037995047867298126)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:379000 episode:2078 last_R: 865.1558204938506 average_R:803.3654462810716\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 161.09392), ('average_q2', 160.93616), ('average_q_func1_loss', 3.496973569393158), ('average_q_func2_loss', 3.485833430290222), ('n_updates', 369001), ('average_entropy', -2.8409307), ('temperature', 0.03666608780622482)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:380000 episode:2083 last_R: 868.4228882133302 average_R:806.6716065036287\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 163.14265), ('average_q2', 163.00586), ('average_q_func1_loss', 4.841761975288391), ('average_q_func2_loss', 4.533958175182343), ('n_updates', 370001), ('average_entropy', -3.035016), ('temperature', 0.037570610642433167)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 382 R: 822.5678124612622\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 295 R: 777.996780478965\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 411 R: 1076.5755172197528\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 555 R: 1573.3519848213705\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 262 R: 595.8991420721917\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 403 R: 1070.162640207518\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 420 R: 1107.7571241451678\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 288 R: 720.7102442499886\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 254 R: 565.3023619930947\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 340 R: 834.602098819824\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:381000 episode:2084 last_R: 872.5232748290105 average_R:810.2587850463867\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 160.56732), ('average_q2', 160.6634), ('average_q_func1_loss', 3.6118380308151243), ('average_q_func2_loss', 3.706289703845978), ('n_updates', 371001), ('average_entropy', -3.0016055), ('temperature', 0.03685208037495613)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:382000 episode:2086 last_R: 1249.2738036778562 average_R:816.891583030173\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 159.677), ('average_q2', 159.5868), ('average_q_func1_loss', 5.218004618883133), ('average_q_func2_loss', 4.577537918090821), ('n_updates', 372001), ('average_entropy', -2.8609657), ('temperature', 0.037043146789073944)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:383000 episode:2088 last_R: 521.2654987160801 average_R:830.0113874490285\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 165.18628), ('average_q2', 165.00574), ('average_q_func1_loss', 4.259781647920608), ('average_q_func2_loss', 4.067274879217148), ('n_updates', 373001), ('average_entropy', -2.9022985), ('temperature', 0.03653649240732193)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:384000 episode:2091 last_R: 2163.0592793183223 average_R:856.6023988773038\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 162.63428), ('average_q2', 162.66982), ('average_q_func1_loss', 4.285689564943314), ('average_q_func2_loss', 4.275917217731476), ('n_updates', 374001), ('average_entropy', -3.0532637), ('temperature', 0.036219559609889984)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:385000 episode:2092 last_R: 513.0995697294427 average_R:852.5747818468342\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 162.89107), ('average_q2', 162.98685), ('average_q_func1_loss', 4.071738924980163), ('average_q_func2_loss', 4.3108129620552065), ('n_updates', 375001), ('average_entropy', -3.1963663), ('temperature', 0.036472443491220474)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 389 R: 884.131902829565\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 396 R: 1102.0528741888959\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 445 R: 1092.224161728271\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 439 R: 1044.6327059699863\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 339 R: 793.989473271008\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 387 R: 969.613144235699\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 390 R: 965.4211652233219\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 337 R: 739.7429220130739\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 404 R: 998.6842272145831\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 340 R: 836.6413352053642\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:386000 episode:2096 last_R: 1590.0941533187174 average_R:871.3031601946172\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 162.24301), ('average_q2', 161.93076), ('average_q_func1_loss', 4.482336325645447), ('average_q_func2_loss', 4.612860554456711), ('n_updates', 376001), ('average_entropy', -3.280387), ('temperature', 0.03674498200416565)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:387000 episode:2097 last_R: 1444.6124960293384 average_R:876.6968030003537\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 163.33199), ('average_q2', 163.33833), ('average_q_func1_loss', 4.548892929553985), ('average_q_func2_loss', 4.053051627874375), ('n_updates', 377001), ('average_entropy', -2.833307), ('temperature', 0.03708724305033684)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:388000 episode:2101 last_R: 828.3924649802858 average_R:885.2082998303214\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 162.7751), ('average_q2', 162.84853), ('average_q_func1_loss', 3.735875616073608), ('average_q_func2_loss', 3.918438880443573), ('n_updates', 378001), ('average_entropy', -2.9420083), ('temperature', 0.03715469315648079)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:389000 episode:2103 last_R: 987.0024052907127 average_R:886.768231070179\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 162.94328), ('average_q2', 163.11877), ('average_q_func1_loss', 3.670649524927139), ('average_q_func2_loss', 3.4199244570732117), ('n_updates', 379001), ('average_entropy', -2.9171686), ('temperature', 0.03719629719853401)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:390000 episode:2105 last_R: 1149.404976263828 average_R:896.8416468549644\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 160.80464), ('average_q2', 160.97025), ('average_q_func1_loss', 3.7282746195793153), ('average_q_func2_loss', 3.6504027020931242), ('n_updates', 380001), ('average_entropy', -2.9630518), ('temperature', 0.0388619638979435)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 520 R: 1450.6605362300147\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 350 R: 946.4158865155764\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 415 R: 1174.7726175281707\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 306 R: 753.768299469004\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 385 R: 1045.6135659143101\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 366 R: 982.5104254046964\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 343 R: 936.609974687783\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 469 R: 1263.4235478274738\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 366 R: 947.6089828590289\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 339 R: 811.0397014341117\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 1009.8843965592733 -> 1031.242353787017\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:391000 episode:2109 last_R: 601.5528915628933 average_R:894.3466073202873\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 165.30316), ('average_q2', 165.35777), ('average_q_func1_loss', 4.0449136447906495), ('average_q_func2_loss', 4.078669415712357), ('n_updates', 381001), ('average_entropy', -2.8929076), ('temperature', 0.03764396905899048)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:392000 episode:2110 last_R: 2254.4359857372424 average_R:911.5798014520877\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 164.54593), ('average_q2', 164.50003), ('average_q_func1_loss', 4.162058283090591), ('average_q_func2_loss', 4.373806793689727), ('n_updates', 382001), ('average_entropy', -3.0647714), ('temperature', 0.038428012281656265)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:393000 episode:2113 last_R: 981.1508913714836 average_R:918.7372793381074\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 165.72871), ('average_q2', 165.63327), ('average_q_func1_loss', 3.7973066067695616), ('average_q_func2_loss', 3.6953762376308443), ('n_updates', 383001), ('average_entropy', -2.9902396), ('temperature', 0.03739328309893608)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:394000 episode:2114 last_R: 1041.2112830642252 average_R:919.4880905556122\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 165.34825), ('average_q2', 165.22057), ('average_q_func1_loss', 4.270691910982132), ('average_q_func2_loss', 4.049056061506271), ('n_updates', 384001), ('average_entropy', -3.122119), ('temperature', 0.038659073412418365)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:395000 episode:2118 last_R: 1012.4716584745258 average_R:916.8418390593291\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 162.4583), ('average_q2', 162.54112), ('average_q_func1_loss', 4.31943008184433), ('average_q_func2_loss', 4.06572439789772), ('n_updates', 385001), ('average_entropy', -2.783858), ('temperature', 0.03717753291130066)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 322 R: 818.8027017240582\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 472 R: 1304.3152024926478\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 340 R: 937.6387748640637\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 383 R: 996.8867079248975\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 339 R: 913.2003192922217\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 310 R: 734.5360545611385\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 406 R: 1055.8724388872301\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 356 R: 948.2910268314863\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 368 R: 1097.319323827575\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 349 R: 1003.5334246043907\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:396000 episode:2121 last_R: 716.3413805364519 average_R:942.6036363632616\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 163.671), ('average_q2', 163.59496), ('average_q_func1_loss', 4.531440534591675), ('average_q_func2_loss', 4.319858396053315), ('n_updates', 386001), ('average_entropy', -2.9293194), ('temperature', 0.037206415086984634)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:397000 episode:2122 last_R: 1222.993958255395 average_R:946.0854740872339\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 163.55959), ('average_q2', 163.64963), ('average_q_func1_loss', 4.5692432856559755), ('average_q_func2_loss', 4.532526435852051), ('n_updates', 387001), ('average_entropy', -3.0584843), ('temperature', 0.03870124742388725)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:398000 episode:2125 last_R: 942.6768705638021 average_R:945.7849126498863\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 165.62521), ('average_q2', 165.68578), ('average_q_func1_loss', 4.011103377342224), ('average_q_func2_loss', 3.645556777715683), ('n_updates', 388001), ('average_entropy', -2.9165077), ('temperature', 0.03976408764719963)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:399000 episode:2126 last_R: 1132.9763398364917 average_R:946.1515227184805\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 165.32104), ('average_q2', 165.25352), ('average_q_func1_loss', 3.864495311975479), ('average_q_func2_loss', 3.908611851930618), ('n_updates', 389001), ('average_entropy', -2.900173), ('temperature', 0.038944095373153687)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:400000 episode:2131 last_R: 2453.4112382018297 average_R:954.6865134014862\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 164.2643), ('average_q2', 164.28143), ('average_q_func1_loss', 4.473983279466629), ('average_q_func2_loss', 4.297501870393753), ('n_updates', 390001), ('average_entropy', -3.1227646), ('temperature', 0.038709014654159546)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 342 R: 673.2392095755966\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 470 R: 1295.802021599886\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 431 R: 1009.6449571698467\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 487 R: 1302.4802193745138\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 398 R: 972.4379029856557\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 309 R: 627.8715271186055\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 405 R: 1022.6552097401005\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 385 R: 949.8437650707299\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 368 R: 768.187415274268\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 385 R: 1050.3616468422315\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:401000 episode:2132 last_R: 750.7335834152185 average_R:953.9277657146844\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 165.49362), ('average_q2', 165.43529), ('average_q_func1_loss', 4.418123502731323), ('average_q_func2_loss', 3.9773951673507693), ('n_updates', 391001), ('average_entropy', -3.139099), ('temperature', 0.0388609804213047)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:402000 episode:2135 last_R: 675.0713368313432 average_R:955.4210266072575\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 162.26581), ('average_q2', 162.1603), ('average_q_func1_loss', 4.317245978116989), ('average_q_func2_loss', 4.060772831439972), ('n_updates', 392001), ('average_entropy', -3.1056197), ('temperature', 0.039418622851371765)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:403000 episode:2137 last_R: 924.0840445305147 average_R:959.9830176820246\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 164.68678), ('average_q2', 164.75653), ('average_q_func1_loss', 4.615353645086288), ('average_q_func2_loss', 5.902381365299225), ('n_updates', 393001), ('average_entropy', -2.7698896), ('temperature', 0.03869682922959328)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:404000 episode:2138 last_R: 1090.921164211954 average_R:964.2053102025701\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 166.75194), ('average_q2', 166.71243), ('average_q_func1_loss', 4.08198390841484), ('average_q_func2_loss', 3.814206848144531), ('n_updates', 394001), ('average_entropy', -3.1897426), ('temperature', 0.037619415670633316)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:405000 episode:2142 last_R: 1001.1897790888594 average_R:979.9796117598322\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 165.16005), ('average_q2', 164.96046), ('average_q_func1_loss', 4.224721148014068), ('average_q_func2_loss', 4.054326446056366), ('n_updates', 395001), ('average_entropy', -2.770395), ('temperature', 0.03765374422073364)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 335 R: 849.1551385960477\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 400 R: 1103.778675555623\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 304 R: 707.0505964899205\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 399 R: 946.5106940235593\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 332 R: 819.9402081675279\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 262 R: 583.9258583606788\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 345 R: 875.3391893740725\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 388 R: 927.1641364651043\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 486 R: 1308.4995899704902\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 310 R: 732.3272108822034\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:406000 episode:2146 last_R: 905.6540669082578 average_R:979.5798138573903\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 164.7053), ('average_q2', 165.01479), ('average_q_func1_loss', 4.983181444406509), ('average_q_func2_loss', 4.808207957744599), ('n_updates', 396001), ('average_entropy', -3.0177748), ('temperature', 0.03837313503026962)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:407000 episode:2147 last_R: 665.1427003255683 average_R:976.1749297667405\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 163.36275), ('average_q2', 163.38132), ('average_q_func1_loss', 4.813467748165131), ('average_q_func2_loss', 4.921520011425018), ('n_updates', 397001), ('average_entropy', -3.106854), ('temperature', 0.03817063197493553)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:408000 episode:2151 last_R: 1066.4463337835089 average_R:981.0582430501715\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 166.51387), ('average_q2', 166.5243), ('average_q_func1_loss', 4.290776481628418), ('average_q_func2_loss', 4.211340948343277), ('n_updates', 398001), ('average_entropy', -3.0005808), ('temperature', 0.03676491603255272)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:409000 episode:2153 last_R: 346.33205778284037 average_R:976.5410047331233\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 166.25772), ('average_q2', 166.03114), ('average_q_func1_loss', 3.856993200778961), ('average_q_func2_loss', 3.698739094734192), ('n_updates', 399001), ('average_entropy', -2.9942343), ('temperature', 0.03670459985733032)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:410000 episode:2157 last_R: 857.3317176533823 average_R:980.4993317362816\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 164.7342), ('average_q2', 164.78194), ('average_q_func1_loss', 4.402423502206802), ('average_q_func2_loss', 4.443617243766784), ('n_updates', 400001), ('average_entropy', -3.0756736), ('temperature', 0.03695133700966835)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 270 R: 596.5728852719801\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 361 R: 917.2103481806872\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 371 R: 922.9036392167184\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 442 R: 1246.2977932887318\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 360 R: 943.954553409968\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 335 R: 891.2198407270521\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 424 R: 1072.6390170387724\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 367 R: 949.8464006230757\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 407 R: 1097.0214431679997\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 359 R: 951.6263343971488\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:411000 episode:2158 last_R: 848.2592366415151 average_R:977.8896829157192\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 166.77539), ('average_q2', 166.947), ('average_q_func1_loss', 5.574117043018341), ('average_q_func2_loss', 5.562908387184143), ('n_updates', 401001), ('average_entropy', -2.9343886), ('temperature', 0.03786333650350571)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:412000 episode:2163 last_R: 971.6781776627781 average_R:971.9245067245455\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 165.20049), ('average_q2', 165.33104), ('average_q_func1_loss', 4.473802174329758), ('average_q_func2_loss', 4.6494398164749144), ('n_updates', 402001), ('average_entropy', -2.8336353), ('temperature', 0.036590494215488434)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:413000 episode:2164 last_R: 639.1618286226366 average_R:969.9273641161413\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 164.17554), ('average_q2', 164.40231), ('average_q_func1_loss', 3.970003391504288), ('average_q_func2_loss', 4.111866565942765), ('n_updates', 403001), ('average_entropy', -2.9758172), ('temperature', 0.03714330866932869)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:414000 episode:2169 last_R: 671.0662386556435 average_R:976.8579720359744\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 165.53046), ('average_q2', 165.10867), ('average_q_func1_loss', 4.695476109981537), ('average_q_func2_loss', 4.548217370510101), ('n_updates', 404001), ('average_entropy', -3.0462334), ('temperature', 0.037932343780994415)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:415000 episode:2170 last_R: 304.72616219916 average_R:971.8414657874475\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 164.49188), ('average_q2', 164.43791), ('average_q_func1_loss', 4.750332673788071), ('average_q_func2_loss', 4.4599554002285), ('n_updates', 405001), ('average_entropy', -3.0424497), ('temperature', 0.03679782152175903)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 347 R: 971.0793468953483\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 231 R: 530.8682046616523\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 223 R: 519.4015189348017\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 223 R: 526.9815884050545\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 181 R: 362.42970563128364\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 219 R: 528.1626488310304\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 231 R: 530.8039438717677\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 247 R: 610.1810529411428\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 222 R: 524.406339199423\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 350 R: 924.354385805618\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:416000 episode:2175 last_R: 894.4880639848147 average_R:972.4413288766013\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 167.67834), ('average_q2', 167.617), ('average_q_func1_loss', 4.171175744533539), ('average_q_func2_loss', 3.9922796988487246), ('n_updates', 406001), ('average_entropy', -3.2699957), ('temperature', 0.03816719725728035)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:417000 episode:2179 last_R: 549.8238045798937 average_R:964.6205497792885\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 166.41025), ('average_q2', 166.38168), ('average_q_func1_loss', 4.769051893949508), ('average_q_func2_loss', 3.8001283657550813), ('n_updates', 407001), ('average_entropy', -2.9916997), ('temperature', 0.03713402897119522)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:418000 episode:2182 last_R: 911.7496628144615 average_R:963.7475988972915\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 168.31052), ('average_q2', 168.2462), ('average_q_func1_loss', 3.774278777837753), ('average_q_func2_loss', 3.9005338835716246), ('n_updates', 408001), ('average_entropy', -2.9029934), ('temperature', 0.036909136921167374)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:419000 episode:2184 last_R: 879.44533724962 average_R:959.908645641532\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 162.18155), ('average_q2', 162.0281), ('average_q_func1_loss', 4.73434118270874), ('average_q_func2_loss', 4.517649170160293), ('n_updates', 409001), ('average_entropy', -3.246311), ('temperature', 0.03588850423693657)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:420000 episode:2186 last_R: 1013.0642357005878 average_R:955.7951406856716\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 167.70343), ('average_q2', 167.63617), ('average_q_func1_loss', 4.5184821760654446), ('average_q_func2_loss', 4.751426006555557), ('n_updates', 410001), ('average_entropy', -3.010116), ('temperature', 0.03678768873214722)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 372 R: 788.3061189897245\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 376 R: 789.570065355646\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 440 R: 1184.5385624967726\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 368 R: 810.4330798736878\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 398 R: 932.686535808096\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 346 R: 689.9484678913713\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 426 R: 1061.9665938972553\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 407 R: 927.5152010985704\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 945 R: 2335.31024967798\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 491 R: 1308.471467664843\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 1031.242353787017 -> 1082.8746342753948\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:421000 episode:2190 last_R: 375.62309114179493 average_R:940.7533100708116\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 165.9543), ('average_q2', 166.04819), ('average_q_func1_loss', 3.598628761768341), ('average_q_func2_loss', 3.487901018857956), ('n_updates', 411001), ('average_entropy', -2.9059365), ('temperature', 0.03705444559454918)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:422000 episode:2191 last_R: 2343.9702277833385 average_R:942.5624195554618\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 163.58482), ('average_q2', 163.63046), ('average_q_func1_loss', 4.4695127737522125), ('average_q_func2_loss', 4.580865104198455), ('n_updates', 412001), ('average_entropy', -3.2477689), ('temperature', 0.036641452461481094)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:423000 episode:2196 last_R: 592.7879902442189 average_R:936.8363728618001\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 167.07074), ('average_q2', 166.92839), ('average_q_func1_loss', 4.216186475753784), ('average_q_func2_loss', 4.275339956283569), ('n_updates', 413001), ('average_entropy', -2.9873354), ('temperature', 0.03740747645497322)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:424000 episode:2198 last_R: 942.4512926781382 average_R:929.9426368871891\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 166.05457), ('average_q2', 165.9279), ('average_q_func1_loss', 4.05765468120575), ('average_q_func2_loss', 4.075735284090042), ('n_updates', 414001), ('average_entropy', -3.035642), ('temperature', 0.03713564947247505)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:425000 episode:2199 last_R: 1260.7329974640584 average_R:934.0047482504332\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 167.06876), ('average_q2', 166.96214), ('average_q_func1_loss', 5.452077703475952), ('average_q_func2_loss', 4.951825790405273), ('n_updates', 415001), ('average_entropy', -3.1385264), ('temperature', 0.03665223345160484)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 291 R: 717.5477106749445\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 320 R: 771.210123016613\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 406 R: 1135.588793082242\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 351 R: 969.252959740058\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 232 R: 557.0972963199873\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 537 R: 1689.0129654707594\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 334 R: 919.533825977929\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 381 R: 1064.7346589977587\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 418 R: 1249.6799981973509\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 369 R: 1019.6662812569074\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:426000 episode:2203 last_R: 884.291392591832 average_R:940.4516590191056\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 166.55725), ('average_q2', 166.29), ('average_q_func1_loss', 5.110550507307052), ('average_q_func2_loss', 4.958431622982025), ('n_updates', 416001), ('average_entropy', -2.9426036), ('temperature', 0.03659983351826668)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:427000 episode:2204 last_R: 907.7327142742689 average_R:937.8667370862593\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 164.18294), ('average_q2', 164.0456), ('average_q_func1_loss', 4.426223069429398), ('average_q_func2_loss', 4.229315603971481), ('n_updates', 417001), ('average_entropy', -2.9788382), ('temperature', 0.03595783933997154)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:428000 episode:2208 last_R: 976.401917498937 average_R:949.1542806246954\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 167.70485), ('average_q2', 167.64546), ('average_q_func1_loss', 3.8897330105304717), ('average_q_func2_loss', 3.7739653933048247), ('n_updates', 418001), ('average_entropy', -2.918123), ('temperature', 0.035259660333395004)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:429000 episode:2209 last_R: 641.0234471841293 average_R:949.5489861809077\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 165.25034), ('average_q2', 165.02638), ('average_q_func1_loss', 4.6380746924877165), ('average_q_func2_loss', 4.171636306047439), ('n_updates', 419001), ('average_entropy', -3.0254786), ('temperature', 0.036599449813365936)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:430000 episode:2212 last_R: 1276.5925352162474 average_R:938.9744567562954\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 168.00922), ('average_q2', 168.04735), ('average_q_func1_loss', 4.618220213651657), ('average_q_func2_loss', 4.569802023172379), ('n_updates', 420001), ('average_entropy', -3.0605173), ('temperature', 0.037104833871126175)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 419 R: 1136.0763761797089\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 368 R: 833.1372596926424\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 418 R: 1085.9150600534804\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 437 R: 1149.956526954237\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 504 R: 1289.3248810850928\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 492 R: 1210.9273022868017\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 389 R: 854.4429181392253\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 514 R: 1357.8985458351121\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 283 R: 624.1936484826399\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 1000 R: 2166.9544821997856\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 1082.8746342753948 -> 1170.8827000908725\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:431000 episode:2214 last_R: 780.7083701024013 average_R:945.0963088880801\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 170.61046), ('average_q2', 170.66612), ('average_q_func1_loss', 5.570629662275314), ('average_q_func2_loss', 5.132731325626374), ('n_updates', 421001), ('average_entropy', -3.0060322), ('temperature', 0.03653886169195175)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:432000 episode:2214 last_R: 780.7083701024013 average_R:945.0963088880801\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 165.01646), ('average_q2', 165.424), ('average_q_func1_loss', 5.569060535430908), ('average_q_func2_loss', 5.2298755049705505), ('n_updates', 422001), ('average_entropy', -3.1534116), ('temperature', 0.037104129791259766)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:433000 episode:2218 last_R: 966.5818166799085 average_R:960.8423957873588\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 164.92241), ('average_q2', 164.80449), ('average_q_func1_loss', 3.807313565015793), ('average_q_func2_loss', 3.9628065860271455), ('n_updates', 423001), ('average_entropy', -2.9095073), ('temperature', 0.037375155836343765)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:434000 episode:2220 last_R: 698.0664305912846 average_R:942.3103567307801\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 167.282), ('average_q2', 167.3993), ('average_q_func1_loss', 4.44286988735199), ('average_q_func2_loss', 4.410412777662277), ('n_updates', 424001), ('average_entropy', -3.1000772), ('temperature', 0.0362209752202034)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:435000 episode:2223 last_R: 1401.101552736277 average_R:962.3098833375184\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 166.49684), ('average_q2', 166.78818), ('average_q_func1_loss', 3.770733572244644), ('average_q_func2_loss', 3.9389604461193084), ('n_updates', 425001), ('average_entropy', -3.0607216), ('temperature', 0.035925112664699554)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 1000 R: 2303.9640376671887\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 455 R: 1140.2405555976627\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 334 R: 841.4874235339734\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 437 R: 1093.821466784135\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 613 R: 1603.335923396068\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 453 R: 1007.8607047465957\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 584 R: 1649.7421903216616\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 811 R: 2508.2198584646667\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 354 R: 981.7160617217071\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 491 R: 1317.4060116239295\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 1170.8827000908725 -> 1444.779423385759\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:436000 episode:2225 last_R: 1267.409435478517 average_R:967.2523296700373\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 166.51822), ('average_q2', 166.26715), ('average_q_func1_loss', 4.1684456634521485), ('average_q_func2_loss', 4.2379510986804965), ('n_updates', 426001), ('average_entropy', -3.157711), ('temperature', 0.03864451125264168)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:437000 episode:2227 last_R: 787.0369205026655 average_R:971.1050954386076\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 169.14706), ('average_q2', 169.27394), ('average_q_func1_loss', 4.049641710519791), ('average_q_func2_loss', 3.9933761394023897), ('n_updates', 427001), ('average_entropy', -2.851373), ('temperature', 0.038561753928661346)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:438000 episode:2229 last_R: 1520.5093127434027 average_R:984.3987387194256\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 164.68866), ('average_q2', 164.86757), ('average_q_func1_loss', 3.8807928562164307), ('average_q_func2_loss', 3.912930055856705), ('n_updates', 428001), ('average_entropy', -2.927186), ('temperature', 0.03850274533033371)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:439000 episode:2231 last_R: 1086.8689583648493 average_R:970.657086062913\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 168.65173), ('average_q2', 168.61334), ('average_q_func1_loss', 4.109114801883697), ('average_q_func2_loss', 4.012984118461609), ('n_updates', 429001), ('average_entropy', -3.0663936), ('temperature', 0.03709369897842407)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:440000 episode:2234 last_R: 1049.849417528557 average_R:991.6971860650234\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 165.40565), ('average_q2', 165.68184), ('average_q_func1_loss', 5.102159063816071), ('average_q_func2_loss', 4.940635154247284), ('n_updates', 430001), ('average_entropy', -3.0290225), ('temperature', 0.037611305713653564)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 342 R: 883.3912279630599\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 378 R: 1131.1470672158941\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 337 R: 840.5183430759163\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 396 R: 1156.18584198134\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 401 R: 1010.8781047483051\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 354 R: 924.8135641405529\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 321 R: 824.5176869904682\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 374 R: 1119.852823299213\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 315 R: 776.9402425888845\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 378 R: 997.7939585633125\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:441000 episode:2237 last_R: 1055.810611640561 average_R:990.6282425013889\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 165.3865), ('average_q2', 165.36258), ('average_q_func1_loss', 4.1583953475952145), ('average_q_func2_loss', 4.335586156845093), ('n_updates', 431001), ('average_entropy', -3.0258858), ('temperature', 0.037382107228040695)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:442000 episode:2241 last_R: 148.9177588486343 average_R:959.3729559550873\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 167.62563), ('average_q2', 167.77046), ('average_q_func1_loss', 4.368183298110962), ('average_q_func2_loss', 3.949393569231033), ('n_updates', 432001), ('average_entropy', -3.0194895), ('temperature', 0.036266081035137177)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:443000 episode:2245 last_R: 1103.3596253006026 average_R:958.0414470557917\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 168.09015), ('average_q2', 167.90793), ('average_q_func1_loss', 5.1922110652923585), ('average_q_func2_loss', 5.258817248344421), ('n_updates', 433001), ('average_entropy', -2.9729264), ('temperature', 0.03686093911528587)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:444000 episode:2245 last_R: 1103.3596253006026 average_R:958.0414470557917\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 167.37285), ('average_q2', 167.41762), ('average_q_func1_loss', 4.023693693876266), ('average_q_func2_loss', 4.079201407432556), ('n_updates', 434001), ('average_entropy', -3.1412013), ('temperature', 0.03706306964159012)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:445000 episode:2248 last_R: 1247.6100139777354 average_R:969.6363668890256\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 166.47375), ('average_q2', 166.30028), ('average_q_func1_loss', 4.26531834602356), ('average_q_func2_loss', 4.977536559104919), ('n_updates', 435001), ('average_entropy', -3.0588202), ('temperature', 0.03716740384697914)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 1000 R: 2638.014715607284\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 530 R: 1436.8644715301216\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 392 R: 1027.7356540900955\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 395 R: 961.6904634535882\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 570 R: 1676.331694047078\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 411 R: 1090.507224788733\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 1000 R: 2803.9126231147707\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 468 R: 1338.4573570510327\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 359 R: 832.5051540057815\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 324 R: 771.6769128213068\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 1444.779423385759 -> 1457.7696270509791\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:446000 episode:2248 last_R: 1247.6100139777354 average_R:969.6363668890256\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 170.13457), ('average_q2', 169.97557), ('average_q_func1_loss', 4.463750019073486), ('average_q_func2_loss', 4.436318736076355), ('n_updates', 436001), ('average_entropy', -2.9144812), ('temperature', 0.035461798310279846)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:447000 episode:2251 last_R: 2391.7409302155884 average_R:986.2494538992704\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 168.39833), ('average_q2', 168.44298), ('average_q_func1_loss', 4.019424135684967), ('average_q_func2_loss', 3.891521155834198), ('n_updates', 437001), ('average_entropy', -2.816235), ('temperature', 0.03813152760267258)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:448000 episode:2253 last_R: 2380.4597958979443 average_R:1008.1342507453005\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 169.62784), ('average_q2', 169.85454), ('average_q_func1_loss', 4.298164384365082), ('average_q_func2_loss', 4.028631440401077), ('n_updates', 438001), ('average_entropy', -2.9740381), ('temperature', 0.03809793293476105)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:449000 episode:2255 last_R: 936.5995153508866 average_R:1004.4871746936255\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 169.52444), ('average_q2', 169.43234), ('average_q_func1_loss', 5.042893121242523), ('average_q_func2_loss', 4.673963146209717), ('n_updates', 439001), ('average_entropy', -3.198621), ('temperature', 0.037488870322704315)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:450000 episode:2258 last_R: 1888.0007428577285 average_R:1018.2239879539269\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 168.6969), ('average_q2', 168.52904), ('average_q_func1_loss', 4.552134261131287), ('average_q_func2_loss', 4.3422543084621426), ('n_updates', 440001), ('average_entropy', -3.124652), ('temperature', 0.038129545748233795)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 432 R: 1162.1655222668894\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 500 R: 1364.6725189172735\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 397 R: 1050.3338062401826\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 442 R: 1118.0557734983327\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 368 R: 872.3096925017421\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 403 R: 1065.3426154447855\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 414 R: 998.7787644038737\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 1000 R: 2176.56559112982\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 366 R: 794.3515734131339\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 174 R: 348.5766568038879\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:451000 episode:2260 last_R: 1388.6590674493646 average_R:1025.8167653666549\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 170.02917), ('average_q2', 170.0443), ('average_q_func1_loss', 4.082791957855225), ('average_q_func2_loss', 4.102678062915802), ('n_updates', 441001), ('average_entropy', -2.9402657), ('temperature', 0.03719203174114227)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:452000 episode:2263 last_R: 1531.8866647814518 average_R:1030.503916654696\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 171.59831), ('average_q2', 171.51482), ('average_q_func1_loss', 4.56915431022644), ('average_q_func2_loss', 4.705918335914612), ('n_updates', 442001), ('average_entropy', -3.05956), ('temperature', 0.03765387088060379)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:453000 episode:2265 last_R: 546.5759159948769 average_R:1026.957919730234\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 166.07318), ('average_q2', 166.27599), ('average_q_func1_loss', 5.688354576826096), ('average_q_func2_loss', 5.99030566573143), ('n_updates', 443001), ('average_entropy', -3.1436212), ('temperature', 0.03604687750339508)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:454000 episode:2266 last_R: 1295.6924284637153 average_R:1031.7549171880505\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 169.44022), ('average_q2', 169.27501), ('average_q_func1_loss', 4.1790802037715915), ('average_q_func2_loss', 3.9725506258010865), ('n_updates', 444001), ('average_entropy', -2.9590733), ('temperature', 0.03674427792429924)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:455000 episode:2271 last_R: 352.55843602701776 average_R:1048.539001980656\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 167.90193), ('average_q2', 167.93185), ('average_q_func1_loss', 4.994710694551468), ('average_q_func2_loss', 4.678227097988128), ('n_updates', 445001), ('average_entropy', -2.9896953), ('temperature', 0.035861700773239136)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 1000 R: 2426.7876316769266\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 489 R: 1372.2673891165355\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 362 R: 954.2440039313038\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 434 R: 1016.7772331881625\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 440 R: 1266.4226471181912\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 461 R: 1106.2868154253968\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 417 R: 1228.4234053783127\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 375 R: 805.1734149107901\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 439 R: 1295.5276191874068\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 312 R: 716.8877871113381\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:456000 episode:2275 last_R: 750.4906753622331 average_R:1045.4475975007692\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 169.39719), ('average_q2', 169.38998), ('average_q_func1_loss', 5.083632181882859), ('average_q_func2_loss', 4.902885669469834), ('n_updates', 446001), ('average_entropy', -3.1443827), ('temperature', 0.035889506340026855)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:457000 episode:2275 last_R: 750.4906753622331 average_R:1045.4475975007692\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 168.2548), ('average_q2', 168.2027), ('average_q_func1_loss', 4.3110377156734465), ('average_q_func2_loss', 4.229524103403091), ('n_updates', 447001), ('average_entropy', -2.9457564), ('temperature', 0.03702806308865547)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:458000 episode:2278 last_R: 1049.2609107699993 average_R:1058.3633275066466\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 170.00114), ('average_q2', 170.15997), ('average_q_func1_loss', 4.676631954908371), ('average_q_func2_loss', 4.496471879482269), ('n_updates', 448001), ('average_entropy', -2.9471657), ('temperature', 0.036830779165029526)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:459000 episode:2281 last_R: 870.4766345406921 average_R:1064.6658536029393\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 171.58957), ('average_q2', 171.60661), ('average_q_func1_loss', 4.506384460926056), ('average_q_func2_loss', 4.162717403173446), ('n_updates', 449001), ('average_entropy', -3.017561), ('temperature', 0.03815281763672829)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:460000 episode:2283 last_R: 773.3647084096958 average_R:1065.3059046898663\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 170.14928), ('average_q2', 170.3355), ('average_q_func1_loss', 4.1599489998817445), ('average_q_func2_loss', 4.375532950162888), ('n_updates', 450001), ('average_entropy', -2.7911003), ('temperature', 0.03755873441696167)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 1000 R: 2419.0868303512752\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 456 R: 1414.19095958035\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 1000 R: 2493.782790252928\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 1000 R: 2542.4389462469458\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 235 R: 526.102599677025\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 404 R: 1079.5919954267226\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 417 R: 1266.4204495217039\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 450 R: 1006.7194101104532\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 1000 R: 2476.8414962385013\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 620 R: 1567.1783391581455\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 1457.7696270509791 -> 1679.235381656405\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:461000 episode:2286 last_R: 2333.5233794452743 average_R:1084.206574818817\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 170.9349), ('average_q2', 170.83131), ('average_q_func1_loss', 3.93096382021904), ('average_q_func2_loss', 3.967159194946289), ('n_updates', 451001), ('average_entropy', -2.879332), ('temperature', 0.03816568851470947)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:462000 episode:2286 last_R: 2333.5233794452743 average_R:1084.206574818817\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 171.62166), ('average_q2', 171.58928), ('average_q_func1_loss', 4.2297821140289305), ('average_q_func2_loss', 4.317300064563751), ('n_updates', 452001), ('average_entropy', -2.8648002), ('temperature', 0.036758992820978165)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:463000 episode:2288 last_R: 1014.7836080107019 average_R:1093.8859159978483\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 169.56459), ('average_q2', 169.55212), ('average_q_func1_loss', 4.450845184326172), ('average_q_func2_loss', 4.243230311870575), ('n_updates', 453001), ('average_entropy', -2.9851098), ('temperature', 0.0365181602537632)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:464000 episode:2290 last_R: 1956.8322065088255 average_R:1114.6505587082686\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 172.92), ('average_q2', 173.2001), ('average_q_func1_loss', 4.442640570402145), ('average_q_func2_loss', 4.421701565980912), ('n_updates', 454001), ('average_entropy', -2.9639034), ('temperature', 0.037067681550979614)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:465000 episode:2291 last_R: 857.5541244728835 average_R:1099.7863976751642\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 171.9813), ('average_q2', 171.86812), ('average_q_func1_loss', 4.276535476446152), ('average_q_func2_loss', 4.454442048072815), ('n_updates', 455001), ('average_entropy', -2.941806), ('temperature', 0.037362225353717804)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 641 R: 1584.1465050508382\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 411 R: 1158.607862389707\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 319 R: 838.3216942421085\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 435 R: 1065.0420066129932\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 470 R: 1294.1452030070755\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 418 R: 1105.0080038436479\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 1000 R: 2218.1551524913716\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 330 R: 809.4722908318489\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 801 R: 2293.273605870624\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 352 R: 960.3490851506756\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:466000 episode:2295 last_R: 2384.0181269654886 average_R:1116.861456180016\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 171.12341), ('average_q2', 171.3205), ('average_q_func1_loss', 4.492061468362809), ('average_q_func2_loss', 4.462989895343781), ('n_updates', 456001), ('average_entropy', -2.9703805), ('temperature', 0.036353107541799545)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:467000 episode:2296 last_R: 1337.6532622148384 average_R:1124.3101088997219\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 171.74292), ('average_q2', 171.7161), ('average_q_func1_loss', 4.314926186800003), ('average_q_func2_loss', 4.464795582294464), ('n_updates', 457001), ('average_entropy', -2.9985552), ('temperature', 0.035850271582603455)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:468000 episode:2299 last_R: 1465.795055735945 average_R:1132.8340452858167\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 172.20844), ('average_q2', 172.13391), ('average_q_func1_loss', 5.8270318937301635), ('average_q_func2_loss', 5.87481578707695), ('n_updates', 458001), ('average_entropy', -3.021167), ('temperature', 0.03705160692334175)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:469000 episode:2301 last_R: 445.2205772294482 average_R:1133.0625982194829\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 171.86372), ('average_q2', 172.13014), ('average_q_func1_loss', 4.376648780107498), ('average_q_func2_loss', 4.26602534532547), ('n_updates', 459001), ('average_entropy', -2.954697), ('temperature', 0.03801601380109787)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:470000 episode:2302 last_R: 1076.2312745709135 average_R:1136.0897803021394\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 172.70502), ('average_q2', 172.22838), ('average_q_func1_loss', 4.247263082265854), ('average_q_func2_loss', 4.247237168550491), ('n_updates', 460001), ('average_entropy', -2.9068441), ('temperature', 0.03804944083094597)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 333 R: 817.8995709247258\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 329 R: 878.3754490717994\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 471 R: 1300.26225893686\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 1000 R: 2301.6258125941417\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 1000 R: 2638.3067501769165\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 617 R: 1590.9933646014383\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 362 R: 947.6215634278993\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 368 R: 1069.8467317734496\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 421 R: 1019.2753478582925\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 515 R: 1384.2689205841984\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:471000 episode:2303 last_R: 908.183988757368 average_R:1136.328706263795\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 175.41957), ('average_q2', 175.76437), ('average_q_func1_loss', 4.631890007257462), ('average_q_func2_loss', 4.7674377417564395), ('n_updates', 461001), ('average_entropy', -3.0172627), ('temperature', 0.0378008671104908)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:472000 episode:2304 last_R: 2403.732075979088 average_R:1151.288699880843\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 176.09659), ('average_q2', 175.97725), ('average_q_func1_loss', 4.28562111735344), ('average_q_func2_loss', 4.425703644752502), ('n_updates', 462001), ('average_entropy', -2.9932716), ('temperature', 0.038770854473114014)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:473000 episode:2306 last_R: 2290.78329501135 average_R:1162.4738152668233\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 174.80847), ('average_q2', 174.76572), ('average_q_func1_loss', 4.643299094438553), ('average_q_func2_loss', 4.842316012382508), ('n_updates', 463001), ('average_entropy', -2.982404), ('temperature', 0.037494778633117676)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:474000 episode:2308 last_R: 1265.2287142323653 average_R:1179.0306646571503\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 172.76659), ('average_q2', 172.69922), ('average_q_func1_loss', 4.29371454000473), ('average_q_func2_loss', 4.320958617925644), ('n_updates', 464001), ('average_entropy', -3.2078032), ('temperature', 0.037316422909498215)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:475000 episode:2310 last_R: 1199.607844358102 average_R:1194.9002373287094\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 172.26778), ('average_q2', 172.3175), ('average_q_func1_loss', 4.138941968679428), ('average_q_func2_loss', 4.371668195724487), ('n_updates', 465001), ('average_entropy', -2.9065762), ('temperature', 0.038266588002443314)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 404 R: 1093.2828004903079\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 505 R: 1483.1274682106284\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 462 R: 1359.0215424079063\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 1000 R: 2398.726360244869\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 459 R: 1305.0471723577675\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 393 R: 1129.3132513960165\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 500 R: 1372.6292699824628\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 302 R: 758.9542951656446\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 275 R: 635.2895908960782\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 407 R: 1162.2124738155167\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:476000 episode:2313 last_R: 707.3986654653138 average_R:1181.9629640028077\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 172.73209), ('average_q2', 172.4908), ('average_q_func1_loss', 4.95879152059555), ('average_q_func2_loss', 5.545964013338089), ('n_updates', 466001), ('average_entropy', -2.8780587), ('temperature', 0.03858381509780884)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:477000 episode:2313 last_R: 707.3986654653138 average_R:1181.9629640028077\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 174.637), ('average_q2', 174.63199), ('average_q_func1_loss', 4.503542811870575), ('average_q_func2_loss', 4.526716426610947), ('n_updates', 467001), ('average_entropy', -3.041968), ('temperature', 0.037057943642139435)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:478000 episode:2316 last_R: 1137.255761539475 average_R:1191.5445880335806\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 173.35863), ('average_q2', 173.2733), ('average_q_func1_loss', 5.01728538274765), ('average_q_func2_loss', 4.8873384881019595), ('n_updates', 468001), ('average_entropy', -2.9564507), ('temperature', 0.0384138785302639)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:479000 episode:2318 last_R: 2276.7831732272084 average_R:1210.6624686720743\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 173.57022), ('average_q2', 173.68651), ('average_q_func1_loss', 5.151211304664612), ('average_q_func2_loss', 5.306987091302871), ('n_updates', 469001), ('average_entropy', -2.8121743), ('temperature', 0.036982353776693344)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:480000 episode:2320 last_R: 814.6435010615301 average_R:1215.6408634564398\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 174.42575), ('average_q2', 174.16771), ('average_q_func1_loss', 4.087285597324371), ('average_q_func2_loss', 4.14377351641655), ('n_updates', 470001), ('average_entropy', -3.1324873), ('temperature', 0.0365416519343853)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 528 R: 1693.1778875003367\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 465 R: 1442.8907982939904\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 390 R: 1264.4774793321403\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 424 R: 1334.265582397912\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 384 R: 1186.452274464921\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 404 R: 1308.0629061808197\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 492 R: 1468.0972393406525\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 397 R: 1169.2163468286146\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 439 R: 1268.7194166269096\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 378 R: 970.8332140465466\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:481000 episode:2322 last_R: 1084.0866536458639 average_R:1202.7438279552707\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 173.23856), ('average_q2', 173.5458), ('average_q_func1_loss', 4.791589889526367), ('average_q_func2_loss', 4.822047590017319), ('n_updates', 471001), ('average_entropy', -2.8869898), ('temperature', 0.03764168918132782)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:482000 episode:2322 last_R: 1084.0866536458639 average_R:1202.7438279552707\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 176.1671), ('average_q2', 176.12975), ('average_q_func1_loss', 4.445747512578964), ('average_q_func2_loss', 4.502304775714874), ('n_updates', 472001), ('average_entropy', -2.9834244), ('temperature', 0.036829620599746704)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:483000 episode:2323 last_R: 2430.2848039322744 average_R:1213.0356604672309\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 172.97546), ('average_q2', 172.93024), ('average_q_func1_loss', 4.105967800617218), ('average_q_func2_loss', 4.052640450000763), ('n_updates', 473001), ('average_entropy', -2.908411), ('temperature', 0.0373421348631382)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:484000 episode:2326 last_R: 2368.211903773515 average_R:1230.1086523426386\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 174.3533), ('average_q2', 173.98758), ('average_q_func1_loss', 4.619154295921326), ('average_q_func2_loss', 4.44089981675148), ('n_updates', 474001), ('average_entropy', -3.163949), ('temperature', 0.037532396614551544)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:485000 episode:2328 last_R: 1582.4180549205723 average_R:1259.446210661025\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 174.83571), ('average_q2', 174.83015), ('average_q_func1_loss', 4.049458334445953), ('average_q_func2_loss', 4.085344709157944), ('n_updates', 475001), ('average_entropy', -2.8364966), ('temperature', 0.03766327351331711)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 1000 R: 2194.2899438875315\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 378 R: 1030.2974160919662\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 1000 R: 2306.038352401708\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 1000 R: 2309.257540975486\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 379 R: 1075.1608306856133\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 428 R: 1145.6616517748885\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 1000 R: 2239.6205594153093\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 1000 R: 2620.9449305468925\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 1000 R: 2520.3597665970337\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 444 R: 1215.161488973458\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 1679.235381656405 -> 1865.6792481349887\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:486000 episode:2332 last_R: 405.43432624948105 average_R:1249.4890842433433\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 177.25343), ('average_q2', 177.08836), ('average_q_func1_loss', 4.679778271913529), ('average_q_func2_loss', 4.707370743751526), ('n_updates', 476001), ('average_entropy', -2.895919), ('temperature', 0.03832871466875076)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:487000 episode:2332 last_R: 405.43432624948105 average_R:1249.4890842433433\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 175.69418), ('average_q2', 175.6565), ('average_q_func1_loss', 5.054984171390533), ('average_q_func2_loss', 4.995444977283478), ('n_updates', 477001), ('average_entropy', -3.0438335), ('temperature', 0.038210559636354446)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:488000 episode:2336 last_R: 1025.4602819367158 average_R:1235.274239700365\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 177.14731), ('average_q2', 176.79688), ('average_q_func1_loss', 4.730568692684174), ('average_q_func2_loss', 4.9226347136497495), ('n_updates', 478001), ('average_entropy', -2.9828238), ('temperature', 0.03827913850545883)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:489000 episode:2338 last_R: 851.3688432916767 average_R:1247.3703839375794\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 175.86076), ('average_q2', 176.05978), ('average_q_func1_loss', 5.3969350743293765), ('average_q_func2_loss', 4.825869287252426), ('n_updates', 479001), ('average_entropy', -3.013033), ('temperature', 0.03970997408032417)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:490000 episode:2339 last_R: 773.9372372239818 average_R:1245.7446993581348\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 172.8455), ('average_q2', 172.57271), ('average_q_func1_loss', 4.359528537988663), ('average_q_func2_loss', 4.19766756772995), ('n_updates', 480001), ('average_entropy', -2.9500833), ('temperature', 0.0390559621155262)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 276 R: 648.1596025314094\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 502 R: 1527.7877607703554\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 473 R: 1506.0093357682224\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 269 R: 628.1140167462155\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 1000 R: 2509.0869987536144\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 375 R: 944.076330780981\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 208 R: 416.60923195428404\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 251 R: 579.6289578646519\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 533 R: 1562.7146449657728\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 1000 R: 2557.994729523364\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:491000 episode:2340 last_R: 1477.8705056618546 average_R:1257.6588024276593\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 176.60043), ('average_q2', 176.50612), ('average_q_func1_loss', 5.226409759521484), ('average_q_func2_loss', 5.079206055402755), ('n_updates', 481001), ('average_entropy', -2.8831933), ('temperature', 0.039776936173439026)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:492000 episode:2340 last_R: 1477.8705056618546 average_R:1257.6588024276593\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 176.0681), ('average_q2', 175.94734), ('average_q_func1_loss', 4.097198362350464), ('average_q_func2_loss', 4.086010946035385), ('n_updates', 482001), ('average_entropy', -3.1634746), ('temperature', 0.03920993581414223)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:493000 episode:2343 last_R: 2549.304070825142 average_R:1303.7435433798862\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 174.2125), ('average_q2', 174.20361), ('average_q_func1_loss', 4.295656833648682), ('average_q_func2_loss', 4.422103818655014), ('n_updates', 483001), ('average_entropy', -3.1435332), ('temperature', 0.038010548800230026)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:494000 episode:2344 last_R: 2491.6092551391466 average_R:1320.0937166957983\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 177.75519), ('average_q2', 177.52693), ('average_q_func1_loss', 5.160466394424438), ('average_q_func2_loss', 5.219455969333649), ('n_updates', 484001), ('average_entropy', -2.926038), ('temperature', 0.03806734457612038)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:495000 episode:2346 last_R: 674.2940151704847 average_R:1328.1458815845854\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 175.6832), ('average_q2', 175.56743), ('average_q_func1_loss', 4.843263635635376), ('average_q_func2_loss', 4.873994265794754), ('n_updates', 485001), ('average_entropy', -3.1437972), ('temperature', 0.03681906685233116)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 690 R: 1831.6655037093917\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 529 R: 1658.9982496699017\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 406 R: 1035.0317435170532\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 1000 R: 2772.6315251444075\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 577 R: 1751.2998054462905\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 1000 R: 2400.2198926038573\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 1000 R: 2534.2893999314456\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 351 R: 836.7230024565841\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 778 R: 1950.6340617154488\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 1000 R: 2594.0972528828593\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 1865.6792481349887 -> 1936.559043707724\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:496000 episode:2348 last_R: 625.2143266278633 average_R:1327.8047552502348\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 177.73485), ('average_q2', 177.69836), ('average_q_func1_loss', 4.9223459172248845), ('average_q_func2_loss', 4.787357431650162), ('n_updates', 486001), ('average_entropy', -3.2157354), ('temperature', 0.03663774952292442)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:497000 episode:2350 last_R: 487.8693659574496 average_R:1326.2879814910202\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 178.44327), ('average_q2', 178.44377), ('average_q_func1_loss', 4.389605133533478), ('average_q_func2_loss', 4.2154611194133755), ('n_updates', 487001), ('average_entropy', -3.228139), ('temperature', 0.03697001934051514)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:498000 episode:2352 last_R: 2470.4041495733704 average_R:1329.5069862173684\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 179.0956), ('average_q2', 179.43591), ('average_q_func1_loss', 6.927291843891144), ('average_q_func2_loss', 7.692525770664215), ('n_updates', 488001), ('average_entropy', -2.9179072), ('temperature', 0.03779216855764389)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:499000 episode:2354 last_R: 2507.5150615199977 average_R:1331.5761570713976\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 177.54494), ('average_q2', 177.53523), ('average_q_func1_loss', 4.212185062170029), ('average_q_func2_loss', 4.2544476246833804), ('n_updates', 489001), ('average_entropy', -3.0317109), ('temperature', 0.038795821368694305)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:500000 episode:2357 last_R: 770.2620667538049 average_R:1331.332125722403\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 178.75438), ('average_q2', 178.63913), ('average_q_func1_loss', 5.323259563446045), ('average_q_func2_loss', 5.422560970783234), ('n_updates', 490001), ('average_entropy', -2.9680994), ('temperature', 0.03848165646195412)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 470 R: 1444.9126605245865\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 570 R: 1685.4162278332055\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 431 R: 1326.0685092518445\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 396 R: 952.7775587319782\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 309 R: 786.868218354152\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 1000 R: 2327.692425088243\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 291 R: 659.9609751697411\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 1000 R: 2734.8349530061814\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 429 R: 1327.338280688356\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 1000 R: 2449.721536850164\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:501000 episode:2359 last_R: 544.8876572273214 average_R:1316.5683458881033\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 176.45688), ('average_q2', 176.34564), ('average_q_func1_loss', 6.357023987770081), ('average_q_func2_loss', 5.62728985786438), ('n_updates', 491001), ('average_entropy', -3.0631132), ('temperature', 0.03785699978470802)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:502000 episode:2362 last_R: 1522.086926619594 average_R:1337.712370268956\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 180.0738), ('average_q2', 180.17911), ('average_q_func1_loss', 4.992894079685211), ('average_q_func2_loss', 4.691729997396469), ('n_updates', 492001), ('average_entropy', -2.9456167), ('temperature', 0.036843784153461456)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:503000 episode:2364 last_R: 823.474958568143 average_R:1329.9643211697403\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 176.5263), ('average_q2', 176.52782), ('average_q_func1_loss', 4.970540597438812), ('average_q_func2_loss', 4.972053396701813), ('n_updates', 493001), ('average_entropy', -3.029708), ('temperature', 0.037604957818984985)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:504000 episode:2367 last_R: 1529.235511295779 average_R:1334.9063392640255\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 177.35606), ('average_q2', 177.29306), ('average_q_func1_loss', 5.045090880393982), ('average_q_func2_loss', 5.062997581958771), ('n_updates', 494001), ('average_entropy', -3.1124299), ('temperature', 0.037971802055835724)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:505000 episode:2367 last_R: 1529.235511295779 average_R:1334.9063392640255\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 177.48814), ('average_q2', 177.47678), ('average_q_func1_loss', 5.265140950679779), ('average_q_func2_loss', 5.427314238548279), ('n_updates', 495001), ('average_entropy', -3.429889), ('temperature', 0.03803326562047005)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 428 R: 1313.6752738419252\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 419 R: 1368.958500384636\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 398 R: 1258.4058503153783\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 1000 R: 2541.114175407078\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 1000 R: 2471.9855346233494\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 1000 R: 2562.2761852557874\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 420 R: 1329.149270226886\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 1000 R: 2460.3660728309405\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 260 R: 643.7934310225444\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 389 R: 1220.9968392988292\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:506000 episode:2370 last_R: 1430.7607466329832 average_R:1337.3643245376265\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 176.34274), ('average_q2', 176.43294), ('average_q_func1_loss', 4.73142901301384), ('average_q_func2_loss', 4.761763279438019), ('n_updates', 496001), ('average_entropy', -3.190611), ('temperature', 0.03853394091129303)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:507000 episode:2371 last_R: 858.6982832442774 average_R:1342.425723009799\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 178.42384), ('average_q2', 178.26816), ('average_q_func1_loss', 5.704560185670853), ('average_q_func2_loss', 5.484281885623932), ('n_updates', 497001), ('average_entropy', -2.9266553), ('temperature', 0.03860015422105789)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:508000 episode:2376 last_R: 239.54804852817978 average_R:1372.2321531127757\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 181.38847), ('average_q2', 181.54701), ('average_q_func1_loss', 5.324482119083404), ('average_q_func2_loss', 5.119036597013474), ('n_updates', 498001), ('average_entropy', -3.2260473), ('temperature', 0.03825860098004341)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:509000 episode:2378 last_R: 457.1998255519548 average_R:1371.2514517755928\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 175.57701), ('average_q2', 175.5242), ('average_q_func1_loss', 5.81378128528595), ('average_q_func2_loss', 6.450317146778107), ('n_updates', 499001), ('average_entropy', -3.1176262), ('temperature', 0.03796903416514397)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:510000 episode:2380 last_R: 983.386614316805 average_R:1370.8269785078105\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 179.21211), ('average_q2', 179.32439), ('average_q_func1_loss', 5.0329539477825165), ('average_q_func2_loss', 4.713150211572647), ('n_updates', 500001), ('average_entropy', -3.020725), ('temperature', 0.03806035593152046)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 1000 R: 2672.4836510488253\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 493 R: 1639.1618051854614\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 1000 R: 2423.56642988328\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 286 R: 743.1235372022071\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 455 R: 1558.3143800141036\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 1000 R: 2552.7913108533203\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 1000 R: 2608.248401631506\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 524 R: 1698.3444281695517\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 510 R: 1692.7919513567306\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 764 R: 2560.4206248294804\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 1936.559043707724 -> 2014.9246520174465\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:511000 episode:2383 last_R: 1255.608493048286 average_R:1386.8768394871856\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 178.86707), ('average_q2', 178.98683), ('average_q_func1_loss', 4.700589723587036), ('average_q_func2_loss', 4.74967792391777), ('n_updates', 501001), ('average_entropy', -3.0582988), ('temperature', 0.037847090512514114)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:512000 episode:2385 last_R: 515.7204218744163 average_R:1382.7885523281727\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 179.38588), ('average_q2', 179.38463), ('average_q_func1_loss', 4.521013890504837), ('average_q_func2_loss', 4.306565941572189), ('n_updates', 502001), ('average_entropy', -2.982672), ('temperature', 0.03679129481315613)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:513000 episode:2387 last_R: 749.0167723921804 average_R:1361.2886102115456\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 180.49985), ('average_q2', 180.47812), ('average_q_func1_loss', 4.453849447965622), ('average_q_func2_loss', 4.447266252040863), ('n_updates', 503001), ('average_entropy', -2.8149056), ('temperature', 0.03707470744848251)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:514000 episode:2388 last_R: 1388.7115839383987 average_R:1365.0278899708228\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 181.05197), ('average_q2', 180.84406), ('average_q_func1_loss', 5.0557177853584285), ('average_q_func2_loss', 4.815832904577255), ('n_updates', 504001), ('average_entropy', -2.9627836), ('temperature', 0.03761182725429535)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:515000 episode:2391 last_R: 823.7239114387708 average_R:1362.8560983901311\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 180.50717), ('average_q2', 180.29774), ('average_q_func1_loss', 6.241357315778732), ('average_q_func2_loss', 6.315738803148269), ('n_updates', 505001), ('average_entropy', -3.0429819), ('temperature', 0.03725850582122803)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 1000 R: 2357.266044107116\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 215 R: 452.8229922356045\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 189 R: 393.3992838120792\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 341 R: 836.307006607657\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 311 R: 767.1384310857975\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 427 R: 1395.5271465913854\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 235 R: 506.4233859870405\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 264 R: 590.1956899612671\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 554 R: 1556.7772865813163\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 245 R: 595.1294761552064\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:516000 episode:2394 last_R: 624.9600186215376 average_R:1344.126776295817\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 175.8941), ('average_q2', 175.91266), ('average_q_func1_loss', 5.012469375133515), ('average_q_func2_loss', 4.849718098640442), ('n_updates', 506001), ('average_entropy', -3.0441365), ('temperature', 0.037166088819503784)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:517000 episode:2395 last_R: 2562.6244141312545 average_R:1345.9128391674747\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 180.76067), ('average_q2', 180.76672), ('average_q_func1_loss', 4.492247550487519), ('average_q_func2_loss', 4.671862673759461), ('n_updates', 507001), ('average_entropy', -2.9895923), ('temperature', 0.03812258318066597)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:518000 episode:2397 last_R: 1318.6390023379593 average_R:1347.5510739431745\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 183.40678), ('average_q2', 183.30695), ('average_q_func1_loss', 5.841365504264831), ('average_q_func2_loss', 5.830664178133011), ('n_updates', 508001), ('average_entropy', -2.7162735), ('temperature', 0.03724828362464905)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:519000 episode:2399 last_R: 445.00567644526797 average_R:1350.3315065955362\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 184.21362), ('average_q2', 184.34508), ('average_q_func1_loss', 4.834601900577545), ('average_q_func2_loss', 4.712136106491089), ('n_updates', 509001), ('average_entropy', -2.9457188), ('temperature', 0.03861723840236664)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:520000 episode:2402 last_R: 1131.9978045424878 average_R:1341.6878542588238\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 179.59824), ('average_q2', 179.74815), ('average_q_func1_loss', 4.427362816333771), ('average_q_func2_loss', 4.388644064664841), ('n_updates', 510001), ('average_entropy', -3.0813627), ('temperature', 0.03837200999259949)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 391 R: 1178.0627268804994\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 214 R: 449.418144299199\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 350 R: 988.3930093175608\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 352 R: 978.5909036421766\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 364 R: 1075.186785065242\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 267 R: 616.8790982699371\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 236 R: 544.525986435593\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 398 R: 1176.4971022116379\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 383 R: 1122.188775316796\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 1000 R: 2506.0037503196418\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:521000 episode:2403 last_R: 2620.4773537519586 average_R:1358.81078790877\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 180.13612), ('average_q2', 180.22247), ('average_q_func1_loss', 5.9511543095111845), ('average_q_func2_loss', 6.111277832984924), ('n_updates', 511001), ('average_entropy', -2.842327), ('temperature', 0.040498483926057816)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:522000 episode:2404 last_R: 2461.6238235139313 average_R:1359.389705384118\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 179.46376), ('average_q2', 179.43372), ('average_q_func1_loss', 4.522464900016785), ('average_q_func2_loss', 4.699324268102646), ('n_updates', 512001), ('average_entropy', -3.0270956), ('temperature', 0.038539666682481766)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:523000 episode:2407 last_R: 1326.1449228550428 average_R:1350.0756374015632\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 179.45578), ('average_q2', 179.44331), ('average_q_func1_loss', 4.622041761875153), ('average_q_func2_loss', 4.245039093494415), ('n_updates', 513001), ('average_entropy', -3.302522), ('temperature', 0.038841620087623596)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:524000 episode:2408 last_R: 633.3525242822932 average_R:1343.7568755020623\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 177.9056), ('average_q2', 178.02344), ('average_q_func1_loss', 5.200478038787842), ('average_q_func2_loss', 4.914267473220825), ('n_updates', 514001), ('average_entropy', -3.1346896), ('temperature', 0.03855842351913452)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:525000 episode:2412 last_R: 1528.2979623960543 average_R:1359.767761644501\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 178.70088), ('average_q2', 178.79874), ('average_q_func1_loss', 4.826335614919662), ('average_q_func2_loss', 5.116752345561981), ('n_updates', 515001), ('average_entropy', -3.058523), ('temperature', 0.037817902863025665)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 484 R: 1594.4037570573498\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 277 R: 603.5511448573449\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 1000 R: 2843.840088472474\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 546 R: 1745.8217894743582\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 1000 R: 2463.364831285732\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 411 R: 1278.1884166376683\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 662 R: 2173.5928941771053\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 1000 R: 2619.900133692558\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 468 R: 1464.9298168990179\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 387 R: 1135.1346538489067\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:526000 episode:2414 last_R: 273.7928365433099 average_R:1348.8985165888068\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 182.97299), ('average_q2', 182.99551), ('average_q_func1_loss', 5.705315134525299), ('average_q_func2_loss', 5.982469272613526), ('n_updates', 516001), ('average_entropy', -3.036033), ('temperature', 0.038764528930187225)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:527000 episode:2417 last_R: 929.8272698532923 average_R:1329.2071953864765\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 179.02078), ('average_q2', 179.38731), ('average_q_func1_loss', 5.523889038562775), ('average_q_func2_loss', 5.311338422298431), ('n_updates', 517001), ('average_entropy', -3.2332447), ('temperature', 0.03916331008076668)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:528000 episode:2417 last_R: 929.8272698532923 average_R:1329.2071953864765\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 182.31216), ('average_q2', 182.33994), ('average_q_func1_loss', 6.477753736972809), ('average_q_func2_loss', 6.485924257040024), ('n_updates', 518001), ('average_entropy', -2.9511333), ('temperature', 0.04012693464756012)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:529000 episode:2418 last_R: 1364.8232581410566 average_R:1320.0875962356151\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 181.1074), ('average_q2', 181.02228), ('average_q_func1_loss', 4.3742965841293335), ('average_q_func2_loss', 4.234466564655304), ('n_updates', 519001), ('average_entropy', -2.947513), ('temperature', 0.040060412138700485)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:530000 episode:2420 last_R: 1816.2854916560925 average_R:1345.120086798386\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 178.4997), ('average_q2', 178.65639), ('average_q_func1_loss', 4.615832898616791), ('average_q_func2_loss', 4.585719327926636), ('n_updates', 520001), ('average_entropy', -2.8329422), ('temperature', 0.039040833711624146)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 300 R: 792.0725749661362\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 301 R: 760.2123783274808\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 313 R: 857.8417935199512\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 1000 R: 2345.4848090749324\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 519 R: 1630.0176087705684\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 456 R: 1361.3159346998596\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 237 R: 544.6429108533142\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 385 R: 1052.9867300003737\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 232 R: 524.1663005822709\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 396 R: 973.9540860867951\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:531000 episode:2423 last_R: 852.1001789921371 average_R:1342.1006307167845\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 181.41351), ('average_q2', 181.14722), ('average_q_func1_loss', 5.3516814494133), ('average_q_func2_loss', 5.279622932672501), ('n_updates', 521001), ('average_entropy', -3.286096), ('temperature', 0.04037133976817131)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:532000 episode:2426 last_R: 515.761435678587 average_R:1328.6138303644404\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 181.95056), ('average_q2', 182.1565), ('average_q_func1_loss', 4.791489145755768), ('average_q_func2_loss', 4.8813621306419375), ('n_updates', 522001), ('average_entropy', -2.8783145), ('temperature', 0.038177281618118286)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:533000 episode:2428 last_R: 532.9360290334098 average_R:1303.29125160753\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 178.95107), ('average_q2', 178.84843), ('average_q_func1_loss', 4.668201458454132), ('average_q_func2_loss', 4.629205205440521), ('n_updates', 523001), ('average_entropy', -3.1827333), ('temperature', 0.0381590910255909)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:534000 episode:2431 last_R: 1291.6598888284655 average_R:1302.9157312589323\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 183.4231), ('average_q2', 183.25577), ('average_q_func1_loss', 6.231436479091644), ('average_q_func2_loss', 6.231192028522491), ('n_updates', 524001), ('average_entropy', -2.7555025), ('temperature', 0.03973706066608429)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:535000 episode:2431 last_R: 1291.6598888284655 average_R:1302.9157312589323\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 181.8094), ('average_q2', 182.05203), ('average_q_func1_loss', 4.506993186473847), ('average_q_func2_loss', 4.571029976606369), ('n_updates', 525001), ('average_entropy', -2.8473306), ('temperature', 0.03777286410331726)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 466 R: 1265.1715721822086\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 1000 R: 2520.68153474959\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 1000 R: 2502.8852188292135\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 401 R: 1211.0429211220287\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 468 R: 1450.4718458704533\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 154 R: 309.857747742873\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 423 R: 1110.6692286611903\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 248 R: 494.8429159158233\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 230 R: 471.1706012541508\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 429 R: 1314.0707694810076\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:536000 episode:2435 last_R: 2518.4396540579864 average_R:1329.2262191561065\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 181.38736), ('average_q2', 181.31825), ('average_q_func1_loss', 4.034623074531555), ('average_q_func2_loss', 4.0762574577331545), ('n_updates', 526001), ('average_entropy', -2.7285924), ('temperature', 0.03884965181350708)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:537000 episode:2437 last_R: 443.1679269511128 average_R:1303.0096976259329\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 179.51236), ('average_q2', 179.4039), ('average_q_func1_loss', 6.086563458442688), ('average_q_func2_loss', 5.843817286491394), ('n_updates', 527001), ('average_entropy', -3.0308585), ('temperature', 0.03839627280831337)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:538000 episode:2442 last_R: 866.4914995719125 average_R:1292.905761919522\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 180.06401), ('average_q2', 180.316), ('average_q_func1_loss', 7.610565898418426), ('average_q_func2_loss', 7.022374606132507), ('n_updates', 528001), ('average_entropy', -3.0283637), ('temperature', 0.03808991238474846)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:539000 episode:2450 last_R: 227.9942637539823 average_R:1194.0791517317195\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 183.08716), ('average_q2', 183.08705), ('average_q_func1_loss', 6.990444173812866), ('average_q_func2_loss', 5.939925694465638), ('n_updates', 529001), ('average_entropy', -3.0189037), ('temperature', 0.03917191922664642)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:540000 episode:2456 last_R: 353.30527796363805 average_R:1119.6092425458075\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 183.19046), ('average_q2', 183.23366), ('average_q_func1_loss', 5.343413231372833), ('average_q_func2_loss', 5.23345207452774), ('n_updates', 530001), ('average_entropy', -3.031562), ('temperature', 0.04003395512700081)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 163 R: 388.7451179140363\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 237 R: 667.6953450557845\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 163 R: 388.82997382306206\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 170 R: 417.2445272098786\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 170 R: 411.82549354647415\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 111 R: 225.77828511151571\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 164 R: 380.20076597149284\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 168 R: 409.8723433012415\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 159 R: 369.648294266072\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 161 R: 365.6474653472862\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:541000 episode:2461 last_R: 378.5585343860668 average_R:1079.9607136475577\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 178.64886), ('average_q2', 178.5981), ('average_q_func1_loss', 6.736976203918457), ('average_q_func2_loss', 6.594843833446503), ('n_updates', 531001), ('average_entropy', -3.1289606), ('temperature', 0.03954923897981644)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:542000 episode:2469 last_R: 391.9372878311608 average_R:1023.5914351084186\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 184.54442), ('average_q2', 184.4044), ('average_q_func1_loss', 4.962410953044891), ('average_q_func2_loss', 4.936369137763977), ('n_updates', 532001), ('average_entropy', -2.883796), ('temperature', 0.039792608469724655)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:543000 episode:2471 last_R: 436.89077323486333 average_R:1009.4133531119833\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 183.3064), ('average_q2', 183.33276), ('average_q_func1_loss', 6.322115058898926), ('average_q_func2_loss', 6.991002385616302), ('n_updates', 533001), ('average_entropy', -3.1705978), ('temperature', 0.03873471915721893)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:544000 episode:2473 last_R: 1059.8524262258597 average_R:995.6510931455338\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 183.23875), ('average_q2', 183.267), ('average_q_func1_loss', 5.750634520053864), ('average_q_func2_loss', 5.673933126926422), ('n_updates', 534001), ('average_entropy', -2.9754806), ('temperature', 0.039522796869277954)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:545000 episode:2475 last_R: 978.7351412026164 average_R:976.9216352240172\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 183.74037), ('average_q2', 183.8404), ('average_q_func1_loss', 5.069740355014801), ('average_q_func2_loss', 4.97076899766922), ('n_updates', 535001), ('average_entropy', -2.987553), ('temperature', 0.0394042432308197)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 340 R: 759.0944584030759\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 462 R: 1333.5911075431711\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 1000 R: 2613.8548031548653\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 371 R: 1117.547042001587\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 505 R: 1592.554091269681\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 383 R: 1054.1489812291259\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 372 R: 896.4209336715356\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 323 R: 813.5266436964598\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 1000 R: 2548.7246217753795\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 1000 R: 2457.1333691872733\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:546000 episode:2477 last_R: 1489.9588948337753 average_R:980.6963857698217\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 181.31505), ('average_q2', 180.93611), ('average_q_func1_loss', 5.674220023155212), ('average_q_func2_loss', 5.731741530895233), ('n_updates', 536001), ('average_entropy', -2.7845762), ('temperature', 0.03993261978030205)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:547000 episode:2479 last_R: 2549.5731477450927 average_R:998.5067528149593\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 181.30278), ('average_q2', 181.4376), ('average_q_func1_loss', 5.285066792964935), ('average_q_func2_loss', 5.106998379230499), ('n_updates', 537001), ('average_entropy', -2.9246192), ('temperature', 0.0385621003806591)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:548000 episode:2480 last_R: 1208.6668221439145 average_R:1000.7595548932305\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 181.96756), ('average_q2', 182.211), ('average_q_func1_loss', 6.15850115776062), ('average_q_func2_loss', 7.128214755058289), ('n_updates', 538001), ('average_entropy', -2.8964972), ('temperature', 0.03969604894518852)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:549000 episode:2484 last_R: 2752.7158859646556 average_R:1011.8572620364549\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 183.5624), ('average_q2', 183.90816), ('average_q_func1_loss', 5.224624977111817), ('average_q_func2_loss', 5.701461555957795), ('n_updates', 539001), ('average_entropy', -2.9866734), ('temperature', 0.03910645842552185)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:550000 episode:2486 last_R: 674.6499176113593 average_R:1018.3710855148005\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 182.46553), ('average_q2', 182.23473), ('average_q_func1_loss', 5.214934012889862), ('average_q_func2_loss', 5.127829309701919), ('n_updates', 540001), ('average_entropy', -3.0565665), ('temperature', 0.03926805406808853)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 1000 R: 2548.2192813956462\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 355 R: 1051.2502838774592\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 1000 R: 2610.053056926058\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 356 R: 1047.7216099558036\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 311 R: 887.710186804801\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 443 R: 1387.6781781174802\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 461 R: 1427.443840362055\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 493 R: 1517.6264247013696\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 1000 R: 2689.899173129721\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 320 R: 929.4102777153141\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:551000 episode:2488 last_R: 630.626032992205 average_R:1016.9565254098902\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 182.6487), ('average_q2', 182.85056), ('average_q_func1_loss', 5.204504809379578), ('average_q_func2_loss', 5.40273684501648), ('n_updates', 541001), ('average_entropy', -3.0051832), ('temperature', 0.03859281539916992)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:552000 episode:2489 last_R: 1369.895394451562 average_R:1022.7418992459762\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 181.11444), ('average_q2', 181.10963), ('average_q_func1_loss', 5.513170912265777), ('average_q_func2_loss', 5.189251854419708), ('n_updates', 542001), ('average_entropy', -2.8820624), ('temperature', 0.037695106118917465)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:553000 episode:2490 last_R: 334.7975219843416 average_R:1000.6236512684919\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 180.80565), ('average_q2', 180.66353), ('average_q_func1_loss', 4.59351113319397), ('average_q_func2_loss', 4.763188188076019), ('n_updates', 543001), ('average_entropy', -2.6476421), ('temperature', 0.0379464328289032)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:554000 episode:2494 last_R: 442.41700308004954 average_R:1045.3671884769965\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 180.85172), ('average_q2', 181.2409), ('average_q_func1_loss', 6.806374635696411), ('average_q_func2_loss', 6.1363362634181975), ('n_updates', 544001), ('average_entropy', -3.0482001), ('temperature', 0.03783859312534332)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:555000 episode:2494 last_R: 442.41700308004954 average_R:1045.3671884769965\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 181.29756), ('average_q2', 181.35466), ('average_q_func1_loss', 5.3417480230331424), ('average_q_func2_loss', 5.223252055644989), ('n_updates', 545001), ('average_entropy', -2.984438), ('temperature', 0.0384339764714241)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 417 R: 1064.8088978242051\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 1000 R: 2377.96621961207\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 469 R: 1275.5275136126693\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 539 R: 1545.0594083700134\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 1000 R: 2440.1676710862957\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 392 R: 1053.0835504945742\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 466 R: 1298.0212775191033\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 268 R: 584.8468301544431\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 1000 R: 2403.391070891681\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 476 R: 1534.4224659114302\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:556000 episode:2498 last_R: 1164.3833726312268 average_R:1021.7420710498004\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 183.51527), ('average_q2', 183.3283), ('average_q_func1_loss', 5.6360414242744445), ('average_q_func2_loss', 5.758538904190064), ('n_updates', 546001), ('average_entropy', -3.0549304), ('temperature', 0.03818284347653389)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:557000 episode:2501 last_R: 433.6015649007887 average_R:1014.1092620876069\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 185.93887), ('average_q2', 186.30475), ('average_q_func1_loss', 5.189278767108918), ('average_q_func2_loss', 4.9946290874481205), ('n_updates', 547001), ('average_entropy', -3.0709405), ('temperature', 0.03732398897409439)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:558000 episode:2502 last_R: 2625.1516126330353 average_R:1029.0408001685125\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 185.47475), ('average_q2', 185.46547), ('average_q_func1_loss', 6.179152855873108), ('average_q_func2_loss', 6.617546963691711), ('n_updates', 548001), ('average_entropy', -2.9293041), ('temperature', 0.03751097992062569)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:559000 episode:2504 last_R: 1362.937433298059 average_R:1006.5129752415857\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 182.40666), ('average_q2', 182.68091), ('average_q_func1_loss', 5.488089864253998), ('average_q_func2_loss', 5.652834092378616), ('n_updates', 549001), ('average_entropy', -3.0603487), ('temperature', 0.0386870875954628)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:560000 episode:2505 last_R: 308.11779394653195 average_R:990.3217816943637\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 182.60669), ('average_q2', 182.77), ('average_q_func1_loss', 7.662314002513885), ('average_q_func2_loss', 7.150444433689118), ('n_updates', 550001), ('average_entropy', -3.0193028), ('temperature', 0.04047190397977829)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 276 R: 677.1208925656719\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 352 R: 955.4897343663523\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 1000 R: 2443.447528764291\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 552 R: 1876.8205864745355\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 1000 R: 2461.9373398730386\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 481 R: 1511.9927274975378\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 347 R: 1074.606091938427\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 448 R: 1378.829659898619\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 1000 R: 2481.668070062269\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 1000 R: 2791.384227335453\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:561000 episode:2508 last_R: 1235.3789557000762 average_R:992.524057471859\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 182.6816), ('average_q2', 182.60165), ('average_q_func1_loss', 5.9698832821846), ('average_q_func2_loss', 5.930473374128342), ('n_updates', 551001), ('average_entropy', -3.0743794), ('temperature', 0.04003625363111496)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:562000 episode:2510 last_R: 1125.2867750311975 average_R:990.0294144632031\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 179.6241), ('average_q2', 179.7844), ('average_q_func1_loss', 5.471918604373932), ('average_q_func2_loss', 5.481057136058808), ('n_updates', 552001), ('average_entropy', -3.1368215), ('temperature', 0.04006354510784149)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:563000 episode:2513 last_R: 2507.4487866427194 average_R:993.8042424325628\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 183.83842), ('average_q2', 183.71877), ('average_q_func1_loss', 5.757735233306885), ('average_q_func2_loss', 6.205270626544952), ('n_updates', 553001), ('average_entropy', -3.1434531), ('temperature', 0.03943780064582825)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:564000 episode:2514 last_R: 1561.2628188382844 average_R:1006.6789422555124\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 183.40312), ('average_q2', 183.66922), ('average_q_func1_loss', 6.560445506572723), ('average_q_func2_loss', 6.80165860414505), ('n_updates', 554001), ('average_entropy', -3.1578183), ('temperature', 0.03847823664546013)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:565000 episode:2518 last_R: 1035.367558310339 average_R:998.4989014062884\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 182.3526), ('average_q2', 182.77654), ('average_q_func1_loss', 5.676199426651001), ('average_q_func2_loss', 5.46870339512825), ('n_updates', 555001), ('average_entropy', -2.9454086), ('temperature', 0.03960204869508743)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 379 R: 983.2691074600428\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 446 R: 1234.1995040378995\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 429 R: 1350.3890456505765\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 354 R: 1042.8552476005698\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 254 R: 597.6865929533036\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 1000 R: 2400.4317223371418\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 392 R: 1019.3279383912271\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 442 R: 1307.5294764452995\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 340 R: 1003.0041485146509\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 239 R: 550.5393652999286\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:566000 episode:2518 last_R: 1035.367558310339 average_R:998.4989014062884\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 182.96385), ('average_q2', 182.57047), ('average_q_func1_loss', 5.96550918340683), ('average_q_func2_loss', 6.134828817844391), ('n_updates', 556001), ('average_entropy', -3.097209), ('temperature', 0.039824195206165314)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:567000 episode:2519 last_R: 903.2929626350508 average_R:981.7232633955427\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 181.90694), ('average_q2', 181.6992), ('average_q_func1_loss', 5.541329189538955), ('average_q_func2_loss', 5.702205193042755), ('n_updates', 557001), ('average_entropy', -3.0447567), ('temperature', 0.041210368275642395)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:568000 episode:2520 last_R: 2682.8900510901067 average_R:990.3893089898827\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 181.37187), ('average_q2', 181.348), ('average_q_func1_loss', 7.116824712753296), ('average_q_func2_loss', 6.118897929191589), ('n_updates', 558001), ('average_entropy', -3.0110621), ('temperature', 0.04098765179514885)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:569000 episode:2523 last_R: 878.1836871001926 average_R:990.3253911049505\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 185.12642), ('average_q2', 185.0956), ('average_q_func1_loss', 5.531847743988037), ('average_q_func2_loss', 5.450206096172333), ('n_updates', 559001), ('average_entropy', -3.0652936), ('temperature', 0.04037265479564667)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:570000 episode:2526 last_R: 583.2995499508824 average_R:1005.5300813786048\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 183.47842), ('average_q2', 183.61429), ('average_q_func1_loss', 5.903931429386139), ('average_q_func2_loss', 5.833546633720398), ('n_updates', 560001), ('average_entropy', -3.2357872), ('temperature', 0.038814444094896317)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 491 R: 1349.0596422822393\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 388 R: 980.3112239926654\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 349 R: 810.1947189976639\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 351 R: 893.5191917398192\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 384 R: 1096.0348994306855\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 311 R: 821.2425187614692\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 286 R: 683.6871265188275\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 362 R: 1074.1989384100234\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 1000 R: 2537.0912768645035\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 480 R: 1455.8006092611304\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:571000 episode:2528 last_R: 1268.4706208165446 average_R:1009.596759294692\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 184.70166), ('average_q2', 184.77896), ('average_q_func1_loss', 6.098524872064591), ('average_q_func2_loss', 5.506471450328827), ('n_updates', 561001), ('average_entropy', -2.9377952), ('temperature', 0.039525169879198074)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:572000 episode:2529 last_R: 834.0631459467369 average_R:1004.8080206802116\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 183.3919), ('average_q2', 182.98076), ('average_q_func1_loss', 5.322474312782288), ('average_q_func2_loss', 5.306755232810974), ('n_updates', 562001), ('average_entropy', -3.1951919), ('temperature', 0.03893328830599785)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:573000 episode:2530 last_R: 1674.1658682548677 average_R:1016.0594394490518\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 184.13615), ('average_q2', 184.23967), ('average_q_func1_loss', 5.814488141536713), ('average_q_func2_loss', 5.466946246623993), ('n_updates', 563001), ('average_entropy', -3.0572147), ('temperature', 0.03856208175420761)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:574000 episode:2530 last_R: 1674.1658682548677 average_R:1016.0594394490518\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 184.69676), ('average_q2', 184.46063), ('average_q_func1_loss', 5.944265367984772), ('average_q_func2_loss', 5.939916422367096), ('n_updates', 564001), ('average_entropy', -2.9130743), ('temperature', 0.039539359509944916)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:575000 episode:2531 last_R: 2488.982647167686 average_R:1028.032667032444\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 187.09213), ('average_q2', 187.01103), ('average_q_func1_loss', 5.712111032009124), ('average_q_func2_loss', 5.913038990497589), ('n_updates', 565001), ('average_entropy', -3.2399614), ('temperature', 0.039589133113622665)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 420 R: 1237.2896384284998\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 285 R: 661.0663553871295\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 480 R: 1310.2332594374175\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 1000 R: 2421.078763017082\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 446 R: 1115.8338512653386\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 434 R: 1224.8232373598205\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 1000 R: 2365.8894176680274\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 1000 R: 2594.423900846616\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 325 R: 728.696720885494\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 584 R: 1609.1037325087236\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:576000 episode:2534 last_R: 2384.528773548884 average_R:1049.7842288670236\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 182.58707), ('average_q2', 182.65956), ('average_q_func1_loss', 6.942706871032715), ('average_q_func2_loss', 6.824386215209961), ('n_updates', 566001), ('average_entropy', -2.9877214), ('temperature', 0.04188278317451477)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:577000 episode:2535 last_R: 2513.400062362403 average_R:1049.7338329500678\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 184.41174), ('average_q2', 184.4336), ('average_q_func1_loss', 6.107814044952392), ('average_q_func2_loss', 6.270504875183105), ('n_updates', 567001), ('average_entropy', -2.8586645), ('temperature', 0.040094587951898575)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:578000 episode:2540 last_R: 673.4903149503389 average_R:1053.7812657845743\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 183.1925), ('average_q2', 183.23982), ('average_q_func1_loss', 6.561258897781372), ('average_q_func2_loss', 6.400858409404755), ('n_updates', 568001), ('average_entropy', -2.9626944), ('temperature', 0.04058465361595154)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:579000 episode:2541 last_R: 490.8783798452123 average_R:1054.204784409409\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 185.59593), ('average_q2', 185.65662), ('average_q_func1_loss', 4.892815337181092), ('average_q_func2_loss', 4.909154322147369), ('n_updates', 569001), ('average_entropy', -2.8661938), ('temperature', 0.04002091661095619)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:580000 episode:2544 last_R: 272.9627776575787 average_R:1059.1958839872048\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 186.56567), ('average_q2', 186.95647), ('average_q_func1_loss', 5.861782101392746), ('average_q_func2_loss', 5.696128934621811), ('n_updates', 570001), ('average_entropy', -3.14598), ('temperature', 0.03935600444674492)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 488 R: 1672.2895439736376\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 280 R: 630.1420851987642\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 483 R: 1512.1208253226803\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 491 R: 1411.3438067995116\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 412 R: 1227.9138517056024\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 342 R: 1095.5110789040655\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 335 R: 837.0470195367067\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 435 R: 1301.6173325569225\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 485 R: 1506.5740400366265\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 370 R: 1219.3085172461829\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:581000 episode:2547 last_R: 1642.204657719877 average_R:1097.4329192889822\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 184.83543), ('average_q2', 185.06682), ('average_q_func1_loss', 6.1347056269645694), ('average_q_func2_loss', 5.6009943437576295), ('n_updates', 571001), ('average_entropy', -2.9189413), ('temperature', 0.0390072762966156)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:582000 episode:2549 last_R: 1330.1345672486761 average_R:1112.3865258788285\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 184.16614), ('average_q2', 184.38115), ('average_q_func1_loss', 6.681279282569886), ('average_q_func2_loss', 6.898915848731995), ('n_updates', 572001), ('average_entropy', -2.9937115), ('temperature', 0.03897286206483841)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:583000 episode:2550 last_R: 891.6554382682457 average_R:1119.023137623971\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 189.30257), ('average_q2', 189.31027), ('average_q_func1_loss', 5.285792775154114), ('average_q_func2_loss', 5.2332088351249695), ('n_updates', 573001), ('average_entropy', -3.045261), ('temperature', 0.03934865817427635)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:584000 episode:2554 last_R: 645.524831868501 average_R:1148.2468621274832\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 185.33276), ('average_q2', 185.41484), ('average_q_func1_loss', 6.733080308437348), ('average_q_func2_loss', 6.516788268089295), ('n_updates', 574001), ('average_entropy', -3.2025552), ('temperature', 0.03968140110373497)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:585000 episode:2554 last_R: 645.524831868501 average_R:1148.2468621274832\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 186.35747), ('average_q2', 186.27654), ('average_q_func1_loss', 6.326507778167724), ('average_q_func2_loss', 6.597172756195068), ('n_updates', 575001), ('average_entropy', -2.8435273), ('temperature', 0.038458265364170074)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 528 R: 1577.3314579789371\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 521 R: 1520.5943926638327\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 351 R: 838.9479019844157\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 1000 R: 2394.8180546331337\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 1000 R: 2460.5302771267834\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 507 R: 1484.8667822071618\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 1000 R: 2446.5404780120784\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 1000 R: 2411.9192733679383\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 411 R: 1213.4879869177055\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 1000 R: 2541.930600014118\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:586000 episode:2558 last_R: 1284.4136649843977 average_R:1188.1924202106834\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 188.13628), ('average_q2', 188.07924), ('average_q_func1_loss', 5.314657938480377), ('average_q_func2_loss', 5.3566889214515685), ('n_updates', 576001), ('average_entropy', -3.0112936), ('temperature', 0.04000501334667206)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:587000 episode:2559 last_R: 562.8749370823459 average_R:1191.5692146169129\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 190.75537), ('average_q2', 190.53793), ('average_q_func1_loss', 6.0123740935325625), ('average_q_func2_loss', 5.758062660694122), ('n_updates', 577001), ('average_entropy', -2.9309952), ('temperature', 0.03877101093530655)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:588000 episode:2561 last_R: 808.2624668072117 average_R:1194.9529583837168\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 183.79521), ('average_q2', 183.70366), ('average_q_func1_loss', 6.773577938079834), ('average_q_func2_loss', 7.08377177119255), ('n_updates', 578001), ('average_entropy', -3.184902), ('temperature', 0.039640508592128754)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:589000 episode:2563 last_R: 841.4127084024585 average_R:1214.3818048297012\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 186.11618), ('average_q2', 186.04805), ('average_q_func1_loss', 5.925306146144867), ('average_q_func2_loss', 6.003780331611633), ('n_updates', 579001), ('average_entropy', -2.824484), ('temperature', 0.03847673162817955)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:590000 episode:2565 last_R: 1276.9934610292723 average_R:1228.0278589957024\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 182.97704), ('average_q2', 183.0565), ('average_q_func1_loss', 6.0223516178131105), ('average_q_func2_loss', 5.7184110248088835), ('n_updates', 580001), ('average_entropy', -2.9123483), ('temperature', 0.038696520030498505)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 344 R: 857.9227234980921\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 1000 R: 2383.32504695415\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 387 R: 1053.8599747833493\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 314 R: 921.377357435681\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 184 R: 407.75380302520256\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 403 R: 957.3667795986952\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 434 R: 1291.844557884214\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 363 R: 864.8717962506064\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 459 R: 1224.6468574794385\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 337 R: 841.1191432809455\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:591000 episode:2566 last_R: 2342.435529749421 average_R:1246.8903059278136\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 185.6523), ('average_q2', 186.11111), ('average_q_func1_loss', 5.364539589881897), ('average_q_func2_loss', 5.568525557518005), ('n_updates', 581001), ('average_entropy', -2.8766172), ('temperature', 0.038901880383491516)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:592000 episode:2568 last_R: 1180.5688092511239 average_R:1275.6673383624643\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 185.84431), ('average_q2', 185.8048), ('average_q_func1_loss', 6.018610403537751), ('average_q_func2_loss', 6.05972493648529), ('n_updates', 582001), ('average_entropy', -3.1124465), ('temperature', 0.040931325405836105)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:593000 episode:2570 last_R: 177.61974561314932 average_R:1276.5251829714443\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 187.04918), ('average_q2', 186.9966), ('average_q_func1_loss', 5.450190050601959), ('average_q_func2_loss', 5.899412705898285), ('n_updates', 583001), ('average_entropy', -3.017949), ('temperature', 0.040418222546577454)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:594000 episode:2574 last_R: 254.365627349874 average_R:1285.102262125679\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 186.3681), ('average_q2', 186.44131), ('average_q_func1_loss', 7.029053814411164), ('average_q_func2_loss', 6.951961898803711), ('n_updates', 584001), ('average_entropy', -3.1874177), ('temperature', 0.03950619697570801)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:595000 episode:2576 last_R: 584.1218543300249 average_R:1300.4984628406582\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 180.65715), ('average_q2', 180.44127), ('average_q_func1_loss', 5.308326325416565), ('average_q_func2_loss', 5.20070995092392), ('n_updates', 585001), ('average_entropy', -3.1610215), ('temperature', 0.040448613464832306)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 489 R: 1480.1811214420159\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 1000 R: 2355.413908497799\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 386 R: 1002.2995399557195\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 635 R: 2020.2565330659543\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 445 R: 1345.4937033772383\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 583 R: 1767.7762635061817\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 727 R: 2027.043573690341\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 555 R: 1675.3765919839086\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 393 R: 1126.8586187899334\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 1000 R: 2394.0164717613698\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:596000 episode:2577 last_R: 1108.0379937221971 average_R:1296.6792538295426\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 185.0168), ('average_q2', 184.7548), ('average_q_func1_loss', 5.781017854213714), ('average_q_func2_loss', 5.906811408996582), ('n_updates', 586001), ('average_entropy', -3.124664), ('temperature', 0.042385831475257874)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:597000 episode:2578 last_R: 1191.5398060106113 average_R:1303.4383067416438\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 188.57327), ('average_q2', 188.68895), ('average_q_func1_loss', 6.831426792144775), ('average_q_func2_loss', 6.833091747760773), ('n_updates', 587001), ('average_entropy', -3.0733163), ('temperature', 0.03980173543095589)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:598000 episode:2580 last_R: 1080.1765365249605 average_R:1301.6038827695065\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 184.66019), ('average_q2', 184.44254), ('average_q_func1_loss', 5.133364969491959), ('average_q_func2_loss', 4.931223142147064), ('n_updates', 588001), ('average_entropy', -3.0162897), ('temperature', 0.041005153208971024)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:599000 episode:2582 last_R: 1273.7677417810648 average_R:1307.8386269846114\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 184.20924), ('average_q2', 184.36635), ('average_q_func1_loss', 6.64785007238388), ('average_q_func2_loss', 6.865044503211975), ('n_updates', 589001), ('average_entropy', -2.8728225), ('temperature', 0.04030478373169899)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/cobblestone_loose_v1 step:600000 episode:2585 last_R: 2390.808830353146 average_R:1302.330650651904\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 185.23744), ('average_q2', 185.03651), ('average_q_func1_loss', 5.884809596538544), ('average_q_func2_loss', 5.705892364978791), ('n_updates', 590001), ('average_entropy', -3.0248394), ('temperature', 0.038002315908670425)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 1000 R: 2319.6767324814155\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 1000 R: 2265.076967524834\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 550 R: 1682.7462121339126\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 1000 R: 2352.3200985337076\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 1000 R: 2365.9592077724074\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 1000 R: 2352.3117288911553\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 1000 R: 2253.446165424092\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 1000 R: 2377.1184153205963\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 456 R: 1373.3587909336377\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 600 R: 1755.9261168963528\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 2014.9246520174465 -> 2109.794043591211\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/best\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/cobblestone_loose_v1/600000_finish\n"]},{"output_type":"execute_result","data":{"text/plain":["(<pfrl.agents.soft_actor_critic.SoftActorCritic at 0x7cb3c6af1660>,\n"," [{'average_q1': nan,\n","   'average_q2': nan,\n","   'average_q_func1_loss': nan,\n","   'average_q_func2_loss': nan,\n","   'n_updates': 0,\n","   'average_entropy': nan,\n","   'temperature': 1.0,\n","   'eval_score': 111.98945070058478},\n","  {'average_q1': 0.30996758,\n","   'average_q2': -0.089171775,\n","   'average_q_func1_loss': 2.8815677165985107,\n","   'average_q_func2_loss': 3.7019572257995605,\n","   'n_updates': 1,\n","   'average_entropy': 1.7611058,\n","   'temperature': 0.9997000694274902,\n","   'eval_score': 139.40115515211141},\n","  {'average_q1': 45.79043,\n","   'average_q2': 45.68711,\n","   'average_q_func1_loss': 4.45368060708046,\n","   'average_q_func2_loss': 4.331842510700226,\n","   'n_updates': 5001,\n","   'average_entropy': 0.02917948,\n","   'temperature': 0.31583425402641296,\n","   'eval_score': 146.52026630102586},\n","  {'average_q1': 75.33048,\n","   'average_q2': 75.54939,\n","   'average_q_func1_loss': 7.732079312801361,\n","   'average_q_func2_loss': 7.249326014518738,\n","   'n_updates': 10001,\n","   'average_entropy': -1.1679319,\n","   'temperature': 0.12129270285367966,\n","   'eval_score': 190.38628491847825},\n","  {'average_q1': 93.844536,\n","   'average_q2': 93.646065,\n","   'average_q_func1_loss': 9.150508782863618,\n","   'average_q_func2_loss': 8.399624762535096,\n","   'n_updates': 15001,\n","   'average_entropy': -2.518567,\n","   'temperature': 0.06502047926187515,\n","   'eval_score': 222.57163994728404},\n","  {'average_q1': 101.18153,\n","   'average_q2': 101.07422,\n","   'average_q_func1_loss': 9.104095113277435,\n","   'average_q_func2_loss': 7.992450640201569,\n","   'n_updates': 20001,\n","   'average_entropy': -3.379538,\n","   'temperature': 0.04633897915482521,\n","   'eval_score': 224.68935768351997},\n","  {'average_q1': 105.11449,\n","   'average_q2': 104.95011,\n","   'average_q_func1_loss': 7.569844057559967,\n","   'average_q_func2_loss': 6.4619205057621,\n","   'n_updates': 25001,\n","   'average_entropy': -3.4582424,\n","   'temperature': 0.051744040101766586,\n","   'eval_score': 159.3401754635977},\n","  {'average_q1': 105.42448,\n","   'average_q2': 105.11164,\n","   'average_q_func1_loss': 7.542997193336487,\n","   'average_q_func2_loss': 6.822030749320984,\n","   'n_updates': 30001,\n","   'average_entropy': -3.0915332,\n","   'temperature': 0.05395682156085968,\n","   'eval_score': 196.82946790146988},\n","  {'average_q1': 106.25495,\n","   'average_q2': 106.24013,\n","   'average_q_func1_loss': 6.741753259897232,\n","   'average_q_func2_loss': 6.287002379894257,\n","   'n_updates': 35001,\n","   'average_entropy': -2.8682325,\n","   'temperature': 0.044154804199934006,\n","   'eval_score': 286.22395874389906},\n","  {'average_q1': 108.88179,\n","   'average_q2': 108.97459,\n","   'average_q_func1_loss': 6.378646149635315,\n","   'average_q_func2_loss': 5.651487646102905,\n","   'n_updates': 40001,\n","   'average_entropy': -3.1349432,\n","   'temperature': 0.05296959728002548,\n","   'eval_score': 362.0658112999552},\n","  {'average_q1': 110.19309,\n","   'average_q2': 110.05947,\n","   'average_q_func1_loss': 5.637869732379913,\n","   'average_q_func2_loss': 5.403436017036438,\n","   'n_updates': 45001,\n","   'average_entropy': -2.7802205,\n","   'temperature': 0.05209958925843239,\n","   'eval_score': 853.3628503912063},\n","  {'average_q1': 115.08695,\n","   'average_q2': 115.04245,\n","   'average_q_func1_loss': 5.379106283187866,\n","   'average_q_func2_loss': 4.8123835587501524,\n","   'n_updates': 50001,\n","   'average_entropy': -3.0176296,\n","   'temperature': 0.04643449932336807,\n","   'eval_score': 512.8987683880524},\n","  {'average_q1': 119.502525,\n","   'average_q2': 119.465065,\n","   'average_q_func1_loss': 4.4169233751296995,\n","   'average_q_func2_loss': 4.352455830574035,\n","   'n_updates': 55001,\n","   'average_entropy': -2.9171479,\n","   'temperature': 0.04866786673665047,\n","   'eval_score': 215.85136465760283},\n","  {'average_q1': 116.67842,\n","   'average_q2': 116.91643,\n","   'average_q_func1_loss': 4.441220309734344,\n","   'average_q_func2_loss': 4.158282560110092,\n","   'n_updates': 60001,\n","   'average_entropy': -3.0262678,\n","   'temperature': 0.05000748857855797,\n","   'eval_score': 223.03675470425458},\n","  {'average_q1': 122.39538,\n","   'average_q2': 122.47491,\n","   'average_q_func1_loss': 4.35991058588028,\n","   'average_q_func2_loss': 3.882915802001953,\n","   'n_updates': 65001,\n","   'average_entropy': -2.9533632,\n","   'temperature': 0.05427759140729904,\n","   'eval_score': 231.5413111138859},\n","  {'average_q1': 122.94754,\n","   'average_q2': 122.809074,\n","   'average_q_func1_loss': 4.442569383382797,\n","   'average_q_func2_loss': 4.510919210910797,\n","   'n_updates': 70001,\n","   'average_entropy': -3.160675,\n","   'temperature': 0.045098207890987396,\n","   'eval_score': 360.9696424445139},\n","  {'average_q1': 123.32404,\n","   'average_q2': 123.31908,\n","   'average_q_func1_loss': 4.104547635316849,\n","   'average_q_func2_loss': 3.9126254105567932,\n","   'n_updates': 75001,\n","   'average_entropy': -3.017258,\n","   'temperature': 0.04823146015405655,\n","   'eval_score': 275.8404208039011},\n","  {'average_q1': 127.88836,\n","   'average_q2': 128.04475,\n","   'average_q_func1_loss': 5.560436770915985,\n","   'average_q_func2_loss': 5.887580766677856,\n","   'n_updates': 80001,\n","   'average_entropy': -2.9009032,\n","   'temperature': 0.053148359060287476,\n","   'eval_score': 223.27793323270765},\n","  {'average_q1': 127.10737,\n","   'average_q2': 127.09536,\n","   'average_q_func1_loss': 4.354608625173569,\n","   'average_q_func2_loss': 4.029893399477005,\n","   'n_updates': 85001,\n","   'average_entropy': -3.029633,\n","   'temperature': 0.04834223538637161,\n","   'eval_score': 283.8388443458571},\n","  {'average_q1': 128.28236,\n","   'average_q2': 128.06677,\n","   'average_q_func1_loss': 4.699758810997009,\n","   'average_q_func2_loss': 4.2572235274314885,\n","   'n_updates': 90001,\n","   'average_entropy': -3.0387995,\n","   'temperature': 0.051050469279289246,\n","   'eval_score': 344.92731252353576},\n","  {'average_q1': 129.19989,\n","   'average_q2': 129.12291,\n","   'average_q_func1_loss': 4.828684659004211,\n","   'average_q_func2_loss': 4.585013166666031,\n","   'n_updates': 95001,\n","   'average_entropy': -2.9429712,\n","   'temperature': 0.04994731396436691,\n","   'eval_score': 374.99670049351704},\n","  {'average_q1': 131.66762,\n","   'average_q2': 131.536,\n","   'average_q_func1_loss': 5.033063015937805,\n","   'average_q_func2_loss': 4.799991016387939,\n","   'n_updates': 100001,\n","   'average_entropy': -2.9276426,\n","   'temperature': 0.04730815440416336,\n","   'eval_score': 250.55490730356843},\n","  {'average_q1': 128.47206,\n","   'average_q2': 128.34908,\n","   'average_q_func1_loss': 4.416478953361511,\n","   'average_q_func2_loss': 4.174887182712555,\n","   'n_updates': 105001,\n","   'average_entropy': -3.0018268,\n","   'temperature': 0.04796016588807106,\n","   'eval_score': 384.9246721582251},\n","  {'average_q1': 130.9246,\n","   'average_q2': 130.70097,\n","   'average_q_func1_loss': 6.6011483144760135,\n","   'average_q_func2_loss': 5.463444979190826,\n","   'n_updates': 110001,\n","   'average_entropy': -2.9103076,\n","   'temperature': 0.04682384431362152,\n","   'eval_score': 426.39841433493757},\n","  {'average_q1': 126.8165,\n","   'average_q2': 127.10684,\n","   'average_q_func1_loss': 4.055371179580688,\n","   'average_q_func2_loss': 3.8542316591739656,\n","   'n_updates': 115001,\n","   'average_entropy': -3.2291622,\n","   'temperature': 0.047062356024980545,\n","   'eval_score': 337.900578746616},\n","  {'average_q1': 125.50167,\n","   'average_q2': 125.563934,\n","   'average_q_func1_loss': 4.049701068401337,\n","   'average_q_func2_loss': 3.9696738469600676,\n","   'n_updates': 120001,\n","   'average_entropy': -2.887026,\n","   'temperature': 0.04442356154322624,\n","   'eval_score': 318.77377929449784},\n","  {'average_q1': 125.4065,\n","   'average_q2': 125.19229,\n","   'average_q_func1_loss': 3.897102597951889,\n","   'average_q_func2_loss': 3.6162486517429353,\n","   'n_updates': 125001,\n","   'average_entropy': -2.946626,\n","   'temperature': 0.0415128730237484,\n","   'eval_score': 414.7192835084784},\n","  {'average_q1': 127.432014,\n","   'average_q2': 127.47394,\n","   'average_q_func1_loss': 3.7104461181163786,\n","   'average_q_func2_loss': 3.581926542520523,\n","   'n_updates': 130001,\n","   'average_entropy': -3.0084643,\n","   'temperature': 0.04044833779335022,\n","   'eval_score': 659.1579717482189},\n","  {'average_q1': 125.983345,\n","   'average_q2': 125.926926,\n","   'average_q_func1_loss': 3.9969363713264467,\n","   'average_q_func2_loss': 3.696513079404831,\n","   'n_updates': 135001,\n","   'average_entropy': -3.0229716,\n","   'temperature': 0.03966042026877403,\n","   'eval_score': 674.0131310938178},\n","  {'average_q1': 130.72598,\n","   'average_q2': 130.98972,\n","   'average_q_func1_loss': 4.1540412366390225,\n","   'average_q_func2_loss': 3.938396692276001,\n","   'n_updates': 140001,\n","   'average_entropy': -2.8327227,\n","   'temperature': 0.039840035140514374,\n","   'eval_score': 791.5669901889969},\n","  {'average_q1': 130.02484,\n","   'average_q2': 129.76285,\n","   'average_q_func1_loss': 5.149871472120285,\n","   'average_q_func2_loss': 4.27690709233284,\n","   'n_updates': 145001,\n","   'average_entropy': -3.0744874,\n","   'temperature': 0.039848897606134415,\n","   'eval_score': 551.8950781382548},\n","  {'average_q1': 133.5158,\n","   'average_q2': 133.39319,\n","   'average_q_func1_loss': 4.886731301546097,\n","   'average_q_func2_loss': 3.875376740694046,\n","   'n_updates': 150001,\n","   'average_entropy': -2.994773,\n","   'temperature': 0.03907419368624687,\n","   'eval_score': 540.2452754415114},\n","  {'average_q1': 135.45981,\n","   'average_q2': 135.80037,\n","   'average_q_func1_loss': 4.728984376192093,\n","   'average_q_func2_loss': 4.789812616109848,\n","   'n_updates': 155001,\n","   'average_entropy': -2.7925327,\n","   'temperature': 0.04145568236708641,\n","   'eval_score': 699.4001094463225},\n","  {'average_q1': 132.29466,\n","   'average_q2': 132.12305,\n","   'average_q_func1_loss': 4.1436090397834775,\n","   'average_q_func2_loss': 4.432747761011123,\n","   'n_updates': 160001,\n","   'average_entropy': -3.0699532,\n","   'temperature': 0.041109848767519,\n","   'eval_score': 770.6432709246769},\n","  {'average_q1': 137.3425,\n","   'average_q2': 137.45187,\n","   'average_q_func1_loss': 4.008537385463715,\n","   'average_q_func2_loss': 3.8664562237262725,\n","   'n_updates': 165001,\n","   'average_entropy': -2.8776553,\n","   'temperature': 0.04122294485569,\n","   'eval_score': 527.8774547594676},\n","  {'average_q1': 137.11264,\n","   'average_q2': 137.29948,\n","   'average_q_func1_loss': 4.449502197504043,\n","   'average_q_func2_loss': 4.3883873963356015,\n","   'n_updates': 170001,\n","   'average_entropy': -2.98425,\n","   'temperature': 0.03798291087150574,\n","   'eval_score': 686.2792404925742},\n","  {'average_q1': 139.13066,\n","   'average_q2': 139.0871,\n","   'average_q_func1_loss': 3.9007810282707216,\n","   'average_q_func2_loss': 3.7256868255138396,\n","   'n_updates': 175001,\n","   'average_entropy': -2.9909914,\n","   'temperature': 0.03913675248622894,\n","   'eval_score': 391.2962777598865},\n","  {'average_q1': 138.68578,\n","   'average_q2': 138.89192,\n","   'average_q_func1_loss': 4.1713090264797215,\n","   'average_q_func2_loss': 3.8357397544384004,\n","   'n_updates': 180001,\n","   'average_entropy': -2.9474394,\n","   'temperature': 0.041888974606990814,\n","   'eval_score': 621.2762707214691},\n","  {'average_q1': 140.23163,\n","   'average_q2': 140.26744,\n","   'average_q_func1_loss': 5.126499309539795,\n","   'average_q_func2_loss': 4.717438836097717,\n","   'n_updates': 185001,\n","   'average_entropy': -3.0411444,\n","   'temperature': 0.04212697595357895,\n","   'eval_score': 798.8360143923522},\n","  {'average_q1': 144.018,\n","   'average_q2': 144.2203,\n","   'average_q_func1_loss': 4.732076562643051,\n","   'average_q_func2_loss': 3.997827670574188,\n","   'n_updates': 190001,\n","   'average_entropy': -2.877335,\n","   'temperature': 0.03895178437232971,\n","   'eval_score': 734.1166011724237},\n","  {'average_q1': 140.18181,\n","   'average_q2': 140.26672,\n","   'average_q_func1_loss': 3.531741646528244,\n","   'average_q_func2_loss': 3.899861401319504,\n","   'n_updates': 195001,\n","   'average_entropy': -2.9721007,\n","   'temperature': 0.039328522980213165,\n","   'eval_score': 549.2736603575312},\n","  {'average_q1': 144.53523,\n","   'average_q2': 144.85953,\n","   'average_q_func1_loss': 3.756942036151886,\n","   'average_q_func2_loss': 3.769896490573883,\n","   'n_updates': 200001,\n","   'average_entropy': -3.1269288,\n","   'temperature': 0.03777989745140076,\n","   'eval_score': 694.91107462295},\n","  {'average_q1': 144.31372,\n","   'average_q2': 144.28423,\n","   'average_q_func1_loss': 4.78781834602356,\n","   'average_q_func2_loss': 4.552818199396134,\n","   'n_updates': 205001,\n","   'average_entropy': -3.1770918,\n","   'temperature': 0.03781251236796379,\n","   'eval_score': 615.6718280783948},\n","  {'average_q1': 144.93517,\n","   'average_q2': 145.14822,\n","   'average_q_func1_loss': 3.9286119747161865,\n","   'average_q_func2_loss': 3.970077773332596,\n","   'n_updates': 210001,\n","   'average_entropy': -3.2669291,\n","   'temperature': 0.03844602778553963,\n","   'eval_score': 610.1164175233137},\n","  {'average_q1': 145.06544,\n","   'average_q2': 144.9078,\n","   'average_q_func1_loss': 4.900830870866775,\n","   'average_q_func2_loss': 4.293763991594314,\n","   'n_updates': 215001,\n","   'average_entropy': -3.1093893,\n","   'temperature': 0.03937806189060211,\n","   'eval_score': 691.9363757584157},\n","  {'average_q1': 147.40225,\n","   'average_q2': 147.41922,\n","   'average_q_func1_loss': 5.011112734079361,\n","   'average_q_func2_loss': 4.024296263456344,\n","   'n_updates': 220001,\n","   'average_entropy': -3.1855047,\n","   'temperature': 0.03942256048321724,\n","   'eval_score': 576.6121343788726},\n","  {'average_q1': 147.35207,\n","   'average_q2': 147.54369,\n","   'average_q_func1_loss': 4.4558722448349,\n","   'average_q_func2_loss': 3.856012661457062,\n","   'n_updates': 225001,\n","   'average_entropy': -2.922999,\n","   'temperature': 0.04051511362195015,\n","   'eval_score': 692.2304070214788},\n","  {'average_q1': 150.36266,\n","   'average_q2': 150.34969,\n","   'average_q_func1_loss': 5.206658687591553,\n","   'average_q_func2_loss': 4.452956287860871,\n","   'n_updates': 230001,\n","   'average_entropy': -3.1827295,\n","   'temperature': 0.03979441151022911,\n","   'eval_score': 614.7089314172451},\n","  {'average_q1': 148.82103,\n","   'average_q2': 148.57439,\n","   'average_q_func1_loss': 4.855163756608963,\n","   'average_q_func2_loss': 3.941081794500351,\n","   'n_updates': 235001,\n","   'average_entropy': -3.1037686,\n","   'temperature': 0.03901536762714386,\n","   'eval_score': 708.3591101145688},\n","  {'average_q1': 150.21826,\n","   'average_q2': 150.25412,\n","   'average_q_func1_loss': 3.9506277203559876,\n","   'average_q_func2_loss': 3.5121162021160126,\n","   'n_updates': 240001,\n","   'average_entropy': -3.0948892,\n","   'temperature': 0.039087433367967606,\n","   'eval_score': 785.2689326537392},\n","  {'average_q1': 151.73799,\n","   'average_q2': 151.98116,\n","   'average_q_func1_loss': 4.5462106621265415,\n","   'average_q_func2_loss': 4.2144965088367465,\n","   'n_updates': 245001,\n","   'average_entropy': -3.1372075,\n","   'temperature': 0.03936179354786873,\n","   'eval_score': 655.0673820605095},\n","  {'average_q1': 151.62494,\n","   'average_q2': 151.46378,\n","   'average_q_func1_loss': 4.7090696108341215,\n","   'average_q_func2_loss': 3.950421402454376,\n","   'n_updates': 250001,\n","   'average_entropy': -2.922812,\n","   'temperature': 0.03889891132712364,\n","   'eval_score': 813.1843901574495},\n","  {'average_q1': 148.66054,\n","   'average_q2': 148.61845,\n","   'average_q_func1_loss': 4.365562182664871,\n","   'average_q_func2_loss': 3.8422410917282104,\n","   'n_updates': 255001,\n","   'average_entropy': -3.11442,\n","   'temperature': 0.038173332810401917,\n","   'eval_score': 559.0384877237752},\n","  {'average_q1': 153.36697,\n","   'average_q2': 153.09337,\n","   'average_q_func1_loss': 3.4001005136966707,\n","   'average_q_func2_loss': 3.402152281999588,\n","   'n_updates': 260001,\n","   'average_entropy': -3.2319922,\n","   'temperature': 0.037141308188438416,\n","   'eval_score': 683.9596008508898},\n","  {'average_q1': 152.93976,\n","   'average_q2': 152.76361,\n","   'average_q_func1_loss': 4.136323540210724,\n","   'average_q_func2_loss': 3.853338816165924,\n","   'n_updates': 265001,\n","   'average_entropy': -3.0087612,\n","   'temperature': 0.03694150969386101,\n","   'eval_score': 541.7734791555116},\n","  {'average_q1': 151.48386,\n","   'average_q2': 151.50305,\n","   'average_q_func1_loss': 3.572449930906296,\n","   'average_q_func2_loss': 3.886289020776749,\n","   'n_updates': 270001,\n","   'average_entropy': -2.9813538,\n","   'temperature': 0.038222458213567734,\n","   'eval_score': 491.1479784645924},\n","  {'average_q1': 154.24649,\n","   'average_q2': 153.98596,\n","   'average_q_func1_loss': 4.112218142747879,\n","   'average_q_func2_loss': 3.762379466295242,\n","   'n_updates': 275001,\n","   'average_entropy': -2.8848565,\n","   'temperature': 0.037600770592689514,\n","   'eval_score': 1009.8843965592733},\n","  {'average_q1': 154.63634,\n","   'average_q2': 154.55296,\n","   'average_q_func1_loss': 4.386210513114929,\n","   'average_q_func2_loss': 3.7029127931594847,\n","   'n_updates': 280001,\n","   'average_entropy': -3.0505977,\n","   'temperature': 0.03893293812870979,\n","   'eval_score': 829.2532624918294},\n","  {'average_q1': 153.73582,\n","   'average_q2': 153.92953,\n","   'average_q_func1_loss': 4.6467323648929595,\n","   'average_q_func2_loss': 3.921242171525955,\n","   'n_updates': 285001,\n","   'average_entropy': -2.737125,\n","   'temperature': 0.038081567734479904,\n","   'eval_score': 814.6784353950934},\n","  {'average_q1': 158.5038,\n","   'average_q2': 158.36818,\n","   'average_q_func1_loss': 3.6096847915649413,\n","   'average_q_func2_loss': 3.5555363583564756,\n","   'n_updates': 290001,\n","   'average_entropy': -3.0984411,\n","   'temperature': 0.03814211115241051,\n","   'eval_score': 705.4692543733992},\n","  {'average_q1': 153.80588,\n","   'average_q2': 153.82321,\n","   'average_q_func1_loss': 3.9379362738132477,\n","   'average_q_func2_loss': 3.9387354862689974,\n","   'n_updates': 295001,\n","   'average_entropy': -3.0613465,\n","   'temperature': 0.0386970192193985,\n","   'eval_score': 904.8016478836564},\n","  {'average_q1': 154.81319,\n","   'average_q2': 154.89613,\n","   'average_q_func1_loss': 4.775585556030274,\n","   'average_q_func2_loss': 5.086853977441788,\n","   'n_updates': 300001,\n","   'average_entropy': -2.7562635,\n","   'temperature': 0.037754714488983154,\n","   'eval_score': 794.5118980260465},\n","  {'average_q1': 156.31856,\n","   'average_q2': 156.11789,\n","   'average_q_func1_loss': 3.7916873717308044,\n","   'average_q_func2_loss': 3.835451513528824,\n","   'n_updates': 305001,\n","   'average_entropy': -3.1892755,\n","   'temperature': 0.03801558166742325,\n","   'eval_score': 765.1346450366274},\n","  {'average_q1': 157.43138,\n","   'average_q2': 157.5204,\n","   'average_q_func1_loss': 3.9069869756698608,\n","   'average_q_func2_loss': 3.6000895369052888,\n","   'n_updates': 310001,\n","   'average_entropy': -2.9439383,\n","   'temperature': 0.0374477319419384,\n","   'eval_score': 901.0990916888384},\n","  {'average_q1': 156.61292,\n","   'average_q2': 156.74347,\n","   'average_q_func1_loss': 4.009208689928055,\n","   'average_q_func2_loss': 3.85881986618042,\n","   'n_updates': 315001,\n","   'average_entropy': -2.90819,\n","   'temperature': 0.03658072650432587,\n","   'eval_score': 860.4281009178974},\n","  {'average_q1': 157.46085,\n","   'average_q2': 157.38867,\n","   'average_q_func1_loss': 3.6604754757881164,\n","   'average_q_func2_loss': 3.6124892354011537,\n","   'n_updates': 320001,\n","   'average_entropy': -2.8646915,\n","   'temperature': 0.03751214221119881,\n","   'eval_score': 748.1600913170761},\n","  {'average_q1': 157.53636,\n","   'average_q2': 157.64198,\n","   'average_q_func1_loss': 4.187159974575042,\n","   'average_q_func2_loss': 4.1650746166706085,\n","   'n_updates': 325001,\n","   'average_entropy': -2.902618,\n","   'temperature': 0.036317937076091766,\n","   'eval_score': 316.0802637095997},\n","  {'average_q1': 155.96112,\n","   'average_q2': 155.97984,\n","   'average_q_func1_loss': 3.891862984895706,\n","   'average_q_func2_loss': 3.666619658470154,\n","   'n_updates': 330001,\n","   'average_entropy': -3.0704327,\n","   'temperature': 0.037407949566841125,\n","   'eval_score': 652.328151305863},\n","  {'average_q1': 158.20099,\n","   'average_q2': 158.22534,\n","   'average_q_func1_loss': 3.8373230481147766,\n","   'average_q_func2_loss': 3.7035799372196196,\n","   'n_updates': 335001,\n","   'average_entropy': -2.8410044,\n","   'temperature': 0.03604591265320778,\n","   'eval_score': 832.3385851954337},\n","  {'average_q1': 158.19185,\n","   'average_q2': 158.46341,\n","   'average_q_func1_loss': 4.175133601427079,\n","   'average_q_func2_loss': 4.163406791687012,\n","   'n_updates': 340001,\n","   'average_entropy': -2.9058945,\n","   'temperature': 0.03748727962374687,\n","   'eval_score': 946.9475827288101},\n","  {'average_q1': 160.99815,\n","   'average_q2': 161.04613,\n","   'average_q_func1_loss': 4.060009917020798,\n","   'average_q_func2_loss': 3.998256183862686,\n","   'n_updates': 345001,\n","   'average_entropy': -3.002413,\n","   'temperature': 0.03733591362833977,\n","   'eval_score': 452.4806496907512},\n","  {'average_q1': 160.70277,\n","   'average_q2': 160.47865,\n","   'average_q_func1_loss': 4.404013803005219,\n","   'average_q_func2_loss': 4.308069581985474,\n","   'n_updates': 350001,\n","   'average_entropy': -2.9944723,\n","   'temperature': 0.03845857083797455,\n","   'eval_score': 992.6251522105686},\n","  {'average_q1': 161.93881,\n","   'average_q2': 162.08423,\n","   'average_q_func1_loss': 4.224367300271988,\n","   'average_q_func2_loss': 4.645860319137573,\n","   'n_updates': 355001,\n","   'average_entropy': -3.0195608,\n","   'temperature': 0.03822493925690651,\n","   'eval_score': 745.6190866629229},\n","  {'average_q1': 163.04222,\n","   'average_q2': 163.12558,\n","   'average_q_func1_loss': 4.397421581745148,\n","   'average_q_func2_loss': 4.132884224653244,\n","   'n_updates': 360001,\n","   'average_entropy': -2.7615006,\n","   'temperature': 0.03920146822929382,\n","   'eval_score': 876.3337641567956},\n","  {'average_q1': 158.99178,\n","   'average_q2': 159.10112,\n","   'average_q_func1_loss': 4.061415746212005,\n","   'average_q_func2_loss': 3.8112755000591276,\n","   'n_updates': 365001,\n","   'average_entropy': -3.0501904,\n","   'temperature': 0.03712614253163338,\n","   'eval_score': 844.8843004358041},\n","  {'average_q1': 163.14265,\n","   'average_q2': 163.00586,\n","   'average_q_func1_loss': 4.841761975288391,\n","   'average_q_func2_loss': 4.533958175182343,\n","   'n_updates': 370001,\n","   'average_entropy': -3.035016,\n","   'temperature': 0.037570610642433167,\n","   'eval_score': 914.4925706469136},\n","  {'average_q1': 162.89107,\n","   'average_q2': 162.98685,\n","   'average_q_func1_loss': 4.071738924980163,\n","   'average_q_func2_loss': 4.3108129620552065,\n","   'n_updates': 375001,\n","   'average_entropy': -3.1963663,\n","   'temperature': 0.036472443491220474,\n","   'eval_score': 942.7133911879769},\n","  {'average_q1': 160.80464,\n","   'average_q2': 160.97025,\n","   'average_q_func1_loss': 3.7282746195793153,\n","   'average_q_func2_loss': 3.6504027020931242,\n","   'n_updates': 380001,\n","   'average_entropy': -2.9630518,\n","   'temperature': 0.0388619638979435,\n","   'eval_score': 1031.242353787017},\n","  {'average_q1': 162.4583,\n","   'average_q2': 162.54112,\n","   'average_q_func1_loss': 4.31943008184433,\n","   'average_q_func2_loss': 4.06572439789772,\n","   'n_updates': 385001,\n","   'average_entropy': -2.783858,\n","   'temperature': 0.03717753291130066,\n","   'eval_score': 981.039597500971},\n","  {'average_q1': 164.2643,\n","   'average_q2': 164.28143,\n","   'average_q_func1_loss': 4.473983279466629,\n","   'average_q_func2_loss': 4.297501870393753,\n","   'n_updates': 390001,\n","   'average_entropy': -3.1227646,\n","   'temperature': 0.038709014654159546,\n","   'eval_score': 967.2523874751435},\n","  {'average_q1': 165.16005,\n","   'average_q2': 164.96046,\n","   'average_q_func1_loss': 4.224721148014068,\n","   'average_q_func2_loss': 4.054326446056366,\n","   'n_updates': 395001,\n","   'average_entropy': -2.770395,\n","   'temperature': 0.03765374422073364,\n","   'eval_score': 885.3691297885227},\n","  {'average_q1': 164.7342,\n","   'average_q2': 164.78194,\n","   'average_q_func1_loss': 4.402423502206802,\n","   'average_q_func2_loss': 4.443617243766784,\n","   'n_updates': 400001,\n","   'average_entropy': -3.0756736,\n","   'temperature': 0.03695133700966835,\n","   'eval_score': 958.9292255322134},\n","  {'average_q1': 164.49188,\n","   'average_q2': 164.43791,\n","   'average_q_func1_loss': 4.750332673788071,\n","   'average_q_func2_loss': 4.4599554002285,\n","   'n_updates': 405001,\n","   'average_entropy': -3.0424497,\n","   'temperature': 0.03679782152175903,\n","   'eval_score': 602.8668735177122},\n","  {'average_q1': 167.70343,\n","   'average_q2': 167.63617,\n","   'average_q_func1_loss': 4.5184821760654446,\n","   'average_q_func2_loss': 4.751426006555557,\n","   'n_updates': 410001,\n","   'average_entropy': -3.010116,\n","   'temperature': 0.03678768873214722,\n","   'eval_score': 1082.8746342753948},\n","  {'average_q1': 167.06876,\n","   'average_q2': 166.96214,\n","   'average_q_func1_loss': 5.452077703475952,\n","   'average_q_func2_loss': 4.951825790405273,\n","   'n_updates': 415001,\n","   'average_entropy': -3.1385264,\n","   'temperature': 0.03665223345160484,\n","   'eval_score': 1009.332461273455},\n","  {'average_q1': 168.00922,\n","   'average_q2': 168.04735,\n","   'average_q_func1_loss': 4.618220213651657,\n","   'average_q_func2_loss': 4.569802023172379,\n","   'n_updates': 420001,\n","   'average_entropy': -3.0605173,\n","   'temperature': 0.037104833871126175,\n","   'eval_score': 1170.8827000908725},\n","  {'average_q1': 166.49684,\n","   'average_q2': 166.78818,\n","   'average_q_func1_loss': 3.770733572244644,\n","   'average_q_func2_loss': 3.9389604461193084,\n","   'n_updates': 425001,\n","   'average_entropy': -3.0607216,\n","   'temperature': 0.035925112664699554,\n","   'eval_score': 1444.779423385759},\n","  {'average_q1': 165.40565,\n","   'average_q2': 165.68184,\n","   'average_q_func1_loss': 5.102159063816071,\n","   'average_q_func2_loss': 4.940635154247284,\n","   'n_updates': 430001,\n","   'average_entropy': -3.0290225,\n","   'temperature': 0.037611305713653564,\n","   'eval_score': 966.6038860566946},\n","  {'average_q1': 166.47375,\n","   'average_q2': 166.30028,\n","   'average_q_func1_loss': 4.26531834602356,\n","   'average_q_func2_loss': 4.977536559104919,\n","   'n_updates': 435001,\n","   'average_entropy': -3.0588202,\n","   'temperature': 0.03716740384697914,\n","   'eval_score': 1457.7696270509791},\n","  {'average_q1': 168.6969,\n","   'average_q2': 168.52904,\n","   'average_q_func1_loss': 4.552134261131287,\n","   'average_q_func2_loss': 4.3422543084621426,\n","   'n_updates': 440001,\n","   'average_entropy': -3.124652,\n","   'temperature': 0.038129545748233795,\n","   'eval_score': 1095.115251461992},\n","  {'average_q1': 167.90193,\n","   'average_q2': 167.93185,\n","   'average_q_func1_loss': 4.994710694551468,\n","   'average_q_func2_loss': 4.678227097988128,\n","   'n_updates': 445001,\n","   'average_entropy': -2.9896953,\n","   'temperature': 0.035861700773239136,\n","   'eval_score': 1218.8797947044363},\n","  {'average_q1': 170.14928,\n","   'average_q2': 170.3355,\n","   'average_q_func1_loss': 4.1599489998817445,\n","   'average_q_func2_loss': 4.375532950162888,\n","   'n_updates': 450001,\n","   'average_entropy': -2.7911003,\n","   'temperature': 0.03755873441696167,\n","   'eval_score': 1679.235381656405},\n","  {'average_q1': 171.9813,\n","   'average_q2': 171.86812,\n","   'average_q_func1_loss': 4.276535476446152,\n","   'average_q_func2_loss': 4.454442048072815,\n","   'n_updates': 455001,\n","   'average_entropy': -2.941806,\n","   'temperature': 0.037362225353717804,\n","   'eval_score': 1332.652140949089},\n","  {'average_q1': 172.70502,\n","   'average_q2': 172.22838,\n","   'average_q_func1_loss': 4.247263082265854,\n","   'average_q_func2_loss': 4.247237168550491,\n","   'n_updates': 460001,\n","   'average_entropy': -2.9068441,\n","   'temperature': 0.03804944083094597,\n","   'eval_score': 1394.847576994972},\n","  {'average_q1': 172.26778,\n","   'average_q2': 172.3175,\n","   'average_q_func1_loss': 4.138941968679428,\n","   'average_q_func2_loss': 4.371668195724487,\n","   'n_updates': 465001,\n","   'average_entropy': -2.9065762,\n","   'temperature': 0.038266588002443314,\n","   'eval_score': 1269.7604224967197},\n","  {'average_q1': 174.42575,\n","   'average_q2': 174.16771,\n","   'average_q_func1_loss': 4.087285597324371,\n","   'average_q_func2_loss': 4.14377351641655,\n","   'n_updates': 470001,\n","   'average_entropy': -3.1324873,\n","   'temperature': 0.0365416519343853,\n","   'eval_score': 1310.6193145012844},\n","  {'average_q1': 174.83571,\n","   'average_q2': 174.83015,\n","   'average_q_func1_loss': 4.049458334445953,\n","   'average_q_func2_loss': 4.085344709157944,\n","   'n_updates': 475001,\n","   'average_entropy': -2.8364966,\n","   'temperature': 0.03766327351331711,\n","   'eval_score': 1865.6792481349887},\n","  {'average_q1': 172.8455,\n","   'average_q2': 172.57271,\n","   'average_q_func1_loss': 4.359528537988663,\n","   'average_q_func2_loss': 4.19766756772995,\n","   'n_updates': 480001,\n","   'average_entropy': -2.9500833,\n","   'temperature': 0.0390559621155262,\n","   'eval_score': 1288.018160965887},\n","  {'average_q1': 175.6832,\n","   'average_q2': 175.56743,\n","   'average_q_func1_loss': 4.843263635635376,\n","   'average_q_func2_loss': 4.873994265794754,\n","   'n_updates': 485001,\n","   'average_entropy': -3.1437972,\n","   'temperature': 0.03681906685233116,\n","   'eval_score': 1936.559043707724},\n","  {'average_q1': 178.75438,\n","   'average_q2': 178.63913,\n","   'average_q_func1_loss': 5.323259563446045,\n","   'average_q_func2_loss': 5.422560970783234,\n","   'n_updates': 490001,\n","   'average_entropy': -2.9680994,\n","   'temperature': 0.03848165646195412,\n","   'eval_score': 1569.559134549845},\n","  {'average_q1': 177.48814,\n","   'average_q2': 177.47678,\n","   'average_q_func1_loss': 5.265140950679779,\n","   'average_q_func2_loss': 5.427314238548279,\n","   'n_updates': 495001,\n","   'average_entropy': -3.429889,\n","   'temperature': 0.03803326562047005,\n","   'eval_score': 1717.0721133207355},\n","  {'average_q1': 179.21211,\n","   'average_q2': 179.32439,\n","   'average_q_func1_loss': 5.0329539477825165,\n","   'average_q_func2_loss': 4.713150211572647,\n","   'n_updates': 500001,\n","   'average_entropy': -3.020725,\n","   'temperature': 0.03806035593152046,\n","   'eval_score': 2014.9246520174465},\n","  {'average_q1': 180.50717,\n","   'average_q2': 180.29774,\n","   'average_q_func1_loss': 6.241357315778732,\n","   'average_q_func2_loss': 6.315738803148269,\n","   'n_updates': 505001,\n","   'average_entropy': -3.0429819,\n","   'temperature': 0.03725850582122803,\n","   'eval_score': 945.098674312447},\n","  {'average_q1': 179.59824,\n","   'average_q2': 179.74815,\n","   'average_q_func1_loss': 4.427362816333771,\n","   'average_q_func2_loss': 4.388644064664841,\n","   'n_updates': 510001,\n","   'average_entropy': -3.0813627,\n","   'temperature': 0.03837200999259949,\n","   'eval_score': 1063.5746281758284},\n","  {'average_q1': 178.70088,\n","   'average_q2': 178.79874,\n","   'average_q_func1_loss': 4.826335614919662,\n","   'average_q_func2_loss': 5.116752345561981,\n","   'n_updates': 515001,\n","   'average_entropy': -3.058523,\n","   'temperature': 0.037817902863025665,\n","   'eval_score': 1792.2727526402516},\n","  {'average_q1': 178.4997,\n","   'average_q2': 178.65639,\n","   'average_q_func1_loss': 4.615832898616791,\n","   'average_q_func2_loss': 4.585719327926636,\n","   'n_updates': 520001,\n","   'average_entropy': -2.8329422,\n","   'temperature': 0.039040833711624146,\n","   'eval_score': 1084.2695126881683},\n","  {'average_q1': 181.8094,\n","   'average_q2': 182.05203,\n","   'average_q_func1_loss': 4.506993186473847,\n","   'average_q_func2_loss': 4.571029976606369,\n","   'n_updates': 525001,\n","   'average_entropy': -2.8473306,\n","   'temperature': 0.03777286410331726,\n","   'eval_score': 1265.086435580854},\n","  {'average_q1': 183.19046,\n","   'average_q2': 183.23366,\n","   'average_q_func1_loss': 5.343413231372833,\n","   'average_q_func2_loss': 5.23345207452774,\n","   'n_updates': 530001,\n","   'average_entropy': -3.031562,\n","   'temperature': 0.04003395512700081,\n","   'eval_score': 402.54876115468437},\n","  {'average_q1': 183.74037,\n","   'average_q2': 183.8404,\n","   'average_q_func1_loss': 5.069740355014801,\n","   'average_q_func2_loss': 4.97076899766922,\n","   'n_updates': 535001,\n","   'average_entropy': -2.987553,\n","   'temperature': 0.0394042432308197,\n","   'eval_score': 1518.6596051932154},\n","  {'average_q1': 182.46553,\n","   'average_q2': 182.23473,\n","   'average_q_func1_loss': 5.214934012889862,\n","   'average_q_func2_loss': 5.127829309701919,\n","   'n_updates': 540001,\n","   'average_entropy': -3.0565665,\n","   'temperature': 0.03926805406808853,\n","   'eval_score': 1609.7012312985707},\n","  {'average_q1': 181.29756,\n","   'average_q2': 181.35466,\n","   'average_q_func1_loss': 5.3417480230331424,\n","   'average_q_func2_loss': 5.223252055644989,\n","   'n_updates': 545001,\n","   'average_entropy': -2.984438,\n","   'temperature': 0.0384339764714241,\n","   'eval_score': 1557.7294905476485},\n","  {'average_q1': 182.60669,\n","   'average_q2': 182.77,\n","   'average_q_func1_loss': 7.662314002513885,\n","   'average_q_func2_loss': 7.150444433689118,\n","   'n_updates': 550001,\n","   'average_entropy': -3.0193028,\n","   'temperature': 0.04047190397977829,\n","   'eval_score': 1765.3296858776196},\n","  {'average_q1': 182.3526,\n","   'average_q2': 182.77654,\n","   'average_q_func1_loss': 5.676199426651001,\n","   'average_q_func2_loss': 5.46870339512825,\n","   'n_updates': 555001,\n","   'average_entropy': -2.9454086,\n","   'temperature': 0.03960204869508743,\n","   'eval_score': 1148.923214869064},\n","  {'average_q1': 183.47842,\n","   'average_q2': 183.61429,\n","   'average_q_func1_loss': 5.903931429386139,\n","   'average_q_func2_loss': 5.833546633720398,\n","   'n_updates': 560001,\n","   'average_entropy': -3.2357872,\n","   'temperature': 0.038814444094896317,\n","   'eval_score': 1170.1140146259027},\n","  {'average_q1': 187.09213,\n","   'average_q2': 187.01103,\n","   'average_q_func1_loss': 5.712111032009124,\n","   'average_q_func2_loss': 5.913038990497589,\n","   'n_updates': 565001,\n","   'average_entropy': -3.2399614,\n","   'temperature': 0.039589133113622665,\n","   'eval_score': 1526.8438876804148},\n","  {'average_q1': 186.56567,\n","   'average_q2': 186.95647,\n","   'average_q_func1_loss': 5.861782101392746,\n","   'average_q_func2_loss': 5.696128934621811,\n","   'n_updates': 570001,\n","   'average_entropy': -3.14598,\n","   'temperature': 0.03935600444674492,\n","   'eval_score': 1241.38681012807},\n","  {'average_q1': 186.35747,\n","   'average_q2': 186.27654,\n","   'average_q_func1_loss': 6.326507778167724,\n","   'average_q_func2_loss': 6.597172756195068,\n","   'n_updates': 575001,\n","   'average_entropy': -2.8435273,\n","   'temperature': 0.038458265364170074,\n","   'eval_score': 1889.0967204906106},\n","  {'average_q1': 182.97704,\n","   'average_q2': 183.0565,\n","   'average_q_func1_loss': 6.0223516178131105,\n","   'average_q_func2_loss': 5.7184110248088835,\n","   'n_updates': 580001,\n","   'average_entropy': -2.9123483,\n","   'temperature': 0.038696520030498505,\n","   'eval_score': 1080.4088040190375},\n","  {'average_q1': 180.65715,\n","   'average_q2': 180.44127,\n","   'average_q_func1_loss': 5.308326325416565,\n","   'average_q_func2_loss': 5.20070995092392,\n","   'n_updates': 585001,\n","   'average_entropy': -3.1610215,\n","   'temperature': 0.040448613464832306,\n","   'eval_score': 1719.471632607046},\n","  {'average_q1': 185.23744,\n","   'average_q2': 185.03651,\n","   'average_q_func1_loss': 5.884809596538544,\n","   'average_q_func2_loss': 5.705892364978791,\n","   'n_updates': 590001,\n","   'average_entropy': -3.0248394,\n","   'temperature': 0.038002315908670425,\n","   'eval_score': 2109.794043591211}])"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["agent.save(\"sac-default\")"],"metadata":{"id":"_fNkGRJQeGQY","executionInfo":{"status":"ok","timestamp":1701321963123,"user_tz":300,"elapsed":3,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"N9h11tqPyKvr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## online evaluation"],"metadata":{"id":"yIr16rRYfCW_"}},{"cell_type":"code","source":["agent.load(\"sac-default\")"],"metadata":{"id":"xKZ7qTYofjtR","executionInfo":{"status":"ok","timestamp":1701323290292,"user_tz":300,"elapsed":161,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["eval_stats = experiments.eval_performance(\n","    env=make_batch_env(test=True),\n","    agent=agent,\n","    n_steps=None,\n","    n_episodes=10,\n","    max_episode_len=timestep_limit,\n",")\n","print(\n","    \"n_runs: {} mean: {} median: {} stdev {}\".format(\n","        10,\n","        eval_stats[\"mean\"],\n","        eval_stats[\"median\"],\n","        eval_stats[\"stdev\"],\n","    )\n",")"],"metadata":{"id":"ACJ1HnCbcq8k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701323383503,"user_tz":300,"elapsed":14534,"user":{"displayName":"Shreelekha Revankar","userId":"04256412577763266582"}},"outputId":"3361817d-e385-4871-fd24-5d9d1952a0d5"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","INFO:pfrl.experiments.evaluator:evaluation episode 0 length: 1000 R: 2360.1097258202813\n","INFO:pfrl.experiments.evaluator:evaluation episode 1 length: 1000 R: 2393.9511577967955\n","INFO:pfrl.experiments.evaluator:evaluation episode 2 length: 572 R: 1700.7564995446824\n","INFO:pfrl.experiments.evaluator:evaluation episode 3 length: 544 R: 1637.4641112481022\n","INFO:pfrl.experiments.evaluator:evaluation episode 4 length: 386 R: 957.8309882652621\n","INFO:pfrl.experiments.evaluator:evaluation episode 5 length: 1000 R: 2296.60750763645\n","INFO:pfrl.experiments.evaluator:evaluation episode 6 length: 1000 R: 2355.53525757114\n","INFO:pfrl.experiments.evaluator:evaluation episode 7 length: 1000 R: 2335.6751360958015\n","INFO:pfrl.experiments.evaluator:evaluation episode 8 length: 430 R: 1002.2626485749136\n","INFO:pfrl.experiments.evaluator:evaluation episode 9 length: 619 R: 1794.0840525067797\n"]},{"output_type":"stream","name":"stdout","text":["n_runs: 10 mean: 1883.4277085060207 median: 2045.3457800716149 stdev 559.8631836789788\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"3JsAD9BiyLng"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## render (TODO)"],"metadata":{"id":"vvnsN-_Kkkko"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from IPython import display as ipythondisplay"],"metadata":{"id":"aEE9zpZokzpX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_env_for_render():\n","    env = gym.make(env_name, render_mode='rgb_array')\n","    env = gym.wrappers.RecordEpisodeStatistics(env)\n","    env.seed(seed)\n","    env.action_space.seed(seed)\n","    env.observation_space.seed(seed)\n","    return env"],"metadata":{"id":"zWMdWGs9lACY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env_ = make_env_for_render()"],"metadata":{"id":"gV4A2mvBlLbf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.axis('off')\n","done = False\n","obs = env_.reset()\n","\n","i = 0\n","while not done:\n","    i += 1\n","    if i % 20 == 0:\n","        ipythondisplay.clear_output(wait=True)\n","        # print(\"At timestep = \", i)\n","        screen = env_.render()\n","        plt.imshow(screen[0])\n","        ipythondisplay.display(plt.gcf())\n","\n","    action = agent.act(obs)\n","    obs, reward, done, info = env_.step(action)\n","\n","    if done:\n","        break"],"metadata":{"id":"cwlgrtDRkmy2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pReZTYC1yMSg"},"execution_count":null,"outputs":[]}]}