{"cells":[{"cell_type":"markdown","metadata":{"id":"4ZNZOeJhUvi5"},"source":["## Installation (work on colab)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"JKvWsG4iUn2K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701801875605,"user_tz":300,"elapsed":58987,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}},"outputId":"79ec4c57-cd50-44b0-cdbf-1bb11707f11e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","software-properties-common is already the newest version (0.99.22.8).\n","The following additional packages will be installed:\n","  libegl-dev libgl-dev libgles-dev libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev\n","  libglvnd-dev libglx-dev libopengl-dev libosmesa6\n","The following NEW packages will be installed:\n","  libegl-dev libgl-dev libgl1-mesa-dev libgl1-mesa-glx libgles-dev libgles1 libglew-dev\n","  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev libopengl-dev libosmesa6\n","  libosmesa6-dev\n","0 upgraded, 15 newly installed, 0 to remove and 15 not upgraded.\n","Need to get 3,952 kB of archives.\n","After this operation, 18.7 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5,584 B]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.0.4-0ubuntu1~22.04.1 [6,510 B]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libglew-dev amd64 2.2.0-4 [287 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6 amd64 23.0.4-0ubuntu1~22.04.1 [3,054 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6-dev amd64 23.0.4-0ubuntu1~22.04.1 [8,986 B]\n","Fetched 3,952 kB in 0s (17.4 MB/s)\n","Selecting previously unselected package libglx-dev:amd64.\n","(Reading database ... 120899 files and directories currently installed.)\n","Preparing to unpack .../00-libglx-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglx-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl-dev:amd64.\n","Preparing to unpack .../01-libgl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libegl-dev:amd64.\n","Preparing to unpack .../02-libegl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libegl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-glx:amd64.\n","Preparing to unpack .../03-libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libgles1:amd64.\n","Preparing to unpack .../04-libgles1_1.4.0-1_amd64.deb ...\n","Unpacking libgles1:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles-dev:amd64.\n","Preparing to unpack .../05-libgles-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgles-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-core-dev:amd64.\n","Preparing to unpack .../06-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libopengl-dev:amd64.\n","Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-dev:amd64.\n","Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-dev:amd64.\n","Preparing to unpack .../09-libgl1-mesa-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglu1-mesa-dev:amd64.\n","Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglew-dev:amd64.\n","Preparing to unpack .../12-libglew-dev_2.2.0-4_amd64.deb ...\n","Unpacking libglew-dev:amd64 (2.2.0-4) ...\n","Selecting previously unselected package libosmesa6:amd64.\n","Preparing to unpack .../13-libosmesa6_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libosmesa6:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libosmesa6-dev:amd64.\n","Preparing to unpack .../14-libosmesa6-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libosmesa6-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Setting up libgles1:amd64 (1.4.0-1) ...\n","Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up libglx-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up libopengl-dev:amd64 (1.4.0-1) ...\n","Setting up libgl-dev:amd64 (1.4.0-1) ...\n","Setting up libosmesa6:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up libegl-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Setting up libosmesa6-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up libgles-dev:amd64 (1.4.0-1) ...\n","Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n","Setting up libglew-dev:amd64 (2.2.0-4) ...\n","Setting up libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  patchelf\n","0 upgraded, 1 newly installed, 0 to remove and 15 not upgraded.\n","Need to get 72.1 kB of archives.\n","After this operation, 186 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 patchelf amd64 0.14.3-1 [72.1 kB]\n","Fetched 72.1 kB in 0s (1,308 kB/s)\n","Selecting previously unselected package patchelf.\n","(Reading database ... 121039 files and directories currently installed.)\n","Preparing to unpack .../patchelf_0.14.3-1_amd64.deb ...\n","Unpacking patchelf (0.14.3-1) ...\n","Setting up patchelf (0.14.3-1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n","Collecting free-mujoco-py\n","  Downloading free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Cython<0.30.0,>=0.29.24 (from free-mujoco-py)\n","  Downloading Cython-0.29.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.16.0)\n","Collecting fasteners==0.15 (from free-mujoco-py)\n","  Downloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n","Collecting glfw<2.0.0,>=1.4.0 (from free-mujoco-py)\n","  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (2.31.6)\n","Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.23.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fasteners==0.15->free-mujoco-py) (1.16.0)\n","Collecting monotonic>=0.1 (from fasteners==0.15->free-mujoco-py)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n","Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (9.4.0)\n","Installing collected packages: monotonic, glfw, fasteners, Cython, free-mujoco-py\n","  Attempting uninstall: Cython\n","    Found existing installation: Cython 3.0.6\n","    Uninstalling Cython-3.0.6:\n","      Successfully uninstalled Cython-3.0.6\n","Successfully installed Cython-0.29.36 fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 monotonic-1.6\n","Collecting imageio==2.4.1\n","  Downloading imageio-2.4.1.tar.gz (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.1) (1.23.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.1) (9.4.0)\n","Building wheels for collected packages: imageio\n","  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imageio: filename=imageio-2.4.1-py3-none-any.whl size=3303885 sha256=a43ad47b56c2e7039683c2f3d6af1e1e2236fefe08f151e55bfbe90803732af2\n","  Stored in directory: /root/.cache/pip/wheels/96/5d/ce/bdbdb04744dac03906336eb0d01ff1e222061d3419c55c55f9\n","Successfully built imageio\n","Installing collected packages: imageio\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.31.6\n","    Uninstalling imageio-2.31.6:\n","      Successfully uninstalled imageio-2.31.6\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","free-mujoco-py 2.1.6 requires imageio<3.0.0,>=2.9.0, but you have imageio 2.4.1 which is incompatible.\n","moviepy 1.0.3 requires imageio<3.0,>=2.5; python_version >= \"3.4\", but you have imageio 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed imageio-2.4.1\n","Collecting colabgymrender\n","  Downloading colabgymrender-1.1.0.tar.gz (3.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from colabgymrender) (1.0.3)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (4.4.2)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (4.66.1)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (2.31.0)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (0.1.10)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (1.23.5)\n","Collecting imageio<3.0,>=2.5 (from moviepy->colabgymrender)\n","  Downloading imageio-2.33.0-py3-none-any.whl (313 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.3/313.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (0.4.9)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy->colabgymrender) (9.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy->colabgymrender) (67.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (2023.11.17)\n","Building wheels for collected packages: colabgymrender\n","  Building wheel for colabgymrender (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for colabgymrender: filename=colabgymrender-1.1.0-py3-none-any.whl size=3113 sha256=a729b8e8e9e4a2e962c566b5db3c5c1cac3ec74c2cf85e4c0c62ed7036a68376\n","  Stored in directory: /root/.cache/pip/wheels/13/62/63/7b3acfb684dd3d665d7fc1d213427b136205a222389767e295\n","Successfully built colabgymrender\n","Installing collected packages: imageio, colabgymrender\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.4.1\n","    Uninstalling imageio-2.4.1:\n","      Successfully uninstalled imageio-2.4.1\n","Successfully installed colabgymrender-1.1.0 imageio-2.33.0\n","Collecting mujoco\n","  Downloading mujoco-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.4.0)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.5.2)\n","Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.23.5)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco) (3.1.7)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (2023.6.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (6.1.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (4.5.0)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (3.17.0)\n","Installing collected packages: mujoco\n","Successfully installed mujoco-3.0.1\n"]}],"source":["!apt-get install -y \\\n","    libgl1-mesa-dev \\\n","    libgl1-mesa-glx \\\n","    libglew-dev \\\n","    libosmesa6-dev \\\n","    software-properties-common\n","\n","!apt-get install -y patchelf\n","!pip install gym\n","\n","!pip install free-mujoco-py\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install imageio==2.4.1\n","!pip install -U colabgymrender\n","!pip install mujoco"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ZhFKM-rDS0yE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701801886306,"user_tz":300,"elapsed":10710,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}},"outputId":"d98b19db-5941-484a-ddf8-babc49376f5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pfrl\n","  Downloading pfrl-0.4.0.tar.gz (112 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/112.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m92.2/112.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/112.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pfrl) (2.1.0+cu118)\n","Requirement already satisfied: gym>=0.9.7 in /usr/local/lib/python3.10/dist-packages (from pfrl) (0.25.2)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from pfrl) (1.23.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pfrl) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from pfrl) (3.13.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.9.7->pfrl) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.9.7->pfrl) (0.0.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->pfrl) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->pfrl) (1.3.0)\n","Building wheels for collected packages: pfrl\n","  Building wheel for pfrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pfrl: filename=pfrl-0.4.0-py3-none-any.whl size=155461 sha256=25733577e158c56225409d244b2ccaddcde17236c7a927f78258183364d1cb20\n","  Stored in directory: /root/.cache/pip/wheels/22/4a/0f/a87cd1ae925086eb3a1b8759f620fcf48e47939fb082946c3b\n","Successfully built pfrl\n","Installing collected packages: pfrl\n","Successfully installed pfrl-0.4.0\n"]}],"source":["!pip install pfrl"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ofM7lHvOUVDc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701801893978,"user_tz":300,"elapsed":7680,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}},"outputId":"962b181d-5895-4c13-8c84-3f5095054e81"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"]}],"source":["!pip install gymnasium"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Oh_oFqaITeKZ","executionInfo":{"status":"ok","timestamp":1701801899555,"user_tz":300,"elapsed":5579,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["from typing import Optional, List\n","from copy import deepcopy\n","\n","import functools\n","import numpy as np\n","from sklearn.utils import check_random_state\n","\n","import gymnasium as gym\n","import gym.wrappers\n","from gym.spaces import Discrete, Box\n","\n","import pfrl"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"joyBiJ7Sedjh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701801920558,"user_tz":300,"elapsed":21006,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}},"outputId":"847f8541-8c07-4710-c19f-7fd2ab244cb1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"62iQsL5jeqca","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701802176593,"user_tz":300,"elapsed":275,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}},"outputId":"b69993b7-96b3-4850-e06a-2d55d83fe02a"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1u1b_5X-UjRqHkfYLcfMEoNxHfy2ozg7f/class-learning-for-robot-decision-making/_final_project/codes\n"]}],"source":["%cd /content/drive/MyDrive/class-learning-for-robot-decision-making/_final_project/codes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zj1lYUccWvmT"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"20oAIZZjEQ05"},"source":["# Original env"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"59FrC2_ziiqL","executionInfo":{"status":"ok","timestamp":1701801920558,"user_tz":300,"elapsed":2,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["env_name = \"Hopper-v3\"\n","num_envs = 5\n","seed = 42\n","\n","monitor = False\n","render = False\n","\n","process_seeds = np.arange(num_envs) + seed * num_envs"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"sIrESw-ZUcLs","executionInfo":{"status":"ok","timestamp":1701801920558,"user_tz":300,"elapsed":1,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["def make_env(process_idx, test):\n","    env = gym.make(env_name)  #\n","    # Unwrap TimiLimit wrapper\n","    assert isinstance(env, gym.wrappers.TimeLimit)\n","    env = env.env\n","    # Use different random seeds for train and test envs\n","    process_seed = int(process_seeds[process_idx])\n","    env_seed = 2**32 - 1 - process_seed if test else process_seed\n","    env.seed(env_seed)\n","    # Cast observations to float32 because our model uses float32\n","    env = pfrl.wrappers.CastObservationToFloat32(env)\n","    # Normalize action space to [-1, 1]^n\n","    env = pfrl.wrappers.NormalizeActionSpace(env)\n","    if monitor:\n","        env = gym.wrappers.Monitor(env, \"monitor\")\n","    if render:\n","        env = pfrl.wrappers.Render(env)\n","    return env"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71354,"status":"ok","timestamp":1701801991910,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"},"user_tz":300},"id":"lxY5rhsHENzN","outputId":"7a2090ae-4563-4c6a-cc54-5f0117efa5d3"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Compiling /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx because it changed.\n","[1/1] Cythonizing /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx\n"]},{"output_type":"stream","name":"stderr","text":["INFO:root:running build_ext\n","INFO:root:building 'mujoco_py.cymj' extension\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl\n","INFO:root:x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/mujoco_py -I/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.o -fopenmp -w\n","INFO:root:x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/mujoco_py -I/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c /usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.c -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.o -fopenmp -w\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310/mujoco_py\n","INFO:root:x86_64-linux-gnu-gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.o -L/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -L/usr/lib/x86_64-linux-gnu -Wl,--enable-new-dtags,-R/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -lmujoco210 -lglewosmesa -lOSMesa -lGL -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310/mujoco_py/cymj.cpython-310-x86_64-linux-gnu.so -fopenmp\n","<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n"]}],"source":["env = make_env(0, test=False)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701801991911,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"},"user_tz":300},"id":"mOhV8u_2Etdg","outputId":"b751ac4f-1a18-4f46-aaf4-2c037e139da2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"execute_result","data":{"text/plain":["Box(-inf, inf, (11,), float64)"]},"metadata":{},"execution_count":10}],"source":["env.observation_space"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1701801991911,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"},"user_tz":300},"id":"1aGiR0BLExZc","outputId":"e930f4f3-52f4-4e72-89e3-171e169b2ffc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(-1.0, 1.0, (3,), float32)"]},"metadata":{},"execution_count":11}],"source":["env.action_space"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"ZpkGfiWdEvph","executionInfo":{"status":"ok","timestamp":1701801991911,"user_tz":300,"elapsed":3,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["obs_size = env.observation_space.shape[0]\n","action_size = env.action_space.shape[0]\n","timestep_limit = env.spec.max_episode_steps"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"DAdddX9cWwQF","executionInfo":{"status":"ok","timestamp":1701801991911,"user_tz":300,"elapsed":2,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"H39EG4rTU6FC"},"source":["# Loading base envs and base policies"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"PFh9GX6EVaW1","executionInfo":{"status":"ok","timestamp":1701801991911,"user_tz":300,"elapsed":2,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["base_policies = None"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"NWbnxZvXVaUo","executionInfo":{"status":"ok","timestamp":1701802047699,"user_tz":300,"elapsed":831,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["# TODO\n","base_envs = []\n","xml_files = [\"hopper.xml\", \"envs/cobblestone_hopper_loose_v1.xml\", \"envs/sandpaper_hopper.xml\"]\n","for i, xml in enumerate(xml_files):\n","    env_ = gym.make(env_name, exclude_current_positions_from_observation=False, xml_file=xml)\n","    base_envs.append(env_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qMe_UKDx71-","executionInfo":{"status":"aborted","timestamp":1701801992210,"user_tz":300,"elapsed":6,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"8HACc3vhU1yz"},"source":["# Meta hopper env with terrain"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"9JWonVqSGWqI","executionInfo":{"status":"ok","timestamp":1701802052191,"user_tz":300,"elapsed":1,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["class HopperEnvWithTerrain(gym.Env):\n","\n","    def __init__(\n","        self,\n","        base_envs: List[gym.Env] = base_envs,\n","        base_policies: Optional[List[pfrl.agent.Agent]] = base_policies,\n","        average_distance_of_each_terrain: float = 1.0,\n","        dim_terrain_feature: int = 5,\n","        random_state: Optional[int] = None,\n","    ):\n","        if random_state is not None:\n","            self.random_ = check_random_state(random_state)\n","        else:\n","            self.random_ = check_random_state(12345)\n","\n","        self.base_envs = base_envs\n","        self.base_policies = base_policies\n","        self.use_base_policy = (base_policies is not None)\n","\n","        self.n_envs = len(base_envs)\n","\n","        if base_policies is not None:\n","            self.n_policies = len(base_policies)\n","        else:\n","            self.n_policies = None\n","\n","        self.base_terrain_feats = self.random_.uniform(\n","            size=(self.n_envs, dim_terrain_feature),\n","        )\n","\n","        self.observation_space = Box(\n","            low=np.concatenate(\n","                (np.zeros((dim_terrain_feature)),\n","                 self.base_envs[0].observation_space.low)),\n","            high=np.concatenate(\n","                (np.ones((dim_terrain_feature)),\n","                 self.base_envs[0].observation_space.high)),\n","        )\n","        self.base_envs[0].observation_space\n","        self.dim_observation = self.observation_space.shape[0] - 1\n","\n","        if self.use_base_policy:\n","            self.action_space = Discrete(3)\n","        else:\n","            self.action_space = self.base_envs[0].action_space\n","\n","        self.average_distance_of_each_terrain = average_distance_of_each_terrain\n","\n","    def _get_env_id_given_position(self):\n","        env_id = np.argmax(self.env_change_position > self.position)\n","        env_id = self.env_ids_position[env_id]\n","        return env_id\n","\n","    def _update_state_of_all_envs(\n","        self,\n","        target_env_id: int,\n","    ):\n","        current_env = self.base_envs[target_env_id]\n","        qpos = current_env.sim.data.qpos\n","        qvel = current_env.sim.data.qvel\n","        for i in range(self.n_envs):\n","            self.base_envs[i].set_state(qpos, qvel)\n","\n","    def reset(self):\n","        self.env_change_position = self.random_.normal(\n","            loc=self.average_distance_of_each_terrain,\n","            scale=self.average_distance_of_each_terrain * 0.1,\n","            size=(10000, )\n","        ).cumsum()\n","        self.env_ids_position = self.random_.choice(\n","            self.n_envs,\n","            size=10000,\n","            replace=True,\n","        )\n","\n","        for i in range(len(base_envs)):\n","            self.base_envs[i].reset()\n","\n","        env_id = self.env_ids_position[0]\n","        initial_state = self.base_envs[env_id].reset()\n","\n","        self._update_state_of_all_envs(env_id)\n","        self.position = initial_state[0]\n","\n","        self.state = initial_state[1:]\n","\n","        noisy_terrain_feats = self.random_.normal(\n","            loc=self.base_terrain_feats[env_id],\n","            scale=0.05,\n","        )\n","        initial_state = np.concatenate((noisy_terrain_feats, initial_state))\n","\n","        return initial_state\n","\n","    def step(self, action: np.array):\n","        env_id = self._get_env_id_given_position()\n","\n","        if self.use_base_policy:\n","            if action == -1:\n","                action = self.base_policies[env_id].act(self.state)\n","            else:\n","                action = self.base_policies[action].act(self.state)\n","\n","        state, reward, done, info = self.base_envs[env_id].step(action)\n","        self._update_state_of_all_envs(env_id)\n","\n","        self.position = state[0]\n","        self.state = state[1:]\n","\n","        noisy_terrain_feats = self.random_.normal(\n","            loc=self.base_terrain_feats[env_id],\n","            scale=0.05,\n","        )\n","        state = np.concatenate((noisy_terrain_feats, state))\n","\n","        return state, reward, done, info\n","\n","    def render(self):\n","        env_id = self._get_env_id_given_position()\n","        return self.base_envs[env_id].render()\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"tQIN2pyeTmIX","executionInfo":{"status":"ok","timestamp":1701802053708,"user_tz":300,"elapsed":275,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["gym.register(\n","    id=\"Hopper-with-terrain-v3\",\n","    entry_point=HopperEnvWithTerrain,\n","    max_episode_steps=timestep_limit,\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"Tbd2X9TVWx-R","executionInfo":{"status":"ok","timestamp":1701802054202,"user_tz":300,"elapsed":1,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["obs_size = env.observation_space.shape[0]\n","action_size = env.action_space.shape[0]\n","timestep_limit = env.spec.max_episode_steps"]},{"cell_type":"code","source":[],"metadata":{"id":"HiWxkXrJ1e3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7aEG-BTXXDm9"},"source":["## Initialize environment"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"AxrcUBuwVBot","executionInfo":{"status":"ok","timestamp":1701802057592,"user_tz":300,"elapsed":2,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["env_name = \"Hopper-with-terrain-v3\""]},{"cell_type":"code","execution_count":20,"metadata":{"id":"yBqINT71VQXL","executionInfo":{"status":"ok","timestamp":1701802057592,"user_tz":300,"elapsed":1,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["env = make_env(0, test=False)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"jR-KaV8_c5HA","executionInfo":{"status":"ok","timestamp":1701802057592,"user_tz":300,"elapsed":1,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["def make_batch_env(test):\n","    return pfrl.envs.MultiprocessVectorEnv(\n","        [\n","            functools.partial(make_env, idx, test)\n","            for idx, env in enumerate(range(num_envs))\n","        ]\n","    )"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"DO_0ZaKrWytJ","executionInfo":{"status":"ok","timestamp":1701802058448,"user_tz":300,"elapsed":1,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["obs_size = env.observation_space.shape[0]\n","action_size = env.action_space.shape[0]\n","timestep_limit = env.spec.max_episode_steps"]},{"cell_type":"code","source":[],"metadata":{"id":"JbJd8GsvxAwy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vw5gYvbBVfEj"},"source":["## SAC Config (with PFRL)\n","\n","sample codes and parameters in [this file](https://github.com/pfnet/pfrl/blob/master/examples/mujoco/reproduction/soft_actor_critic/train_soft_actor_critic.py)."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"yuhTJFd8xO1g","executionInfo":{"status":"ok","timestamp":1701802061992,"user_tz":300,"elapsed":1,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["import torch\n","from torch import distributions, nn\n","\n","from pfrl import experiments, replay_buffers, utils\n","from pfrl.nn.lmbda import Lambda"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"YSXLsGBMYMk2","executionInfo":{"status":"ok","timestamp":1701802061992,"user_tz":300,"elapsed":1,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["# default params\n","policy_output_scale = 1.0\n","\n","policy_hidden_dim = 256\n","policy_lr = 3e-4\n","\n","q_func_hidden_dim = 256\n","q_func_lr = 3e-4\n","\n","temperature_optimizer_lr = 3e-4\n","\n","buffer_size = 10 ** 6\n","replay_start_size = 10 ** 4\n","batch_size = 256\n","gamma = 0.99\n","\n","# steps = 10 ** 6\n","steps = 5 * 10 ** 5"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"aSlCn-rJX3WY","executionInfo":{"status":"ok","timestamp":1701802062975,"user_tz":300,"elapsed":1,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["class QFunction(torch.nn.Module):\n","\n","    def __init__(self, obs_size, n_actions):\n","        super().__init__()\n","        self.l1 = torch.nn.Linear(obs_size, 50)\n","        self.l2 = torch.nn.Linear(50, 50)\n","        self.l3 = torch.nn.Linear(50, n_actions)\n","\n","    def forward(self, x):\n","        h = x\n","        h = torch.nn.functional.relu(self.l1(h))\n","        h = torch.nn.functional.relu(self.l2(h))\n","        h = self.l3(h)\n","        return pfrl.action_value.DiscreteActionValue(h)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"i5hsyxCSxTeB","executionInfo":{"status":"ok","timestamp":1701802063692,"user_tz":300,"elapsed":253,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["def burnin_action_func():\n","    \"\"\"Select random actions until model is updated one or more times.\"\"\"\n","    return np.random.uniform(env.action_space.low, env.action_space.high).astype(np.float32)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"lOQoFLsixT_V","executionInfo":{"status":"ok","timestamp":1701802064790,"user_tz":300,"elapsed":1,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["def squashed_diagonal_gaussian_head(x):\n","    assert x.shape[-1] == action_size * 2\n","    mean, log_scale = torch.chunk(x, 2, dim=1)\n","    log_scale = torch.clamp(log_scale, -20.0, 2.0)\n","    var = torch.exp(log_scale * 2)\n","    base_distribution = distributions.Independent(\n","        distributions.Normal(loc=mean, scale=torch.sqrt(var)), 1\n","    )\n","    # cache_size=1 is required for numerical stability\n","    return distributions.transformed_distribution.TransformedDistribution(\n","        base_distribution, [distributions.transforms.TanhTransform(cache_size=1)]\n","    )"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"sRMAUbQCxWU8","executionInfo":{"status":"ok","timestamp":1701802065559,"user_tz":300,"elapsed":1,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["def make_policy_with_optimizer():\n","    policy = nn.Sequential(\n","        nn.Linear(obs_size, policy_hidden_dim),\n","        nn.ReLU(),\n","        nn.Linear(policy_hidden_dim, policy_hidden_dim),\n","        nn.ReLU(),\n","        nn.Linear(policy_hidden_dim, action_size * 2),\n","        Lambda(squashed_diagonal_gaussian_head),\n","    )\n","    torch.nn.init.xavier_uniform_(policy[0].weight)\n","    torch.nn.init.xavier_uniform_(policy[2].weight)\n","    torch.nn.init.xavier_uniform_(policy[4].weight, gain=policy_output_scale)\n","    policy_optimizer = torch.optim.Adam(policy.parameters(), lr=policy_lr)\n","    return policy, policy_optimizer"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"3MxHA2zmxYwb","executionInfo":{"status":"ok","timestamp":1701802066635,"user_tz":300,"elapsed":1,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["def make_q_func_with_optimizer():\n","    q_func = nn.Sequential(\n","        pfrl.nn.ConcatObsAndAction(),\n","        nn.Linear(obs_size + action_size, q_func_hidden_dim),\n","        nn.ReLU(),\n","        nn.Linear(q_func_hidden_dim, q_func_hidden_dim),\n","        nn.ReLU(),\n","        nn.Linear(q_func_hidden_dim, 1),\n","    )\n","    torch.nn.init.xavier_uniform_(q_func[1].weight)\n","    torch.nn.init.xavier_uniform_(q_func[3].weight)\n","    torch.nn.init.xavier_uniform_(q_func[5].weight)\n","    q_func_optimizer = torch.optim.Adam(q_func.parameters(), lr=q_func_lr)\n","    return q_func, q_func_optimizer"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"DyAyhIpqX7m0","executionInfo":{"status":"ok","timestamp":1701802070857,"user_tz":300,"elapsed":1659,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["policy, policy_optimizer = make_policy_with_optimizer()\n","q_func1, q_func1_optimizer = make_q_func_with_optimizer()\n","q_func2, q_func2_optimizer = make_q_func_with_optimizer()"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"Mt-gMOeKxgi7","executionInfo":{"status":"ok","timestamp":1701802070857,"user_tz":300,"elapsed":1,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ed74bc56-800a-4394-8eaa-818e69e66a65"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["rbuffer = replay_buffers.ReplayBuffer(buffer_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8uMedFsLxgY5"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"gY1IkWhJdMK8"},"source":["## Train SAC policy online"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"JEpNKlQPX7vw","executionInfo":{"status":"ok","timestamp":1701802078261,"user_tz":300,"elapsed":5093,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["agent = pfrl.agents.SoftActorCritic(\n","    policy,\n","    q_func1,\n","    q_func2,\n","    policy_optimizer,\n","    q_func1_optimizer,\n","    q_func2_optimizer,\n","    rbuffer,\n","    gamma=gamma,\n","    replay_start_size=replay_start_size,\n","    gpu=0 if torch.cuda.is_available() else -1,\n","    minibatch_size=batch_size,\n","    burnin_action_func=burnin_action_func,\n","    entropy_target=-action_size,\n","    temperature_optimizer_lr=temperature_optimizer_lr,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O1KQLV3SbeEX","outputId":"763ae3f0-d83d-4ee8-d957-6892a10c80d5","executionInfo":{"status":"ok","timestamp":1701720420106,"user_tz":300,"elapsed":10396920,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:1000 episode:44 last_R: 15.204790177321478 average_R:16.967367729319264\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:2000 episode:92 last_R: 10.822416051959722 average_R:17.15368660875731\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:3000 episode:139 last_R: 10.20309895608732 average_R:17.163553375075583\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:4000 episode:182 last_R: 49.32594627966339 average_R:18.0140179296707\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:5000 episode:226 last_R: 7.99127205762616 average_R:19.314653055343765\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 15 R: 11.753258313751845\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 15 R: 11.736402640754875\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 15 R: 11.656594514599453\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 15 R: 11.686759386833801\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 15 R: 11.720749775320177\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 17 R: 10.519621180202497\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 16 R: 10.31873825090714\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 16 R: 10.447167891154626\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 17 R: 10.420746652667063\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 16 R: 10.329812339830504\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 14 R: 12.604204069179067\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 14 R: 12.590513258966148\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 14 R: 12.561230195612655\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 16 R: 10.51569083257613\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 16 R: 10.422451735673919\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 15 R: 12.961706064460657\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 14 R: 12.413255723856135\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 14 R: 12.641524205191825\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 16 R: 10.480855375896693\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 16 R: 10.503194856116354\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 15 R: 11.56120878564522\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 15 R: 11.689881417641018\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 15 R: 11.622603208827488\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 15 R: 13.075395773966974\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 14 R: 12.472188259054494\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 15 R: 11.813574572193724\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 15 R: 11.78692717012779\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 15 R: 11.830662674078562\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 15 R: 12.965310696356546\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 15 R: 11.610310073672524\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated -3.4028235e+38 -> 11.623751329837196\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:6000 episode:272 last_R: 32.47702096795898 average_R:19.05383841059044\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:7000 episode:318 last_R: 9.22735826946982 average_R:16.81627083845825\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:8000 episode:364 last_R: 11.588377651045366 average_R:18.875260339661352\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:9000 episode:414 last_R: 8.911624843590532 average_R:17.33993357245374\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","/usr/local/lib/python3.10/dist-packages/pfrl/replay_buffer.py:180: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n","  \"action\": torch.as_tensor(\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:10000 episode:453 last_R: 8.193143304727775 average_R:19.0525304216397\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', -0.17558007), ('average_q2', -0.23542228), ('average_q_func1_loss', 3.1768946647644043), ('average_q_func2_loss', 3.3060193061828613), ('n_updates', 1), ('average_entropy', 1.5813969), ('temperature', 0.9997000694274902)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 15 R: 13.418902138002846\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 18 R: 11.23532250438676\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 17 R: 10.981304147385629\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 18 R: 11.352997075536402\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 19 R: 11.352411374407989\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 15 R: 13.418457710132001\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 15 R: 13.289453322811424\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 15 R: 13.371416516229202\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 15 R: 12.037008313439364\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 18 R: 11.077486369579644\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 15 R: 13.364721771655205\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 15 R: 13.410643175888845\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 16 R: 12.298379199611432\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 17 R: 11.02126769250336\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 16 R: 12.401318439509867\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 15 R: 12.04975267200977\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 17 R: 10.907269712147118\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 16 R: 12.32839673268077\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 16 R: 12.368454613172615\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 15 R: 13.562189641266222\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 15 R: 13.509523318216319\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 17 R: 11.058539664070336\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 15 R: 11.937252954979597\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 15 R: 13.595707782795563\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 15 R: 13.40083278857447\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 15 R: 12.018623650131344\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 15 R: 13.410231493196743\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 17 R: 11.198388809476805\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 17 R: 11.043803372909947\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 16 R: 12.378053365302033\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 11.623751329837196 -> 12.293270344066988\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:11000 episode:478 last_R: 16.781213325442845 average_R:22.83709470421102\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 11.116621), ('average_q2', 11.116559), ('average_q_func1_loss', 0.6564812311530113), ('average_q_func2_loss', 0.6508325806260109), ('n_updates', 1001), ('average_entropy', 1.760143), ('temperature', 0.7562717199325562)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:12000 episode:492 last_R: 136.19416614030413 average_R:34.55081314303141\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 19.489332), ('average_q2', 19.442566), ('average_q_func1_loss', 1.2411760365962983), ('average_q_func2_loss', 1.2555684062838555), ('n_updates', 2001), ('average_entropy', 1.2420743), ('temperature', 0.595487654209137)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:13000 episode:502 last_R: 204.3525020929458 average_R:49.52031584480128\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 28.75288), ('average_q2', 28.795355), ('average_q_func1_loss', 2.3954969584941863), ('average_q_func2_loss', 2.393452578186989), ('n_updates', 3001), ('average_entropy', 0.58898425), ('temperature', 0.48241332173347473)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:14000 episode:512 last_R: 191.5174585245878 average_R:67.72875185678616\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 38.225933), ('average_q2', 38.13139), ('average_q_func1_loss', 3.347603693008423), ('average_q_func2_loss', 3.3622692966461183), ('n_updates', 4001), ('average_entropy', -0.09219202), ('temperature', 0.39533737301826477)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:15000 episode:523 last_R: 175.301411907865 average_R:86.29284115089541\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 46.819084), ('average_q2', 46.875404), ('average_q_func1_loss', 3.7903609001636505), ('average_q_func2_loss', 3.8519053626060487), ('n_updates', 5001), ('average_entropy', -0.41934192), ('temperature', 0.32523313164711)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 73 R: 114.41121870530557\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 74 R: 116.16705301801565\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 97 R: 216.59941027328594\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 73 R: 114.09739204530398\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 96 R: 214.55130592844958\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 97 R: 216.4904016427848\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 74 R: 116.4609142671807\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 73 R: 115.34040922702118\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 75 R: 119.88222761260899\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 90 R: 178.08045793463032\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 73 R: 115.12009409381011\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 95 R: 213.53322491070128\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 74 R: 117.247457872727\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 95 R: 212.51258644303118\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 96 R: 216.56118259816918\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 89 R: 175.2220842546909\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 91 R: 179.5655952497034\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 90 R: 179.68167514969699\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 89 R: 178.1128574780975\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 95 R: 212.25108250970948\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 98 R: 218.8408746292937\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 91 R: 184.54591611752073\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 90 R: 178.8963143355233\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 74 R: 117.30087460562925\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 95 R: 212.5616732631617\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 88 R: 176.32794489790408\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 95 R: 212.26672440983484\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 73 R: 114.44241310375628\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 74 R: 116.79343514438696\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 90 R: 178.55733050866388\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 12.293270344066988 -> 167.74740440768662\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:16000 episode:536 last_R: 122.8967265503367 average_R:101.39071220349022\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 55.566574), ('average_q2', 55.569923), ('average_q_func1_loss', 5.626933116912841), ('average_q_func2_loss', 5.555335896015167), ('n_updates', 6001), ('average_entropy', -0.5554544), ('temperature', 0.2703019976615906)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:17000 episode:547 last_R: 190.13354202633818 average_R:118.19796575067485\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 62.14536), ('average_q2', 62.15818), ('average_q_func1_loss', 6.2666436743736265), ('average_q_func2_loss', 6.340217659473419), ('n_updates', 7001), ('average_entropy', -0.8547128), ('temperature', 0.22457607090473175)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:18000 episode:556 last_R: 218.62950073294672 average_R:133.8248594437581\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 70.65604), ('average_q2', 70.63578), ('average_q_func1_loss', 6.650753128528595), ('average_q_func2_loss', 6.649339618682862), ('n_updates', 8001), ('average_entropy', -1.2318594), ('temperature', 0.1861903965473175)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:19000 episode:565 last_R: 239.33336574573542 average_R:151.8507095327971\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 76.568886), ('average_q2', 76.622116), ('average_q_func1_loss', 6.904455013275147), ('average_q_func2_loss', 6.902113418579102), ('n_updates', 9001), ('average_entropy', -1.6152514), ('temperature', 0.15562519431114197)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:20000 episode:574 last_R: 268.2047757230083 average_R:165.81019305538393\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 80.97967), ('average_q2', 81.0113), ('average_q_func1_loss', 7.401262819766998), ('average_q_func2_loss', 7.123835413455963), ('n_updates', 10001), ('average_entropy', -1.921351), ('temperature', 0.13190008699893951)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 90 R: 197.29716427748258\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 130 R: 159.10346811974105\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 123 R: 254.35805007391812\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 124 R: 257.41407335985934\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 91 R: 199.25836925657836\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 126 R: 262.2949836771735\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 90 R: 197.7092355058518\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 91 R: 201.13900830432317\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 124 R: 251.9399822065118\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 136 R: 238.94937328906175\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 132 R: 182.38748781514514\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 92 R: 203.34677180418223\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 150 R: 246.58232314228502\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 91 R: 199.01976664216204\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 124 R: 246.4553825616635\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 91 R: 199.74149476627937\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 129 R: 189.32490976789538\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 129 R: 188.4533778421888\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 90 R: 197.14473474496\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 92 R: 206.04431382764108\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 90 R: 197.46402516727377\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 91 R: 199.9268419216017\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 150 R: 252.43328946876125\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 127 R: 263.29201670124013\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 125 R: 258.4814938190367\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 90 R: 197.29690778052404\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 149 R: 251.72455472129096\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 139 R: 196.07280489322952\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 125 R: 257.2990765302073\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 124 R: 251.066126974458\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 167.74740440768662 -> 220.10071363208425\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:21000 episode:583 last_R: 254.36213128636183 average_R:183.28480204556425\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 86.31161), ('average_q2', 86.37856), ('average_q_func1_loss', 7.518246252536773), ('average_q_func2_loss', 7.3973633527755736), ('n_updates', 11001), ('average_entropy', -1.8499187), ('temperature', 0.11270545423030853)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:22000 episode:589 last_R: 209.63495680870508 average_R:195.13494161854888\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 90.63686), ('average_q2', 90.76262), ('average_q_func1_loss', 9.033402037620544), ('average_q_func2_loss', 8.904113581180573), ('n_updates', 12001), ('average_entropy', -2.372912), ('temperature', 0.09787216037511826)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:23000 episode:596 last_R: 275.9520981694477 average_R:207.92094274935747\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 93.874664), ('average_q2', 93.83606), ('average_q_func1_loss', 9.38083212375641), ('average_q_func2_loss', 9.083912973403931), ('n_updates', 13001), ('average_entropy', -2.7417855), ('temperature', 0.08842843770980835)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:24000 episode:603 last_R: 326.0537620246695 average_R:216.26257959190113\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 99.309235), ('average_q2', 99.18137), ('average_q_func1_loss', 9.509571731090546), ('average_q_func2_loss', 9.321326954364777), ('n_updates', 14001), ('average_entropy', -2.8072681), ('temperature', 0.08446700870990753)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:25000 episode:608 last_R: 272.4471856087467 average_R:220.57291367820238\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 104.10988), ('average_q2', 104.013954), ('average_q_func1_loss', 9.586803798675538), ('average_q_func2_loss', 9.440652923583984), ('n_updates', 15001), ('average_entropy', -2.9215157), ('temperature', 0.08157888799905777)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 131 R: 240.79660466722643\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 134 R: 249.8158049162164\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 133 R: 250.1331872814089\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 215 R: 419.5643989461938\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 132 R: 243.1594314385954\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 218 R: 428.3696167802948\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 230 R: 441.8093469596346\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 128 R: 230.86094305970718\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 148 R: 296.2588661785717\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 133 R: 243.90661894761405\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 135 R: 249.31400368882635\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 127 R: 235.2338738147608\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 224 R: 441.25183764174545\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 131 R: 242.01195351170998\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 220 R: 406.9836868121066\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 142 R: 252.96006357717948\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 126 R: 225.0838167389265\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 159 R: 313.57530086549787\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 127 R: 233.42729072141773\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 217 R: 407.1259230122857\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 134 R: 249.87997074585076\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 219 R: 408.4702425603737\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 132 R: 251.1986957667597\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 128 R: 234.4379393673334\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 137 R: 238.30019341040307\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 214 R: 382.4757622109921\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 135 R: 243.41271427261748\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 135 R: 255.7706487568591\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 219 R: 409.16989876184533\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 108 R: 196.9150460973398\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 220.10071363208425 -> 297.3891227170098\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:26000 episode:616 last_R: 296.2593125873269 average_R:226.61464392386497\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 106.46192), ('average_q2', 106.35275), ('average_q_func1_loss', 9.64296352148056), ('average_q_func2_loss', 9.537347989082336), ('n_updates', 16001), ('average_entropy', -2.8652613), ('temperature', 0.08274499326944351)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:27000 episode:622 last_R: 290.9688132821699 average_R:231.94571713170353\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 108.012436), ('average_q2', 107.81361), ('average_q_func1_loss', 10.22919289112091), ('average_q_func2_loss', 10.264085664749146), ('n_updates', 17001), ('average_entropy', -3.105188), ('temperature', 0.08467371761798859)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:28000 episode:628 last_R: 342.4469812972284 average_R:241.24335584152337\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 110.22844), ('average_q2', 110.032906), ('average_q_func1_loss', 10.930563573837281), ('average_q_func2_loss', 10.667627110481263), ('n_updates', 18001), ('average_entropy', -2.9049273), ('temperature', 0.08055076748132706)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:29000 episode:637 last_R: 231.14519521060794 average_R:255.5834861535563\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 114.94724), ('average_q2', 115.12245), ('average_q_func1_loss', 10.468699436187745), ('average_q_func2_loss', 10.394802207946777), ('n_updates', 19001), ('average_entropy', -2.7021778), ('temperature', 0.08416689932346344)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:30000 episode:645 last_R: 309.71304368178426 average_R:261.0232785691148\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 114.342445), ('average_q2', 114.15969), ('average_q_func1_loss', 9.052737379074097), ('average_q_func2_loss', 8.876459584236144), ('n_updates', 20001), ('average_entropy', -3.1683161), ('temperature', 0.0845397561788559)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 154 R: 351.92969662159237\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 154 R: 341.06766864357854\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 151 R: 261.4488293192011\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 156 R: 351.30983109652584\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 150 R: 248.30502843426618\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 149 R: 236.70728629957588\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 152 R: 260.6721042826891\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 157 R: 353.4268552230652\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 155 R: 351.67129532778716\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 134 R: 327.07266606581993\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 133 R: 322.3960418149412\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 135 R: 323.902032698629\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 152 R: 260.9227396267858\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 151 R: 257.8428362283233\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 152 R: 259.2943358395217\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 136 R: 326.76734684482386\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 152 R: 244.7121421565999\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 153 R: 340.90619616874267\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 148 R: 237.64214912258308\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 146 R: 228.70838325768008\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 134 R: 326.5246998401745\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 148 R: 245.97146494545282\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 134 R: 329.19585264523545\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 137 R: 331.7852846187403\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 158 R: 355.96120496290644\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 134 R: 327.5410210804093\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 153 R: 340.9495995470805\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 150 R: 248.94797786112179\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 152 R: 258.1443009235728\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 135 R: 327.7621136172139\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 297.3891227170098 -> 299.3162995038213\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:31000 episode:651 last_R: 347.3626383811361 average_R:268.44043988094\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 115.71322), ('average_q2', 115.2621), ('average_q_func1_loss', 10.185746195316314), ('average_q_func2_loss', 10.447333102226258), ('n_updates', 21001), ('average_entropy', -3.2232862), ('temperature', 0.08566877245903015)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:32000 episode:659 last_R: 273.7596700774112 average_R:276.282523583548\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 119.26263), ('average_q2', 119.336), ('average_q_func1_loss', 9.837049591541291), ('average_q_func2_loss', 9.946480345726012), ('n_updates', 22001), ('average_entropy', -3.1331825), ('temperature', 0.08565115183591843)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:33000 episode:666 last_R: 271.42883067809305 average_R:282.86530635566083\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 120.96454), ('average_q2', 121.2027), ('average_q_func1_loss', 9.459380373954772), ('average_q_func2_loss', 9.724783160686492), ('n_updates', 23001), ('average_entropy', -2.9017205), ('temperature', 0.08612130582332611)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:34000 episode:674 last_R: 353.7064514353184 average_R:292.1408214024376\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.40731), ('average_q2', 123.663635), ('average_q_func1_loss', 10.151737847328185), ('average_q_func2_loss', 10.05554684638977), ('n_updates', 24001), ('average_entropy', -2.989324), ('temperature', 0.08352352678775787)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:35000 episode:680 last_R: 341.335668535305 average_R:295.58459596073556\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 124.21359), ('average_q2', 124.17542), ('average_q_func1_loss', 10.370877277851104), ('average_q_func2_loss', 10.110443425178527), ('n_updates', 25001), ('average_entropy', -2.8208227), ('temperature', 0.08254407346248627)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 120 R: 312.91506252205596\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 136 R: 348.4201248000747\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 107 R: 171.70105511543844\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 129 R: 212.3929535914648\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 104 R: 164.76512078895868\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 134 R: 346.0410800323887\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 138 R: 358.2889796312151\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 118 R: 304.49862304053175\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 136 R: 341.4032376426757\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 117 R: 301.93192676446773\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 145 R: 351.3320419516352\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 118 R: 305.3884252580776\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 119 R: 307.7093401508777\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 106 R: 170.46030464214715\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 130 R: 207.5580468664177\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 131 R: 216.12845250307643\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 135 R: 348.12348778893664\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 134 R: 347.91618783112114\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 145 R: 356.12910519764097\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 118 R: 305.242117919068\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 144 R: 354.34993711552636\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 129 R: 326.3765224636365\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 120 R: 312.8310631821082\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 102 R: 156.32577685335076\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 107 R: 170.57058782707915\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 133 R: 343.74726277751813\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 101 R: 166.97557849654888\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 118 R: 304.0783522563055\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 106 R: 171.00007646498042\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 105 R: 169.0064822689127\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:36000 episode:689 last_R: 327.1243317812203 average_R:298.2551326174527\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.192955), ('average_q2', 123.37978), ('average_q_func1_loss', 8.963946950435638), ('average_q_func2_loss', 9.041797399520874), ('n_updates', 26001), ('average_entropy', -2.9878871), ('temperature', 0.082522451877594)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:37000 episode:695 last_R: 333.0722140490432 average_R:299.3342024992217\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.96303), ('average_q2', 123.15463), ('average_q_func1_loss', 9.59492288351059), ('average_q_func2_loss', 9.62518247127533), ('n_updates', 27001), ('average_entropy', -2.9929395), ('temperature', 0.07916433364152908)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:38000 episode:704 last_R: 288.7590497222947 average_R:297.0901704212091\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.61668), ('average_q2', 123.83071), ('average_q_func1_loss', 10.23281893968582), ('average_q_func2_loss', 10.07474925518036), ('n_updates', 28001), ('average_entropy', -2.818337), ('temperature', 0.07713867723941803)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:39000 episode:710 last_R: 327.152097821467 average_R:300.1324693081077\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.82386), ('average_q2', 126.95631), ('average_q_func1_loss', 9.349731040000915), ('average_q_func2_loss', 9.233348333835602), ('n_updates', 29001), ('average_entropy', -2.9968138), ('temperature', 0.07336539030075073)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:40000 episode:719 last_R: 354.6870892315535 average_R:299.4407773661632\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.00124), ('average_q2', 128.50717), ('average_q_func1_loss', 9.480453732013702), ('average_q_func2_loss', 9.263550608158111), ('n_updates', 30001), ('average_entropy', -3.0707881), ('temperature', 0.07162618637084961)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 132 R: 243.18494129515636\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 133 R: 353.46555605351654\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 140 R: 353.5846767059282\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 135 R: 353.6215483195801\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 120 R: 289.3675864801787\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 142 R: 242.93883788227674\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 135 R: 231.09052946250318\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 132 R: 350.0756891980087\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 150 R: 250.4355069841506\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 117 R: 282.6354173573127\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 131 R: 346.0146887944914\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 138 R: 231.77801241597166\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 133 R: 349.3166290458371\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 116 R: 279.3139467499838\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 132 R: 343.96292085068086\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 134 R: 355.06311848603235\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 136 R: 229.01803502256067\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 121 R: 292.8435759784654\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 140 R: 227.41940175600277\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 134 R: 352.8046422211927\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 145 R: 266.0973730814539\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 119 R: 286.0333866309742\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 133 R: 352.6811064965683\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 133 R: 352.92107783618053\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 116 R: 280.1594090315626\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 149 R: 249.13210022987857\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 115 R: 277.20689960675884\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 134 R: 353.3221369682485\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 140 R: 356.0664274961308\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 133 R: 229.37621776201738\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:41000 episode:726 last_R: 323.7711561260551 average_R:295.76157205738417\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.22389), ('average_q2', 126.74257), ('average_q_func1_loss', 9.355512368679047), ('average_q_func2_loss', 9.045492146015167), ('n_updates', 31001), ('average_entropy', -2.802626), ('temperature', 0.07334501296281815)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:42000 episode:734 last_R: 373.5470157611934 average_R:295.949693690682\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.05923), ('average_q2', 127.898994), ('average_q_func1_loss', 7.859685316085815), ('average_q_func2_loss', 7.478439586162567), ('n_updates', 32001), ('average_entropy', -2.880397), ('temperature', 0.07442161440849304)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:43000 episode:741 last_R: 326.6307119906031 average_R:297.74193963127317\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 124.59883), ('average_q2', 124.76831), ('average_q_func1_loss', 7.649284093379975), ('average_q_func2_loss', 7.526683959960938), ('n_updates', 33001), ('average_entropy', -3.3144283), ('temperature', 0.07453710585832596)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:44000 episode:749 last_R: 236.3864746915678 average_R:300.94032891400906\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.99594), ('average_q2', 128.23117), ('average_q_func1_loss', 7.973685822486877), ('average_q_func2_loss', 8.145631728172303), ('n_updates', 34001), ('average_entropy', -2.9151804), ('temperature', 0.07729358226060867)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:45000 episode:756 last_R: 368.1820520256778 average_R:302.000424515557\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 124.457375), ('average_q2', 124.4998), ('average_q_func1_loss', 8.409743783473969), ('average_q_func2_loss', 8.320825901031494), ('n_updates', 35001), ('average_entropy', -3.2050128), ('temperature', 0.07797486335039139)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 123 R: 234.627630326067\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 130 R: 240.17057933999973\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 154 R: 358.31757998196866\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 118 R: 228.90246377428852\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 155 R: 370.2232441749129\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 128 R: 238.81688536805288\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 153 R: 354.3851601613941\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 154 R: 367.8720320143401\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 123 R: 234.45168387294396\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 154 R: 358.39689805007606\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 155 R: 374.28975039616535\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 125 R: 310.00918888301635\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 126 R: 325.5623167639232\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 125 R: 309.65620208565264\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 154 R: 360.84186665546\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 154 R: 359.4322377300632\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 127 R: 236.68176507465395\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 155 R: 360.9441630436182\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 127 R: 235.65106247848271\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 156 R: 372.54379455100525\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 138 R: 366.82630541583177\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 154 R: 361.51326346805126\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 122 R: 233.4899613989808\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 127 R: 236.24200075499454\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 123 R: 297.2785030544699\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 153 R: 362.6021361368029\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 154 R: 363.40980516983257\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 155 R: 372.6225251883369\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 128 R: 333.0129970843788\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 155 R: 365.28852791079595\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 299.3162995038213 -> 317.46875101028536\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:46000 episode:763 last_R: 352.2938547689952 average_R:302.41804857351855\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.51578), ('average_q2', 127.95095), ('average_q_func1_loss', 8.175614616870881), ('average_q_func2_loss', 8.035749149322509), ('n_updates', 36001), ('average_entropy', -2.77104), ('temperature', 0.07502700388431549)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:47000 episode:771 last_R: 231.08968441701202 average_R:300.73135375836864\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.734985), ('average_q2', 125.76583), ('average_q_func1_loss', 7.388726782798767), ('average_q_func2_loss', 7.093850793838501), ('n_updates', 37001), ('average_entropy', -3.1022432), ('temperature', 0.07401352375745773)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:48000 episode:776 last_R: 370.9422911673014 average_R:302.0371024077629\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.98955), ('average_q2', 126.89922), ('average_q_func1_loss', 8.116804559230804), ('average_q_func2_loss', 7.556127145290374), ('n_updates', 38001), ('average_entropy', -2.9827058), ('temperature', 0.07251249253749847)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:49000 episode:784 last_R: 286.59586821233165 average_R:300.73734479401645\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.26357), ('average_q2', 128.03262), ('average_q_func1_loss', 7.2153740000724795), ('average_q_func2_loss', 7.115693094730378), ('n_updates', 39001), ('average_entropy', -2.897217), ('temperature', 0.07107264548540115)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:50000 episode:792 last_R: 244.5486574453449 average_R:298.2853704944418\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.938484), ('average_q2', 125.71501), ('average_q_func1_loss', 6.86320969581604), ('average_q_func2_loss', 6.664995210170746), ('n_updates', 40001), ('average_entropy', -3.0765018), ('temperature', 0.06769993156194687)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 152 R: 324.8175124686698\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 175 R: 283.722302011656\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 159 R: 353.2967147587309\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 147 R: 344.97366684154844\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 159 R: 351.1713229501755\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 196 R: 355.6805123157595\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 152 R: 367.60322893408505\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 159 R: 344.2252138997327\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 150 R: 353.1413659286694\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 162 R: 252.0194932027084\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 194 R: 359.16624682542493\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 176 R: 286.69456495308094\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 155 R: 342.18753571057374\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 150 R: 352.8243097010645\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 127 R: 290.2941281968041\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 154 R: 360.3026955397371\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 149 R: 322.0032377217477\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 159 R: 233.16219433557362\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 162 R: 246.29009137669422\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 175 R: 289.19155146793963\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 157 R: 345.40508256900927\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 152 R: 337.95064525269294\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 169 R: 267.10546954536017\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 158 R: 252.1334864444822\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 162 R: 274.75576286369653\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 153 R: 338.83723228818087\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 156 R: 339.031339143239\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 152 R: 358.6797626726534\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 154 R: 345.40003030250705\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 130 R: 295.5629321796508\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 317.46875101028536 -> 318.92098774672826\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:51000 episode:797 last_R: 282.1626691240198 average_R:297.7092561243491\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.187706), ('average_q2', 125.938866), ('average_q_func1_loss', 8.010491380691528), ('average_q_func2_loss', 7.687889800071717), ('n_updates', 41001), ('average_entropy', -2.7286084), ('temperature', 0.06609222292900085)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:52000 episode:805 last_R: 307.15986834360575 average_R:300.4823088880343\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 124.69269), ('average_q2', 124.81791), ('average_q_func1_loss', 7.45823070526123), ('average_q_func2_loss', 6.880204713344574), ('n_updates', 42001), ('average_entropy', -2.905561), ('temperature', 0.06596081703901291)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:53000 episode:810 last_R: 381.00997585118387 average_R:301.7118419520385\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.18206), ('average_q2', 126.09719), ('average_q_func1_loss', 8.218269715309143), ('average_q_func2_loss', 7.844931952953338), ('n_updates', 43001), ('average_entropy', -2.9617167), ('temperature', 0.06315536797046661)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:54000 episode:817 last_R: 209.70798701853064 average_R:300.9192108093224\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.594185), ('average_q2', 123.2235), ('average_q_func1_loss', 7.527801022529602), ('average_q_func2_loss', 7.279299902915954), ('n_updates', 44001), ('average_entropy', -2.9258754), ('temperature', 0.05998646840453148)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:55000 episode:825 last_R: 279.4233274189815 average_R:304.56746386376875\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 124.089), ('average_q2', 124.15546), ('average_q_func1_loss', 7.6001570868492125), ('average_q_func2_loss', 7.814398910999298), ('n_updates', 45001), ('average_entropy', -3.0278785), ('temperature', 0.05726976320147514)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 139 R: 211.1760643646659\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 117 R: 293.07691745150294\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 117 R: 295.0180706513312\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 136 R: 209.35007092317684\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 135 R: 326.9428571677968\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 146 R: 240.97278527291184\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 123 R: 312.81738967203745\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 141 R: 346.23603950237884\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 119 R: 303.304965260438\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 145 R: 357.38981528991616\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 119 R: 302.31089453554677\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 118 R: 297.65689472918183\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 145 R: 352.0822479381408\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 148 R: 347.86441504339564\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 135 R: 332.5854965223633\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 137 R: 210.2811246323188\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 139 R: 211.56103162068555\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 119 R: 303.8968130959999\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 117 R: 295.63472388498036\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 144 R: 216.31518291125724\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 139 R: 338.6838454243095\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 148 R: 338.3158063495418\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 119 R: 300.61783249440396\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 119 R: 305.5673815511018\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 144 R: 352.02689991648305\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 141 R: 345.75420960708414\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 142 R: 338.22106774423185\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 118 R: 299.31426950735863\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 141 R: 347.44765900792675\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 140 R: 212.23289049080407\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:56000 episode:831 last_R: 266.7919487167416 average_R:305.3673704716653\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.65964), ('average_q2', 123.90601), ('average_q_func1_loss', 7.427386674880982), ('average_q_func2_loss', 7.431646468639374), ('n_updates', 46001), ('average_entropy', -2.8173487), ('temperature', 0.05606279894709587)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:57000 episode:838 last_R: 361.6681257705658 average_R:302.703009265657\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.098404), ('average_q2', 126.032875), ('average_q_func1_loss', 5.889580914974212), ('average_q_func2_loss', 5.92194331407547), ('n_updates', 47001), ('average_entropy', -2.77669), ('temperature', 0.056451860815286636)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:58000 episode:847 last_R: 252.80501404840047 average_R:301.36585591114164\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.091), ('average_q2', 121.86244), ('average_q_func1_loss', 6.039747428894043), ('average_q_func2_loss', 6.129570391178131), ('n_updates', 48001), ('average_entropy', -2.8031561), ('temperature', 0.05533839762210846)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:59000 episode:852 last_R: 225.13084709940898 average_R:302.6155406069646\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.675705), ('average_q2', 123.82319), ('average_q_func1_loss', 5.831777091026306), ('average_q_func2_loss', 5.599246029853821), ('n_updates', 49001), ('average_entropy', -2.8765981), ('temperature', 0.05445118248462677)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:60000 episode:860 last_R: 272.6201192991871 average_R:299.2377493542232\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.78739), ('average_q2', 123.10428), ('average_q_func1_loss', 5.6164488077163695), ('average_q_func2_loss', 5.289942495822906), ('n_updates', 50001), ('average_entropy', -2.8908067), ('temperature', 0.052786312997341156)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 147 R: 401.1859722672979\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 154 R: 261.6369757805669\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 157 R: 267.32489687154646\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 123 R: 314.76415020821076\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 128 R: 325.82441628328525\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 134 R: 360.51323809110977\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 148 R: 404.284587046167\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 148 R: 405.239446379696\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 143 R: 366.6408908455951\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 146 R: 391.9528546423857\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 143 R: 366.39708977476874\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 132 R: 361.6799564916031\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 135 R: 266.32968030262094\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 136 R: 264.14799371274665\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 128 R: 211.09425215333786\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 137 R: 248.08160750631342\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 135 R: 334.860635498197\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 133 R: 334.1860237682062\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 131 R: 351.9684231253556\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 154 R: 264.06046305075284\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 148 R: 400.20308758034963\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 131 R: 319.59691598198884\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 148 R: 394.16760916948897\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 146 R: 391.68682472026\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 155 R: 263.3132993351065\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 157 R: 268.2936170032277\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 124 R: 217.14889003496404\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 131 R: 351.884906449579\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 135 R: 374.84436100858784\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 131 R: 217.7644857962859\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 318.92098774672826 -> 323.36925169598675\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:61000 episode:868 last_R: 380.93393642700585 average_R:300.42235313772426\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 120.33763), ('average_q2', 120.34861), ('average_q_func1_loss', 6.488328857421875), ('average_q_func2_loss', 6.129932208061218), ('n_updates', 51001), ('average_entropy', -3.0284393), ('temperature', 0.05382939800620079)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:62000 episode:874 last_R: 258.5079715422381 average_R:303.2251877899213\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.66164), ('average_q2', 122.841034), ('average_q_func1_loss', 5.7539400863647465), ('average_q_func2_loss', 5.600012679100036), ('n_updates', 52001), ('average_entropy', -3.0489676), ('temperature', 0.052780210971832275)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:63000 episode:882 last_R: 255.08474126817575 average_R:302.5954733726446\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 121.401596), ('average_q2', 121.00746), ('average_q_func1_loss', 4.949531672000885), ('average_q_func2_loss', 4.8539112448692325), ('n_updates', 53001), ('average_entropy', -3.0033233), ('temperature', 0.054771050810813904)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:64000 episode:890 last_R: 345.5855896496675 average_R:301.0509525072238\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 118.623314), ('average_q2', 118.50676), ('average_q_func1_loss', 5.657121374607086), ('average_q_func2_loss', 5.5234563708305355), ('n_updates', 54001), ('average_entropy', -2.8924346), ('temperature', 0.055781397968530655)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:65000 episode:897 last_R: 251.22212186875436 average_R:303.09132847893534\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 121.29489), ('average_q2', 121.24704), ('average_q_func1_loss', 5.059508438110352), ('average_q_func2_loss', 5.151977744102478), ('n_updates', 55001), ('average_entropy', -3.1606257), ('temperature', 0.054202355444431305)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 123 R: 335.7482485644823\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 122 R: 338.09759493622016\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 141 R: 251.78856691001775\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 148 R: 367.7999514115665\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 127 R: 354.20278067968\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 128 R: 358.9124530769132\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 122 R: 338.4622941504885\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 152 R: 386.17096308493745\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 142 R: 257.2317022230101\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 143 R: 256.425779367525\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 140 R: 253.3169206659646\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 147 R: 367.3053482497595\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 152 R: 392.7880245735973\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 126 R: 356.5393328766322\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 142 R: 254.2776327240183\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 122 R: 337.1963454464806\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 141 R: 253.69751843630513\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 148 R: 368.5296951456126\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 140 R: 253.44146879780158\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 128 R: 354.5219140000539\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 126 R: 352.02211149274615\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 125 R: 352.4720660263651\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 149 R: 371.25041414675144\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 131 R: 349.9548377605054\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 124 R: 350.7207429991614\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 129 R: 365.2838471287929\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 126 R: 353.10318929506576\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 126 R: 357.0395537279099\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 143 R: 257.55685467130587\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 138 R: 249.42839648950246\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 323.36925169598675 -> 326.5095516353058\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:66000 episode:904 last_R: 328.4828634661404 average_R:302.7266398888253\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.32789), ('average_q2', 122.6805), ('average_q_func1_loss', 5.568350337743759), ('average_q_func2_loss', 5.477053527832031), ('n_updates', 56001), ('average_entropy', -2.8469334), ('temperature', 0.051716696470975876)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:67000 episode:912 last_R: 261.27224463648326 average_R:303.022762862213\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 120.15534), ('average_q2', 119.90228), ('average_q_func1_loss', 5.909612673521042), ('average_q_func2_loss', 5.851778209209442), ('n_updates', 57001), ('average_entropy', -2.6577704), ('temperature', 0.0508604533970356)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:68000 episode:920 last_R: 345.66197297923077 average_R:307.6799442302484\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 121.15939), ('average_q2', 121.04946), ('average_q_func1_loss', 5.3747903966903685), ('average_q_func2_loss', 5.1679275870323185), ('n_updates', 58001), ('average_entropy', -2.7114046), ('temperature', 0.04897370934486389)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:69000 episode:928 last_R: 171.03482177881693 average_R:301.7362975794425\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 119.13159), ('average_q2', 118.88344), ('average_q_func1_loss', 5.4239156579971315), ('average_q_func2_loss', 5.621782200336456), ('n_updates', 59001), ('average_entropy', -3.15293), ('temperature', 0.045932501554489136)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:70000 episode:940 last_R: 18.196741331694394 average_R:283.85525316387316\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.391014), ('average_q2', 122.50424), ('average_q_func1_loss', 6.526731491088867), ('average_q_func2_loss', 6.317715957164764), ('n_updates', 60001), ('average_entropy', -3.0337033), ('temperature', 0.04775659367442131)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 135 R: 387.930865423057\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 39 R: 8.941159186249996\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 129 R: 361.7116055970153\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 40 R: 9.365919458249234\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 37 R: 19.61452886011334\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 39 R: 20.6006948193497\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 35 R: 18.901863761313084\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 35 R: 19.37256335276665\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 47 R: 25.928667419883883\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 145 R: 394.63236518781207\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 44 R: 23.46467241639728\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 147 R: 393.0536244796053\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 34 R: 19.024985653555582\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 40 R: 8.407387586250183\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 40 R: 8.852952324649998\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 129 R: 357.5136995272437\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 128 R: 345.67129348421577\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 145 R: 395.0907237231185\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 46 R: 24.84915812887354\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 34 R: 18.783613254713035\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 40 R: 8.902680695702374\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 43 R: 22.592183614937717\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 40 R: 9.012817720859076\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 127 R: 344.43606034836984\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 39 R: 9.273020713873615\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 41 R: 22.096428604507036\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 39 R: 8.854482712807304\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 39 R: 8.621163328087912\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 40 R: 9.358134573970032\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 45 R: 24.20703924088074\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:71000 episode:953 last_R: 387.0875599860215 average_R:260.0767208395737\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 120.883606), ('average_q2', 120.6272), ('average_q_func1_loss', 6.3453129243850706), ('average_q_func2_loss', 6.400332727432251), ('n_updates', 61001), ('average_entropy', -3.084336), ('temperature', 0.04877656698226929)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:72000 episode:970 last_R: 9.519924919640218 average_R:220.90921009258727\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 121.15003), ('average_q2', 121.017166), ('average_q_func1_loss', 10.053233544826508), ('average_q_func2_loss', 9.272740001678466), ('n_updates', 62001), ('average_entropy', -2.9284043), ('temperature', 0.04827461391687393)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:73000 episode:979 last_R: 14.539935701941857 average_R:198.28585273188807\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 121.92431), ('average_q2', 122.32359), ('average_q_func1_loss', 9.143065702915191), ('average_q_func2_loss', 9.40999662399292), ('n_updates', 63001), ('average_entropy', -2.8190444), ('temperature', 0.04863087832927704)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:74000 episode:984 last_R: 504.99117690335913 average_R:191.18904188794625\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.54119), ('average_q2', 122.23479), ('average_q_func1_loss', 8.170697875022888), ('average_q_func2_loss', 8.297574760913848), ('n_updates', 64001), ('average_entropy', -2.9278948), ('temperature', 0.04876336082816124)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:75000 episode:994 last_R: 43.64445252854424 average_R:193.89578574163946\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.93949), ('average_q2', 123.109985), ('average_q_func1_loss', 8.743790230751038), ('average_q_func2_loss', 8.843002891540527), ('n_updates', 65001), ('average_entropy', -2.9866068), ('temperature', 0.049122344702482224)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 154 R: 288.6794336168329\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 57 R: 39.98350611487507\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 150 R: 357.47383202098985\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 57 R: 38.827122832784475\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 148 R: 354.24784818777175\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 150 R: 370.2220960455712\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 150 R: 366.62014828167844\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 73 R: 42.10966168565313\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 76 R: 45.49394645319241\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 151 R: 372.23379154964886\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 149 R: 356.0862957359397\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 68 R: 61.22945911593634\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 80 R: 49.19303688281127\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 77 R: 45.964111310593246\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 154 R: 387.86646680302704\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 58 R: 41.01854855755767\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 117 R: 299.1453166814448\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 72 R: 41.74948681808867\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 61 R: 43.990936675852474\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 151 R: 373.68556666616155\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 149 R: 355.4038125647552\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 73 R: 42.11169227137982\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 77 R: 46.52178213094651\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 149 R: 354.82950444914485\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 149 R: 370.46080192758063\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 149 R: 371.0923015813242\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 71 R: 64.62295456740213\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 75 R: 44.6129855785827\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 72 R: 41.66028471353622\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 150 R: 354.2485146013095\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:76000 episode:1000 last_R: 272.226361200731 average_R:193.58038078953948\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.63302), ('average_q2', 122.82715), ('average_q_func1_loss', 9.247452249526978), ('average_q_func2_loss', 9.104103889465332), ('n_updates', 66001), ('average_entropy', -2.8625908), ('temperature', 0.050765324383974075)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:77000 episode:1008 last_R: 361.25869743662173 average_R:189.73564657275142\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.00918), ('average_q2', 125.251595), ('average_q_func1_loss', 7.412632479667663), ('average_q_func2_loss', 7.803500349521637), ('n_updates', 67001), ('average_entropy', -2.928506), ('temperature', 0.05053982511162758)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:78000 episode:1016 last_R: 212.34949155931218 average_R:187.41829972810916\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.7599), ('average_q2', 122.637505), ('average_q_func1_loss', 6.287445404529572), ('average_q_func2_loss', 6.521423077583313), ('n_updates', 68001), ('average_entropy', -3.303816), ('temperature', 0.04950086399912834)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:79000 episode:1023 last_R: 379.83652553386355 average_R:190.791161018886\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.67208), ('average_q2', 123.62409), ('average_q_func1_loss', 6.5411346554756165), ('average_q_func2_loss', 6.404621601104736), ('n_updates', 69001), ('average_entropy', -3.2416294), ('temperature', 0.04908096045255661)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:80000 episode:1031 last_R: 233.51739885648965 average_R:202.5295901707087\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 121.51975), ('average_q2', 121.386185), ('average_q_func1_loss', 6.483057370185852), ('average_q_func2_loss', 6.565099052190781), ('n_updates', 70001), ('average_entropy', -3.1841202), ('temperature', 0.0493968203663826)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 127 R: 351.4494550663332\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 149 R: 355.6607988803916\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 130 R: 363.18256405417554\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 152 R: 391.75313615912665\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 148 R: 378.1900415771775\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 136 R: 251.97335124680703\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 150 R: 375.92917222866583\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 136 R: 239.09993003594948\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 149 R: 381.7454315603567\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 150 R: 382.52202837415484\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 150 R: 379.6871148554533\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 128 R: 357.5252789806065\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 148 R: 373.84581520096253\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 136 R: 236.42532592011648\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 137 R: 241.03512007371728\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 134 R: 237.11764398063343\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 132 R: 236.36006893314894\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 149 R: 375.90587203708264\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 148 R: 360.1551796679446\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 149 R: 382.267690386154\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 149 R: 360.6698716279193\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 149 R: 365.8974583300734\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 137 R: 245.42922979996536\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 127 R: 352.68510253830925\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 129 R: 365.0980895015876\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 151 R: 387.1310535357704\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 149 R: 372.85728716162174\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 148 R: 377.7362062516955\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 135 R: 238.6419843928058\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 128 R: 362.6514186528311\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 326.5095516353058 -> 336.02095736705127\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:81000 episode:1038 last_R: 384.92250078748543 average_R:215.90381424277896\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.27478), ('average_q2', 125.19331), ('average_q_func1_loss', 5.642941496372223), ('average_q_func2_loss', 5.797729876041412), ('n_updates', 71001), ('average_entropy', -2.9587312), ('temperature', 0.050141990184783936)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:82000 episode:1045 last_R: 329.56638660309545 average_R:229.56997703592944\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.68337), ('average_q2', 124.05992), ('average_q_func1_loss', 6.232714807987213), ('average_q_func2_loss', 6.493319082260132), ('n_updates', 72001), ('average_entropy', -2.9714682), ('temperature', 0.05031553655862808)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:83000 episode:1052 last_R: 354.1351750848239 average_R:246.40738049920742\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 124.2536), ('average_q2', 124.25777), ('average_q_func1_loss', 5.931856141090393), ('average_q_func2_loss', 6.0508647584915165), ('n_updates', 73001), ('average_entropy', -3.0779932), ('temperature', 0.05011314898729324)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:84000 episode:1059 last_R: 349.91564904275833 average_R:253.18016685916118\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.369385), ('average_q2', 123.36316), ('average_q_func1_loss', 5.935249710083008), ('average_q_func2_loss', 5.832498791217804), ('n_updates', 74001), ('average_entropy', -2.8131816), ('temperature', 0.04879748448729515)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:85000 episode:1066 last_R: 214.93145939298162 average_R:273.6251839788414\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.905464), ('average_q2', 123.02214), ('average_q_func1_loss', 5.355314695835114), ('average_q_func2_loss', 5.552821037769317), ('n_updates', 75001), ('average_entropy', -3.0706518), ('temperature', 0.04644804075360298)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 152 R: 375.30739544397863\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 148 R: 353.0650497181567\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 150 R: 356.9041644809497\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 147 R: 252.8520852430182\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 149 R: 357.51329128922913\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 152 R: 253.95037844133566\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 143 R: 335.41528369596955\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 151 R: 257.77066655032513\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 142 R: 341.36061891004044\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 141 R: 319.0918237384055\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 141 R: 333.9183645583389\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 152 R: 374.6166804609733\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 151 R: 373.44648360765126\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 143 R: 336.4223880789388\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 141 R: 332.1472449301849\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 150 R: 253.3839705984924\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 142 R: 347.37102764117265\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 150 R: 371.2516716790227\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 140 R: 319.3514868580762\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 142 R: 319.86682688750193\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 152 R: 251.50648335048783\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 151 R: 245.0911044187831\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 141 R: 326.81617433567015\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 149 R: 252.90960281313957\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 137 R: 356.55569415642253\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 141 R: 321.18724026503503\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 152 R: 340.9189839200792\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 141 R: 334.25788987185615\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 150 R: 371.99950559750516\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 152 R: 347.4052704895318\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:86000 episode:1073 last_R: 363.90545662896056 average_R:290.66870396814835\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.79783), ('average_q2', 126.7046), ('average_q_func1_loss', 5.459226408004761), ('average_q_func2_loss', 5.671788676977157), ('n_updates', 76001), ('average_entropy', -2.9136443), ('temperature', 0.04575935751199722)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:87000 episode:1081 last_R: 381.1492832583654 average_R:316.5844314637385\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.55712), ('average_q2', 122.68415), ('average_q_func1_loss', 5.138621699810028), ('average_q_func2_loss', 5.079738485813141), ('n_updates', 77001), ('average_entropy', -2.7716906), ('temperature', 0.04783505201339722)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:88000 episode:1087 last_R: 380.508929670423 average_R:315.95361452295145\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.86358), ('average_q2', 123.86519), ('average_q_func1_loss', 5.850611345767975), ('average_q_func2_loss', 5.690080218315124), ('n_updates', 78001), ('average_entropy', -3.0693645), ('temperature', 0.05059212073683739)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:89000 episode:1095 last_R: 371.8461879160453 average_R:324.7727680134986\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.916374), ('average_q2', 123.75477), ('average_q_func1_loss', 4.987769215106964), ('average_q_func2_loss', 4.995242964029313), ('n_updates', 79001), ('average_entropy', -2.8030186), ('temperature', 0.04963453486561775)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:90000 episode:1101 last_R: 247.2923884104201 average_R:326.30400636275664\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.26673), ('average_q2', 122.516884), ('average_q_func1_loss', 5.607533845901489), ('average_q_func2_loss', 5.621132576465607), ('n_updates', 80001), ('average_entropy', -3.0149727), ('temperature', 0.05050103738903999)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 141 R: 238.4522556909616\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 151 R: 379.0400234395825\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 167 R: 295.0644075281427\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 149 R: 250.47996497652576\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 138 R: 382.68018725458325\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 133 R: 374.55858489326374\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 148 R: 245.24425488418584\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 154 R: 275.45430506587684\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 157 R: 398.56449571403203\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 133 R: 375.0522093331502\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 152 R: 378.0663892066977\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 151 R: 379.691343680901\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 134 R: 378.68365189994216\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 135 R: 370.06914603078775\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 138 R: 382.68960618256693\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 135 R: 371.85384795983066\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 152 R: 264.1179527476335\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 149 R: 255.2260431979278\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 143 R: 239.7395807421364\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 144 R: 324.7650153681431\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 151 R: 380.2035565214168\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 153 R: 375.0299436679448\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 146 R: 260.37072926880506\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 140 R: 310.15270919199895\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 137 R: 379.9989985452274\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 139 R: 383.4808436942569\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 151 R: 376.38663148596424\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 133 R: 377.4660266871696\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 146 R: 249.84214229580766\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 135 R: 371.1379278125077\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:91000 episode:1109 last_R: 243.2415132120011 average_R:328.215442988908\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 121.29852), ('average_q2', 121.51325), ('average_q_func1_loss', 5.660505279302597), ('average_q_func2_loss', 5.557162070274353), ('n_updates', 81001), ('average_entropy', -2.9842176), ('temperature', 0.049882762134075165)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:92000 episode:1115 last_R: 257.6112599187481 average_R:331.81560616534057\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 121.91838), ('average_q2', 122.057014), ('average_q_func1_loss', 5.087834376096725), ('average_q_func2_loss', 5.194315881729126), ('n_updates', 82001), ('average_entropy', -2.8559282), ('temperature', 0.04813659191131592)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:93000 episode:1122 last_R: 266.50713286877306 average_R:332.9933118432236\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.713196), ('average_q2', 123.97955), ('average_q_func1_loss', 4.540802646875381), ('average_q_func2_loss', 4.577908688783646), ('n_updates', 83001), ('average_entropy', -2.769294), ('temperature', 0.04669806361198425)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:94000 episode:1129 last_R: 377.8936043477771 average_R:336.12218927542983\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 124.68805), ('average_q2', 124.66568), ('average_q_func1_loss', 5.216662632226944), ('average_q_func2_loss', 5.027954812049866), ('n_updates', 84001), ('average_entropy', -2.9768736), ('temperature', 0.046011023223400116)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:95000 episode:1135 last_R: 352.52314518979574 average_R:338.22776775193415\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 121.09882), ('average_q2', 120.987656), ('average_q_func1_loss', 4.524137527942657), ('average_q_func2_loss', 4.4901178467273715), ('n_updates', 85001), ('average_entropy', -2.7568157), ('temperature', 0.04485511779785156)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 194 R: 360.1411091758097\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 210 R: 418.69096305411784\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 148 R: 375.0699336534126\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 200 R: 426.3234213425764\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 142 R: 385.349084507704\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 147 R: 389.0940421832974\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 182 R: 351.55135458863765\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 188 R: 362.21473202587686\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 147 R: 382.73541390265757\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 173 R: 315.48193486409633\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 146 R: 385.5923366071601\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 151 R: 404.92576547044814\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 146 R: 371.8745814354248\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 131 R: 357.6234996223501\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 175 R: 312.85885378533436\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 131 R: 353.0322960633388\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 198 R: 428.92871000360896\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 147 R: 374.30925598051806\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 147 R: 389.7158054706155\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 168 R: 298.2768148049439\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 128 R: 344.88797048067255\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 147 R: 388.6242825733625\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 138 R: 371.0254939862249\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 144 R: 367.99744861428866\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 145 R: 380.77859512011696\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 201 R: 410.2248191362765\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 206 R: 445.6360905952357\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 146 R: 370.57975857950646\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 130 R: 351.45114709202903\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 146 R: 362.98679485436384\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 336.02095736705127 -> 374.59941031913354\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:96000 episode:1143 last_R: 367.46112172108815 average_R:334.89704244014996\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.96831), ('average_q2', 123.903755), ('average_q_func1_loss', 4.514797279834747), ('average_q_func2_loss', 4.383048152923584), ('n_updates', 86001), ('average_entropy', -2.8149188), ('temperature', 0.0432606041431427)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:97000 episode:1149 last_R: 400.5335234731646 average_R:337.19824243643336\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.37984), ('average_q2', 126.44886), ('average_q_func1_loss', 4.349652601480484), ('average_q_func2_loss', 4.237775245904922), ('n_updates', 87001), ('average_entropy', -2.914783), ('temperature', 0.04464211314916611)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:98000 episode:1155 last_R: 376.1784268459277 average_R:340.02991501610313\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.61681), ('average_q2', 125.87556), ('average_q_func1_loss', 4.43286229968071), ('average_q_func2_loss', 4.135240651369095), ('n_updates', 88001), ('average_entropy', -2.8653212), ('temperature', 0.04532937332987785)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:99000 episode:1162 last_R: 383.72393414622655 average_R:343.8152772727679\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.566765), ('average_q2', 126.61464), ('average_q_func1_loss', 3.946045674085617), ('average_q_func2_loss', 3.7068355762958527), ('n_updates', 89001), ('average_entropy', -2.901268), ('temperature', 0.04535392299294472)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:100000 episode:1169 last_R: 371.8314667460547 average_R:347.67457409306104\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 124.731995), ('average_q2', 124.7424), ('average_q_func1_loss', 3.789105545282364), ('average_q_func2_loss', 3.6626888871192933), ('n_updates', 90001), ('average_entropy', -3.1919453), ('temperature', 0.04494844749569893)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 142 R: 256.6556118850005\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 137 R: 378.89427954982347\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 142 R: 369.877461004122\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 140 R: 365.4675557261587\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 142 R: 383.4029427868245\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 149 R: 278.51940760007295\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 146 R: 266.1804297800843\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 139 R: 364.3158950600434\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 140 R: 269.6327223809663\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 133 R: 364.9445060486953\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 150 R: 285.0383563900613\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 144 R: 260.58005557341215\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 153 R: 425.2046098822981\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 147 R: 266.43943841720125\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 145 R: 383.895084812293\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 141 R: 368.7253679282399\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 140 R: 386.5061938896612\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 143 R: 276.2848850479744\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 145 R: 274.5168038553381\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 141 R: 369.68567633542625\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 136 R: 376.68609409783943\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 149 R: 289.1429907906178\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 142 R: 369.51750427323134\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 139 R: 386.9249804092319\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 152 R: 285.273761988582\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 146 R: 275.8652307043163\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 140 R: 370.99818964331985\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 142 R: 391.47618560731325\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 140 R: 365.6147282163974\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 139 R: 382.28642852198874\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:101000 episode:1176 last_R: 372.595025511196 average_R:347.65203709320286\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.63617), ('average_q2', 125.46042), ('average_q_func1_loss', 3.979832900762558), ('average_q_func2_loss', 3.8013757133483885), ('n_updates', 91001), ('average_entropy', -2.8100278), ('temperature', 0.04301571846008301)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:102000 episode:1183 last_R: 391.8706540163448 average_R:349.58167457774937\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.40203), ('average_q2', 122.6557), ('average_q_func1_loss', 3.9281915581226348), ('average_q_func2_loss', 3.5980893576145174), ('n_updates', 92001), ('average_entropy', -3.0365016), ('temperature', 0.04276607185602188)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:103000 episode:1190 last_R: 240.79109567653543 average_R:348.3700681442489\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.6058), ('average_q2', 123.40058), ('average_q_func1_loss', 3.9689577627182007), ('average_q_func2_loss', 4.0312644433975215), ('n_updates', 93001), ('average_entropy', -3.0933876), ('temperature', 0.041895847767591476)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:104000 episode:1197 last_R: 200.8816665395352 average_R:348.2202560493245\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.572945), ('average_q2', 122.240585), ('average_q_func1_loss', 3.8332138311862947), ('average_q_func2_loss', 3.851237384080887), ('n_updates', 94001), ('average_entropy', -3.0761662), ('temperature', 0.041478294879198074)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:105000 episode:1203 last_R: 156.96356609712763 average_R:346.0380593363523\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.594986), ('average_q2', 122.79432), ('average_q_func1_loss', 4.34410706281662), ('average_q_func2_loss', 3.9274955415725707), ('n_updates', 95001), ('average_entropy', -2.9433465), ('temperature', 0.04007555916905403)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 130 R: 355.3768426241609\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 161 R: 379.68485821244457\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 139 R: 385.22654953742546\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 160 R: 251.3880863283642\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 152 R: 375.50116008749836\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 192 R: 359.0704799058608\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 164 R: 273.12896111259437\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 137 R: 383.4514600194641\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 130 R: 360.63318156437987\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 165 R: 262.87402813592223\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 129 R: 350.5700444174741\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 151 R: 382.0649376778505\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 158 R: 369.0621715802982\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 151 R: 374.7547063998392\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 210 R: 372.67685470470803\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 137 R: 387.82064497345715\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 152 R: 382.023395714268\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 152 R: 378.49372228344396\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 156 R: 380.82757449452913\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 136 R: 384.9962481865486\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 172 R: 319.47651152220675\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 159 R: 378.99878568965465\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 189 R: 348.4894212380786\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 180 R: 329.66445344383555\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 175 R: 324.2766753615892\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 199 R: 361.8684888110556\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 211 R: 379.15106426618\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 137 R: 388.0663813551466\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 135 R: 380.50325079884254\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 195 R: 360.6724380794796\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:106000 episode:1210 last_R: 414.6277088749515 average_R:349.9413942114608\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.64505), ('average_q2', 123.84172), ('average_q_func1_loss', 4.14575968503952), ('average_q_func2_loss', 3.8315903532505033), ('n_updates', 96001), ('average_entropy', -2.9574716), ('temperature', 0.039619144052267075)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:107000 episode:1216 last_R: 379.0964843671726 average_R:348.52598735377507\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.672325), ('average_q2', 123.35203), ('average_q_func1_loss', 3.7297020530700684), ('average_q_func2_loss', 3.5944849479198457), ('n_updates', 97001), ('average_entropy', -3.0467675), ('temperature', 0.03969868645071983)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:108000 episode:1222 last_R: 388.7904972913144 average_R:349.02302696212706\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.22676), ('average_q2', 128.0793), ('average_q_func1_loss', 4.33466274023056), ('average_q_func2_loss', 4.333378365039826), ('n_updates', 98001), ('average_entropy', -2.5775497), ('temperature', 0.04102166369557381)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:109000 episode:1230 last_R: 382.7421424484735 average_R:348.59964522803074\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.12633), ('average_q2', 124.83744), ('average_q_func1_loss', 3.6299187612533568), ('average_q_func2_loss', 3.5033044350147247), ('n_updates', 99001), ('average_entropy', -2.9837544), ('temperature', 0.040885016322135925)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:110000 episode:1236 last_R: 380.97415801851724 average_R:350.55927879332575\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 124.31445), ('average_q2', 124.39586), ('average_q_func1_loss', 4.010192065238953), ('average_q_func2_loss', 3.5577271962165833), ('n_updates', 100001), ('average_entropy', -3.12465), ('temperature', 0.039695121347904205)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 138 R: 393.860372348113\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 138 R: 378.47116837626805\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 149 R: 382.33334659824595\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 143 R: 398.9761886237284\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 134 R: 308.0585493413034\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 139 R: 282.5486898353653\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 150 R: 384.1646677220025\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 144 R: 271.3854052064874\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 153 R: 385.59508990202374\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 155 R: 299.7087506261449\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 151 R: 385.34928718828235\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 135 R: 383.4489469883008\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 139 R: 378.7044066574092\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 139 R: 390.5139499782629\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 134 R: 312.7171969905887\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 137 R: 385.23433346080765\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 135 R: 295.8849236827458\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 146 R: 402.79177512988673\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 102 R: 193.1764640515114\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 147 R: 375.3596123150321\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 136 R: 382.52509090148317\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 137 R: 382.0761912426157\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 153 R: 299.6020826377073\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 155 R: 296.67272347178874\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 155 R: 278.59682404437757\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 151 R: 384.02897011180977\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 166 R: 323.74235237524107\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 151 R: 286.51680524863804\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 138 R: 386.53405988946787\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 145 R: 288.479970104059\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:111000 episode:1242 last_R: 405.9139200263521 average_R:352.0254608311666\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.78692), ('average_q2', 126.91487), ('average_q_func1_loss', 3.824096987247467), ('average_q_func2_loss', 3.9373630809783937), ('n_updates', 101001), ('average_entropy', -2.9402597), ('temperature', 0.041050925850868225)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:112000 episode:1249 last_R: 423.6706526391169 average_R:353.8424961409762\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.66737), ('average_q2', 125.621735), ('average_q_func1_loss', 3.370264683961868), ('average_q_func2_loss', 3.424487577676773), ('n_updates', 102001), ('average_entropy', -2.941547), ('temperature', 0.04077700525522232)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:113000 episode:1256 last_R: 259.89029766611856 average_R:352.25483273828695\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.141045), ('average_q2', 125.97375), ('average_q_func1_loss', 3.8444667184352874), ('average_q_func2_loss', 3.849640451669693), ('n_updates', 103001), ('average_entropy', -3.1164923), ('temperature', 0.038211286067962646)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:114000 episode:1263 last_R: 384.0886691460797 average_R:351.5581117610967\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.570404), ('average_q2', 125.62141), ('average_q_func1_loss', 3.602719762325287), ('average_q_func2_loss', 3.628071949481964), ('n_updates', 104001), ('average_entropy', -3.0537882), ('temperature', 0.03840436413884163)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:115000 episode:1270 last_R: 393.1453340284345 average_R:355.07819235785655\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 124.40661), ('average_q2', 124.55679), ('average_q_func1_loss', 3.0845156574249266), ('average_q_func2_loss', 3.0997548377513886), ('n_updates', 105001), ('average_entropy', -2.869399), ('temperature', 0.038742225617170334)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 150 R: 397.7647155828938\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 149 R: 280.73402259049203\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 142 R: 346.6468193027852\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 140 R: 398.4784266070536\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 139 R: 383.00287589498447\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 150 R: 394.15117751122506\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 167 R: 331.6137036724575\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 151 R: 396.83980324871936\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 136 R: 322.9615372062638\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 153 R: 289.5109812518095\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 134 R: 383.69656287029267\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 140 R: 277.8288962970977\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 131 R: 314.6011854504375\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 168 R: 322.5990611277297\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 151 R: 388.11360354781505\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 136 R: 309.9026822463721\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 150 R: 396.04952823148676\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 104 R: 221.9273288200377\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 149 R: 390.65870490077174\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 150 R: 392.9067226334986\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 152 R: 402.705133442729\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 141 R: 409.01667690777407\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 150 R: 396.4259618216266\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 137 R: 380.1449163777218\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 165 R: 328.7805979967508\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 140 R: 397.9366698138593\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 164 R: 314.8553279406686\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 138 R: 382.2994701558385\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 144 R: 395.79020414005015\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 140 R: 313.9323429779575\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:116000 episode:1277 last_R: 426.0982982040295 average_R:354.07904875056107\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.747314), ('average_q2', 125.832664), ('average_q_func1_loss', 4.113099991083145), ('average_q_func2_loss', 3.9389241659641265), ('n_updates', 106001), ('average_entropy', -2.9277074), ('temperature', 0.03772217780351639)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:117000 episode:1283 last_R: 402.9520989432954 average_R:354.01036562277136\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.48436), ('average_q2', 129.43983), ('average_q_func1_loss', 3.271971750855446), ('average_q_func2_loss', 3.225885372161865), ('n_updates', 107001), ('average_entropy', -2.9473665), ('temperature', 0.035561759024858475)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:118000 episode:1289 last_R: 370.2059991859854 average_R:355.30049939731197\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 124.68605), ('average_q2', 124.80957), ('average_q_func1_loss', 3.4948965215682986), ('average_q_func2_loss', 3.4931415164470674), ('n_updates', 108001), ('average_entropy', -2.9860737), ('temperature', 0.03513382747769356)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:119000 episode:1297 last_R: 394.76856836899333 average_R:358.3540257302367\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.07403), ('average_q2', 123.20812), ('average_q_func1_loss', 3.0945507788658144), ('average_q_func2_loss', 2.8703070175647736), ('n_updates', 109001), ('average_entropy', -3.0911694), ('temperature', 0.03638947010040283)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:120000 episode:1304 last_R: 403.3908875056489 average_R:361.5157393014044\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.161156), ('average_q2', 126.08895), ('average_q_func1_loss', 3.003091285228729), ('average_q_func2_loss', 3.3248129427433013), ('n_updates', 110001), ('average_entropy', -2.9622731), ('temperature', 0.036003243178129196)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 148 R: 398.47209525524465\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 163 R: 308.08331403710844\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 164 R: 318.36198737759656\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 138 R: 396.1746373463794\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 148 R: 396.4071226946466\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 148 R: 398.25751697250155\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 149 R: 397.35723089734233\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 182 R: 344.8888144148379\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 165 R: 286.09068600226317\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 148 R: 395.1260271669447\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 138 R: 381.8502432884349\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 138 R: 396.6496622071813\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 152 R: 297.3788995945053\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 144 R: 270.4671587545764\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 178 R: 340.45470556734455\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 138 R: 385.6396646796951\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 147 R: 393.73567073414563\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 135 R: 375.5904383352553\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 137 R: 383.0812989001648\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 171 R: 336.06096956558576\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 137 R: 388.6115190080324\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 141 R: 404.3215947365549\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 130 R: 366.552127846559\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 176 R: 329.1879461264428\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 156 R: 300.7496472871059\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 148 R: 395.8092842526037\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 138 R: 394.55445406891016\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 161 R: 290.5428454893618\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 138 R: 395.8106305520338\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 148 R: 400.6019307778519\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:121000 episode:1310 last_R: 269.76451965871763 average_R:362.30954720721405\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 125.248566), ('average_q2', 125.19268), ('average_q_func1_loss', 3.008694897890091), ('average_q_func2_loss', 2.9513466894626617), ('n_updates', 111001), ('average_entropy', -2.85941), ('temperature', 0.0350194051861763)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:122000 episode:1316 last_R: 401.9457310998828 average_R:366.15140093904427\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.987816), ('average_q2', 128.00343), ('average_q_func1_loss', 3.0266293334960936), ('average_q_func2_loss', 2.949289570450783), ('n_updates', 112001), ('average_entropy', -2.8532324), ('temperature', 0.03467094898223877)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:123000 episode:1323 last_R: 390.60676471103005 average_R:368.37875032671036\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.62764), ('average_q2', 126.62466), ('average_q_func1_loss', 3.2377351462841033), ('average_q_func2_loss', 3.238492521047592), ('n_updates', 113001), ('average_entropy', -3.321375), ('temperature', 0.03424852713942528)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:124000 episode:1331 last_R: 405.11731079159284 average_R:368.5569039935453\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 126.45849), ('average_q2', 126.52502), ('average_q_func1_loss', 3.300383003950119), ('average_q_func2_loss', 3.1087135744094847), ('n_updates', 114001), ('average_entropy', -3.0309153), ('temperature', 0.033600300550460815)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:125000 episode:1336 last_R: 279.2213488976698 average_R:366.6575280009909\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.2514), ('average_q2', 128.89598), ('average_q_func1_loss', 3.283387280702591), ('average_q_func2_loss', 3.2576371169090272), ('n_updates', 115001), ('average_entropy', -2.916414), ('temperature', 0.03390228748321533)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 148 R: 262.43895784120843\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 145 R: 381.6817746788892\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 130 R: 214.26640165901344\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 145 R: 394.178319721064\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 146 R: 387.9119434919315\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 140 R: 221.8497791650722\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 139 R: 392.89716898254886\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 134 R: 214.93328978845517\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 133 R: 369.4160578242356\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 137 R: 377.06404471057573\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 141 R: 248.2810414169148\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 151 R: 268.781127213645\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 155 R: 277.01845917479136\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 146 R: 258.17028430912774\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 144 R: 386.8947542801095\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 153 R: 269.78369194762615\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 137 R: 378.53036833868873\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 154 R: 275.94421148139514\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 153 R: 271.6364988678795\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 144 R: 387.0874839343659\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 148 R: 400.337151589114\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 146 R: 389.7264934517352\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 144 R: 386.40104984777156\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 137 R: 385.1636817413484\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 137 R: 382.71920712922565\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 137 R: 382.7300661529118\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 137 R: 384.3748236427558\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 145 R: 393.7420240952414\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 152 R: 274.46449422450365\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 144 R: 247.3899343928026\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:126000 episode:1344 last_R: 398.0470539004742 average_R:365.99921354776666\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 123.51345), ('average_q2', 123.57647), ('average_q_func1_loss', 3.4046334397792815), ('average_q_func2_loss', 3.418586392402649), ('n_updates', 116001), ('average_entropy', -2.858226), ('temperature', 0.03292672336101532)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:127000 episode:1351 last_R: 375.79389839175144 average_R:364.20358905456374\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.39776), ('average_q2', 127.38028), ('average_q_func1_loss', 3.464373137950897), ('average_q_func2_loss', 3.6186805403232576), ('n_updates', 117001), ('average_entropy', -3.073369), ('temperature', 0.033557310700416565)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:128000 episode:1356 last_R: 388.31932374706906 average_R:365.42829032306554\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.52147), ('average_q2', 127.17131), ('average_q_func1_loss', 3.449928457736969), ('average_q_func2_loss', 3.4311669945716856), ('n_updates', 118001), ('average_entropy', -3.195649), ('temperature', 0.03375297039747238)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:129000 episode:1364 last_R: 396.6145771283556 average_R:366.0501443170496\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.98889), ('average_q2', 131.17616), ('average_q_func1_loss', 3.1204998183250425), ('average_q_func2_loss', 3.211998872756958), ('n_updates', 119001), ('average_entropy', -3.0660129), ('temperature', 0.03420647606253624)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:130000 episode:1370 last_R: 372.82204276134377 average_R:365.4238726492111\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.16127), ('average_q2', 130.15392), ('average_q_func1_loss', 3.856102274656296), ('average_q_func2_loss', 3.686848638057709), ('n_updates', 120001), ('average_entropy', -2.935104), ('temperature', 0.034408748149871826)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 146 R: 237.27807120897347\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 147 R: 254.10061828353642\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 144 R: 391.75851290319156\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 134 R: 370.9854457662116\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 149 R: 255.74488279684698\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 146 R: 402.8722211874485\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 130 R: 205.53223577888397\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 135 R: 372.90997929422497\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 146 R: 396.6959070668436\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 194 R: 388.9330420058867\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 145 R: 391.942843432202\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 151 R: 271.7967162835716\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 149 R: 268.8482155950537\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 133 R: 221.58065117609908\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 145 R: 395.7007988450201\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 134 R: 371.90560793657687\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 145 R: 392.73041067446127\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 134 R: 370.8413499994858\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 144 R: 391.4314946113557\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 136 R: 382.51841026154653\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 144 R: 391.47964049769155\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 135 R: 242.86201102948147\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 158 R: 276.2381797596024\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 145 R: 261.8553507871665\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 146 R: 397.81827347998643\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 133 R: 363.84597068367106\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 174 R: 338.338839718614\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 132 R: 228.67027784199541\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 134 R: 370.69784939935874\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 134 R: 371.16594426773645\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:131000 episode:1376 last_R: 383.81788859116045 average_R:367.3164029397492\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.53853), ('average_q2', 127.10926), ('average_q_func1_loss', 3.439709256887436), ('average_q_func2_loss', 3.731036673784256), ('n_updates', 121001), ('average_entropy', -3.0532758), ('temperature', 0.03463651239871979)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:132000 episode:1384 last_R: 382.11089185133386 average_R:365.3917400328744\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.31773), ('average_q2', 127.2768), ('average_q_func1_loss', 3.388232684135437), ('average_q_func2_loss', 3.579610970020294), ('n_updates', 122001), ('average_entropy', -3.0870938), ('temperature', 0.03379424661397934)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:133000 episode:1389 last_R: 249.62030517542763 average_R:363.8845225395327\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.62134), ('average_q2', 128.42381), ('average_q_func1_loss', 3.624215874671936), ('average_q_func2_loss', 3.4877205789089203), ('n_updates', 123001), ('average_entropy', -3.0555484), ('temperature', 0.034289609640836716)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:134000 episode:1396 last_R: 392.66551545564886 average_R:367.13778112893897\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.6421), ('average_q2', 128.54314), ('average_q_func1_loss', 3.6765704572200777), ('average_q_func2_loss', 3.734217507839203), ('n_updates', 124001), ('average_entropy', -3.1844091), ('temperature', 0.03502004221081734)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:135000 episode:1404 last_R: 464.60050245115696 average_R:365.03958531278244\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.64702), ('average_q2', 127.84229), ('average_q_func1_loss', 3.139889750480652), ('average_q_func2_loss', 3.2599162459373474), ('n_updates', 125001), ('average_entropy', -3.1534848), ('temperature', 0.033919814974069595)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 144 R: 395.2467823328338\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 163 R: 479.0199571138296\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 146 R: 408.6443460710267\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 134 R: 378.44784633356284\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 151 R: 423.2086239947493\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 123 R: 217.28701570289576\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 149 R: 417.3975139203811\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 142 R: 392.67409110680103\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 132 R: 178.03136448891587\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 142 R: 402.9581328208299\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 146 R: 408.9300832171201\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 136 R: 224.22585920283134\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 135 R: 382.0662840267542\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 122 R: 211.24992265858504\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 174 R: 323.75043595838537\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 136 R: 383.7287138868889\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 147 R: 245.68075448737002\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 228 R: 468.29998643596156\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 146 R: 408.3465926447643\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 115 R: 202.52796991530172\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 160 R: 302.0089921427658\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 116 R: 208.3109412422517\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 144 R: 394.34483364591586\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 141 R: 393.15461074637784\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 144 R: 394.7023704431378\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 139 R: 392.6596284298164\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 118 R: 209.743425643163\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 151 R: 425.921046685022\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 224 R: 489.94848381325204\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 221 R: 367.53961772137984\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:136000 episode:1410 last_R: 401.9582278865951 average_R:362.5339699620504\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.7278), ('average_q2', 129.62846), ('average_q_func1_loss', 3.035611405968666), ('average_q_func2_loss', 3.1573885798454286), ('n_updates', 126001), ('average_entropy', -2.9843404), ('temperature', 0.0336792878806591)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:137000 episode:1419 last_R: 238.00998353198278 average_R:360.3883945073121\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.9832), ('average_q2', 130.79692), ('average_q_func1_loss', 3.5248428308963775), ('average_q_func2_loss', 3.3781481993198397), ('n_updates', 127001), ('average_entropy', -3.169085), ('temperature', 0.033104974776506424)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:138000 episode:1424 last_R: 395.82992655438017 average_R:359.3273879419605\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.36018), ('average_q2', 128.36836), ('average_q_func1_loss', 2.789672292470932), ('average_q_func2_loss', 2.782663571834564), ('n_updates', 128001), ('average_entropy', -3.051071), ('temperature', 0.03349597007036209)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:139000 episode:1431 last_R: 249.89940245690377 average_R:361.9402659045778\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.58446), ('average_q2', 128.57957), ('average_q_func1_loss', 3.933354995250702), ('average_q_func2_loss', 3.695113562345505), ('n_updates', 129001), ('average_entropy', -2.9595044), ('temperature', 0.033934660255908966)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:140000 episode:1439 last_R: 385.1096995399879 average_R:363.0264604902555\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.30444), ('average_q2', 127.90416), ('average_q_func1_loss', 2.8420016288757326), ('average_q_func2_loss', 2.688931342959404), ('n_updates', 130001), ('average_entropy', -2.966671), ('temperature', 0.03364962339401245)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 141 R: 394.4874291876705\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 159 R: 317.1931070133199\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 145 R: 398.23207456118683\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 141 R: 395.4581882014352\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 143 R: 397.85415876594277\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 131 R: 238.86471781338253\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 150 R: 424.9500818807312\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 145 R: 401.0545862861754\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 137 R: 253.40491587388755\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 140 R: 392.93642516195195\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 146 R: 402.41894427284484\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 141 R: 394.37481884592665\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 140 R: 254.7064661790466\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 143 R: 394.8660299503996\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 144 R: 396.1915181558957\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 139 R: 272.7535170757807\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 134 R: 237.34789544397063\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 141 R: 394.3154218236837\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 144 R: 398.5622353354676\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 159 R: 346.5448288834861\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 130 R: 223.00538348663386\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 140 R: 388.1058643188446\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 146 R: 405.07040038963737\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 131 R: 238.63633798920313\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 143 R: 400.77099711975893\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 144 R: 402.5463505172788\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 147 R: 403.71172369611065\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 143 R: 400.0396118288719\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 145 R: 399.87458262202233\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 145 R: 401.50925318055374\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:141000 episode:1445 last_R: 262.07243995349404 average_R:362.81451172237524\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.19917), ('average_q2', 130.43799), ('average_q_func1_loss', 3.183857184648514), ('average_q_func2_loss', 2.926114621758461), ('n_updates', 131001), ('average_entropy', -3.0140815), ('temperature', 0.033891838043928146)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:142000 episode:1452 last_R: 289.833594914445 average_R:365.6898683996696\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 131.34521), ('average_q2', 131.476), ('average_q_func1_loss', 2.728919231891632), ('average_q_func2_loss', 2.603513450622559), ('n_updates', 132001), ('average_entropy', -2.9608078), ('temperature', 0.03246168792247772)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:143000 episode:1459 last_R: 396.2743297621024 average_R:369.29890156931725\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 133.52542), ('average_q2', 133.53903), ('average_q_func1_loss', 3.0895365154743195), ('average_q_func2_loss', 3.0397459363937376), ('n_updates', 133001), ('average_entropy', -2.8742402), ('temperature', 0.03303738310933113)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:144000 episode:1466 last_R: 325.54862304529274 average_R:365.94274263305414\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.16937), ('average_q2', 129.31297), ('average_q_func1_loss', 3.606686742305756), ('average_q_func2_loss', 3.6642149126529695), ('n_updates', 134001), ('average_entropy', -2.8682625), ('temperature', 0.033141475170850754)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:145000 episode:1472 last_R: 448.2137979108195 average_R:363.52678965876976\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.26639), ('average_q2', 130.1317), ('average_q_func1_loss', 3.1645715606212614), ('average_q_func2_loss', 3.1238716822862624), ('n_updates', 135001), ('average_entropy', -2.9932604), ('temperature', 0.03280836343765259)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 152 R: 328.3360181431909\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 155 R: 433.8944108197661\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 144 R: 392.4998312429252\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 144 R: 388.02262722692535\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 143 R: 405.0429072468014\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 128 R: 223.325869787547\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 144 R: 404.1387971225553\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 145 R: 392.7046326100205\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 145 R: 407.64648477438266\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 125 R: 221.43163276839172\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 143 R: 390.46159924540973\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 174 R: 474.2087487117612\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 147 R: 414.0515350416081\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 125 R: 219.42263941305444\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 146 R: 410.64000761423887\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 108 R: 188.5373812881163\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 145 R: 408.7378446033032\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 144 R: 403.9685445095699\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 129 R: 227.15772021634237\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 144 R: 388.4095770688334\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 144 R: 387.40394646702094\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 124 R: 316.38497318541306\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 146 R: 414.05233334511644\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 145 R: 406.4633469497836\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 146 R: 395.43174707399396\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 144 R: 389.16254356663853\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 146 R: 397.0987594909769\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 144 R: 390.63828514623594\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 147 R: 413.7920395365639\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 145 R: 405.14372380623723\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:146000 episode:1480 last_R: 416.0388260845434 average_R:366.15215803748805\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 131.667), ('average_q2', 131.53925), ('average_q_func1_loss', 2.9513308823108675), ('average_q_func2_loss', 3.1799733352661135), ('n_updates', 136001), ('average_entropy', -3.2030053), ('temperature', 0.031906984746456146)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:147000 episode:1486 last_R: 402.42862302665395 average_R:364.8427865819993\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.32709), ('average_q2', 132.45047), ('average_q_func1_loss', 2.877785556316376), ('average_q_func2_loss', 2.7101782512664796), ('n_updates', 137001), ('average_entropy', -2.7909458), ('temperature', 0.030729392543435097)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:148000 episode:1492 last_R: 225.4432871300426 average_R:365.1566628165585\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 133.40068), ('average_q2', 133.40263), ('average_q_func1_loss', 2.8639204239845277), ('average_q_func2_loss', 3.267762253880501), ('n_updates', 138001), ('average_entropy', -2.7951064), ('temperature', 0.03178882226347923)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:149000 episode:1499 last_R: 401.2282079998073 average_R:365.81982096233094\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.94415), ('average_q2', 129.2015), ('average_q_func1_loss', 2.626532483100891), ('average_q_func2_loss', 2.7310443234443667), ('n_updates', 139001), ('average_entropy', -2.96925), ('temperature', 0.03156279772520065)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:150000 episode:1506 last_R: 382.97486948445857 average_R:367.574023896445\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.15472), ('average_q2', 129.14294), ('average_q_func1_loss', 3.2561815083026886), ('average_q_func2_loss', 3.484640417098999), ('n_updates', 140001), ('average_entropy', -3.0013032), ('temperature', 0.032060716301202774)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 155 R: 221.79957619684026\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 142 R: 395.90967623305386\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 154 R: 221.45887787222412\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 142 R: 390.3305187625702\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 146 R: 410.63111829039786\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 211 R: 353.30674156292685\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 161 R: 253.00762638803505\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 152 R: 212.61372055250672\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 143 R: 394.57454188258293\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 150 R: 440.9756205143851\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 138 R: 391.62720456420095\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 138 R: 390.63580344095885\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 139 R: 393.0571802417091\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 153 R: 220.63291428628946\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 207 R: 414.14865520961536\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 142 R: 393.2864515397693\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 143 R: 393.9749741252317\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 212 R: 342.4631632057466\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 147 R: 413.41394842440286\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 144 R: 404.560939658803\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 141 R: 392.7455057757317\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 136 R: 387.3284896702924\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 228 R: 412.57919798279084\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 143 R: 396.00745143198804\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 157 R: 467.62545925838816\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 149 R: 430.87263858459437\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 140 R: 395.6924801123604\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 143 R: 399.9187813452136\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 201 R: 360.6665382907344\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 141 R: 389.90496543795297\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:151000 episode:1513 last_R: 233.06257584974895 average_R:365.9946240191341\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.31146), ('average_q2', 132.44115), ('average_q_func1_loss', 2.7702831482887267), ('average_q_func2_loss', 2.7072096741199494), ('n_updates', 141001), ('average_entropy', -3.05913), ('temperature', 0.03089873492717743)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:152000 episode:1520 last_R: 401.5522244052967 average_R:368.2063890492052\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.87514), ('average_q2', 129.78052), ('average_q_func1_loss', 2.68737114071846), ('average_q_func2_loss', 2.946988288164139), ('n_updates', 142001), ('average_entropy', -3.1438792), ('temperature', 0.03128298372030258)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:153000 episode:1525 last_R: 413.75005154544573 average_R:369.76860342616425\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.04068), ('average_q2', 131.90509), ('average_q_func1_loss', 2.787298676967621), ('average_q_func2_loss', 2.821734064221382), ('n_updates', 143001), ('average_entropy', -2.8743582), ('temperature', 0.031951017677783966)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:154000 episode:1533 last_R: 156.4325274101407 average_R:364.6504821136649\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.926445), ('average_q2', 127.74033), ('average_q_func1_loss', 2.4403341776132583), ('average_q_func2_loss', 2.387085643410683), ('n_updates', 144001), ('average_entropy', -2.8622117), ('temperature', 0.032043881714344025)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:155000 episode:1540 last_R: 392.72843981740414 average_R:365.05600838372374\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.23712), ('average_q2', 132.35825), ('average_q_func1_loss', 2.5730197119712828), ('average_q_func2_loss', 2.693130419254303), ('n_updates', 145001), ('average_entropy', -2.8541594), ('temperature', 0.031910575926303864)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 139 R: 391.14829550444836\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 144 R: 404.2111196510841\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 114 R: 203.31906371522822\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 139 R: 395.36718779315845\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 181 R: 294.0155801132539\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 126 R: 234.9164534416318\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 149 R: 420.0486861858652\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 143 R: 404.740935924519\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 119 R: 222.2868302312121\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 144 R: 407.4396447723897\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 228 R: 428.24378808032355\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 145 R: 407.4679555336273\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 177 R: 333.8429553739592\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 145 R: 407.3203480122493\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 138 R: 389.2731162599395\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 140 R: 400.4546457235295\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 144 R: 406.3705323358622\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 145 R: 408.7864667878122\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 140 R: 395.7772339625032\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 144 R: 407.6797625876761\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 144 R: 409.7199086574843\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 121 R: 224.05438852155362\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 205 R: 459.1396864849594\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 128 R: 241.4329684790702\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 194 R: 394.22722543289996\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 218 R: 415.3831847931354\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 144 R: 406.1576231933392\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 264 R: 510.2780594369149\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 143 R: 409.5084823370016\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 146 R: 408.22023388586393\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 374.59941031913354 -> 374.69441210708317\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:156000 episode:1546 last_R: 392.5871433383854 average_R:366.62593064803923\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.40692), ('average_q2', 128.4361), ('average_q_func1_loss', 3.5001335167884826), ('average_q_func2_loss', 3.34573033452034), ('n_updates', 146001), ('average_entropy', -2.8848052), ('temperature', 0.031443651765584946)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:157000 episode:1552 last_R: 378.6242164537228 average_R:364.88964889601544\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.57466), ('average_q2', 132.52782), ('average_q_func1_loss', 3.2664976072311402), ('average_q_func2_loss', 3.4190750789642332), ('n_updates', 147001), ('average_entropy', -3.046158), ('temperature', 0.03055080585181713)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:158000 episode:1558 last_R: 416.33698037886575 average_R:364.4575593668123\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 133.15804), ('average_q2', 133.04466), ('average_q_func1_loss', 2.569359192252159), ('average_q_func2_loss', 2.8256822848320007), ('n_updates', 148001), ('average_entropy', -2.92081), ('temperature', 0.03110356815159321)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:159000 episode:1566 last_R: 420.15690527043427 average_R:366.14877826192145\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.75027), ('average_q2', 129.58607), ('average_q_func1_loss', 3.0777959299087523), ('average_q_func2_loss', 3.182695000171661), ('n_updates', 149001), ('average_entropy', -3.0621874), ('temperature', 0.031451910734176636)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:160000 episode:1572 last_R: 236.19452236169295 average_R:365.57355538404596\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.1832), ('average_q2', 129.95709), ('average_q_func1_loss', 3.406732716560364), ('average_q_func2_loss', 3.4972707921266557), ('n_updates', 150001), ('average_entropy', -2.9848585), ('temperature', 0.03128320351243019)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 125 R: 187.87159894318592\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 147 R: 403.05018127443833\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 150 R: 432.6767850167249\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 126 R: 191.5299302070945\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 155 R: 455.96973947312773\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 144 R: 400.6155228319455\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 145 R: 404.4382878537351\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 146 R: 399.24563022017304\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 145 R: 401.80285475665085\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 146 R: 390.42697598924184\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 138 R: 375.16532988912434\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 140 R: 374.6655591135431\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 147 R: 412.9959964777994\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 141 R: 378.5587586870744\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 140 R: 378.75878294442737\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 125 R: 187.38871099117594\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 141 R: 380.58479943999066\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 145 R: 402.03969777110126\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 140 R: 376.64838035593425\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 140 R: 377.4050117839627\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 125 R: 189.00138765235997\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 120 R: 176.07025586600463\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 146 R: 389.9891217035368\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 143 R: 397.3780916731641\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 145 R: 390.76560876147533\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 124 R: 187.26181871732354\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 125 R: 186.38531870627727\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 147 R: 408.8721373013954\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 142 R: 383.4066804674288\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 140 R: 376.9836013708748\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:161000 episode:1580 last_R: 400.8927996031225 average_R:366.3523691148883\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 131.27956), ('average_q2', 131.21411), ('average_q_func1_loss', 2.9375570684671404), ('average_q_func2_loss', 3.0585728931427), ('n_updates', 151001), ('average_entropy', -3.0609086), ('temperature', 0.030724376440048218)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:162000 episode:1585 last_R: 406.4510165193853 average_R:369.83229392647235\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.09709), ('average_q2', 129.12395), ('average_q_func1_loss', 2.9966856515407563), ('average_q_func2_loss', 3.114807661175728), ('n_updates', 152001), ('average_entropy', -3.0175288), ('temperature', 0.03048781491816044)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:163000 episode:1593 last_R: 401.5871237277514 average_R:372.7316915938346\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 131.86484), ('average_q2', 131.73877), ('average_q_func1_loss', 3.051873619556427), ('average_q_func2_loss', 3.4173268496990206), ('n_updates', 153001), ('average_entropy', -3.1391764), ('temperature', 0.02973390929400921)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:164000 episode:1599 last_R: 451.4749229062086 average_R:374.535932612088\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.36958), ('average_q2', 130.53448), ('average_q_func1_loss', 3.1481416350603104), ('average_q_func2_loss', 3.30845056951046), ('n_updates', 154001), ('average_entropy', -2.882675), ('temperature', 0.029601771384477615)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:165000 episode:1605 last_R: 188.1017270974568 average_R:371.54872898161835\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.2872), ('average_q2', 134.97041), ('average_q_func1_loss', 2.451434463262558), ('average_q_func2_loss', 2.424351145029068), ('n_updates', 155001), ('average_entropy', -2.921554), ('temperature', 0.02965972200036049)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 114 R: 164.2591299865041\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 139 R: 388.6746082958902\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 140 R: 391.7311665320714\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 139 R: 389.1390842730717\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 137 R: 388.24369378358045\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 113 R: 165.73286558796298\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 129 R: 169.03236676730256\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 138 R: 383.2912000386109\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 142 R: 404.1940253894809\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 143 R: 406.0431112431139\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 149 R: 441.1123587511914\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 155 R: 454.33087119135104\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 142 R: 405.4591102435116\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 142 R: 406.56310754019046\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 143 R: 397.6906081363655\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 150 R: 438.2471614935398\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 141 R: 402.8780947118337\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 113 R: 157.27686483007386\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 142 R: 399.069604958204\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 139 R: 390.0779974516927\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 116 R: 165.43843274232609\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 143 R: 404.9383241163139\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 113 R: 167.86179785806897\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 121 R: 165.50283757202234\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 139 R: 390.5162168507722\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 140 R: 402.654148378543\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 139 R: 389.572234698125\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 142 R: 397.46367287110763\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 147 R: 435.57843949470936\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 124 R: 163.7647013029352\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:166000 episode:1611 last_R: 315.1401764303525 average_R:371.1603528461962\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 128.90523), ('average_q2', 128.93007), ('average_q_func1_loss', 2.650123438835144), ('average_q_func2_loss', 2.7678903305530547), ('n_updates', 156001), ('average_entropy', -3.0094948), ('temperature', 0.02869117259979248)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:167000 episode:1618 last_R: 425.4660407045623 average_R:373.44600655599294\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 133.74455), ('average_q2', 133.97426), ('average_q_func1_loss', 3.361214997768402), ('average_q_func2_loss', 3.4858449292182923), ('n_updates', 157001), ('average_entropy', -2.952645), ('temperature', 0.02927899919450283)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:168000 episode:1625 last_R: 392.29636902059116 average_R:372.3753276622582\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.48785), ('average_q2', 132.45325), ('average_q_func1_loss', 2.9008615028858187), ('average_q_func2_loss', 2.9981601643562317), ('n_updates', 158001), ('average_entropy', -3.0001693), ('temperature', 0.02993200533092022)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:169000 episode:1630 last_R: 289.74850620937013 average_R:374.3741226127171\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.83488), ('average_q2', 132.72334), ('average_q_func1_loss', 3.4400046449899673), ('average_q_func2_loss', 3.657553714513779), ('n_updates', 159001), ('average_entropy', -2.9870322), ('temperature', 0.030338436365127563)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:170000 episode:1636 last_R: 398.8443307578336 average_R:381.4356807979339\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 131.03117), ('average_q2', 130.72006), ('average_q_func1_loss', 2.7928471970558166), ('average_q_func2_loss', 2.919542702436447), ('n_updates', 160001), ('average_entropy', -2.833094), ('temperature', 0.02964065596461296)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 136 R: 394.68942717298944\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 139 R: 254.2869940672626\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 134 R: 249.0036256954123\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 246 R: 459.0983127006386\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 138 R: 252.86777627007\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 146 R: 273.3911128143076\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 146 R: 274.4650347739889\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 138 R: 288.47228169291407\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 138 R: 395.9562528746362\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 137 R: 287.1899834916811\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 135 R: 389.4594527158933\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 139 R: 279.5508966422087\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 136 R: 391.005102387171\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 137 R: 391.99403046094244\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 135 R: 392.4275194531365\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 138 R: 398.91200731413613\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 133 R: 380.0519229359603\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 143 R: 253.19876495197587\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 140 R: 271.56394470354803\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 139 R: 256.01405966330043\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 137 R: 397.0734911780858\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 135 R: 392.9093368711251\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 151 R: 327.4627280304312\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 126 R: 203.13083462959537\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 152 R: 461.7382161237234\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 138 R: 398.4494774467468\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 142 R: 260.94605618311596\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 139 R: 401.2306401297941\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 135 R: 386.4712541125947\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 279 R: 492.6760195459377\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:171000 episode:1645 last_R: 374.4361044076182 average_R:380.67068670561656\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 131.18106), ('average_q2', 130.98257), ('average_q_func1_loss', 4.1037489712238315), ('average_q_func2_loss', 4.102832103967667), ('n_updates', 161001), ('average_entropy', -3.0022833), ('temperature', 0.02975461632013321)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:172000 episode:1651 last_R: 407.71911673104063 average_R:381.76773314918285\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.85362), ('average_q2', 135.22028), ('average_q_func1_loss', 3.3970274323225023), ('average_q_func2_loss', 3.610526303052902), ('n_updates', 162001), ('average_entropy', -3.045977), ('temperature', 0.029032938182353973)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:173000 episode:1657 last_R: 376.2583683937068 average_R:377.6108320053354\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.81435), ('average_q2', 133.0746), ('average_q_func1_loss', 3.1482320153713226), ('average_q_func2_loss', 3.353097754716873), ('n_updates', 163001), ('average_entropy', -2.9664934), ('temperature', 0.02863362617790699)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:174000 episode:1665 last_R: 407.2446469779219 average_R:376.7637495886392\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 130.62535), ('average_q2', 130.59357), ('average_q_func1_loss', 2.9801315397024153), ('average_q_func2_loss', 3.038269951939583), ('n_updates', 164001), ('average_entropy', -3.0156832), ('temperature', 0.02846294827759266)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:175000 episode:1671 last_R: 400.67574642373194 average_R:377.4973092256158\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.90448), ('average_q2', 133.00786), ('average_q_func1_loss', 3.0461021292209627), ('average_q_func2_loss', 3.14154527425766), ('n_updates', 165001), ('average_entropy', -2.9712794), ('temperature', 0.02792862243950367)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 170 R: 325.5827905744724\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 140 R: 368.7244446357946\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 143 R: 401.24146181425135\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 140 R: 371.67739096773107\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 143 R: 251.02512924915962\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 163 R: 332.2939268623718\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 155 R: 458.53924119631847\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 158 R: 312.41959805470657\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 143 R: 402.1609184739585\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 164 R: 334.50539147337344\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 143 R: 403.57059995826177\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 160 R: 330.39882024575695\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 143 R: 401.0928726945231\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 143 R: 402.5384157479059\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 143 R: 384.39422851512563\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 140 R: 368.0807769451592\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 140 R: 370.4384320928302\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 147 R: 284.31912816082195\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 159 R: 337.74396298024766\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 141 R: 377.9008899368029\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 141 R: 375.3166803925869\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 162 R: 328.9618439008478\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 161 R: 337.39497323438735\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 139 R: 369.57561373683234\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 143 R: 401.57605393065523\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 157 R: 326.6726220725626\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 141 R: 374.00358245211737\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 140 R: 376.305600281852\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 131 R: 250.28749045416484\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 161 R: 320.55157378976077\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:176000 episode:1679 last_R: 390.3288316241166 average_R:377.32104936964754\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.88757), ('average_q2', 129.88182), ('average_q_func1_loss', 2.9690456557273865), ('average_q_func2_loss', 3.002409396767616), ('n_updates', 166001), ('average_entropy', -2.985058), ('temperature', 0.02611733227968216)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:177000 episode:1686 last_R: 247.95114071464468 average_R:373.29507499772086\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.95497), ('average_q2', 132.9466), ('average_q_func1_loss', 2.618005896806717), ('average_q_func2_loss', 2.6023422557115556), ('n_updates', 167001), ('average_entropy', -3.1817515), ('temperature', 0.026034217327833176)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:178000 episode:1693 last_R: 417.229733132271 average_R:370.28946493732724\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.2471), ('average_q2', 133.94455), ('average_q_func1_loss', 3.022299424409866), ('average_q_func2_loss', 3.0535732102394104), ('n_updates', 168001), ('average_entropy', -3.1854079), ('temperature', 0.02678612247109413)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:179000 episode:1699 last_R: 424.44069891697075 average_R:370.02194210128147\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 133.3336), ('average_q2', 133.1995), ('average_q_func1_loss', 2.8247749614715576), ('average_q_func2_loss', 2.9089139860868456), ('n_updates', 169001), ('average_entropy', -2.805618), ('temperature', 0.026927804574370384)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:180000 episode:1707 last_R: 212.845465375129 average_R:368.86966468886203\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 129.3101), ('average_q2', 129.35883), ('average_q_func1_loss', 2.718321130871773), ('average_q_func2_loss', 2.6559742420911787), ('n_updates', 170001), ('average_entropy', -2.941643), ('temperature', 0.0265815369784832)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 122 R: 240.99963808271755\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 145 R: 410.793114081191\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 146 R: 417.8696882351768\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 147 R: 408.69453825606024\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 130 R: 254.11325509651127\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 145 R: 393.33884662026077\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 154 R: 417.3385259195293\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 146 R: 409.1163719156484\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 147 R: 406.2881270780845\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 141 R: 264.30125959270276\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 143 R: 393.80698442825815\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 122 R: 243.27086334133088\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 146 R: 388.5034267050969\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 146 R: 419.740493608887\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 144 R: 395.77356245761683\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 150 R: 414.23910482087376\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 126 R: 248.59479203119128\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 130 R: 269.90984186725535\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 146 R: 409.24011157913463\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 146 R: 410.45028474082636\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 151 R: 411.72325562392786\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 145 R: 397.72202823552436\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 124 R: 243.82061744415\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 150 R: 415.1139673025549\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 146 R: 412.5845464353843\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 145 R: 415.23541003075775\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 159 R: 463.72004532310564\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 145 R: 405.888325091928\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 147 R: 422.5660669827474\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 144 R: 409.87387767716905\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:181000 episode:1713 last_R: 433.255561683715 average_R:368.3030296269581\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.09174), ('average_q2', 133.93544), ('average_q_func1_loss', 3.315458461046219), ('average_q_func2_loss', 3.3351338613033295), ('n_updates', 171001), ('average_entropy', -3.0895019), ('temperature', 0.026761775836348534)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:182000 episode:1720 last_R: 413.03332802341555 average_R:365.297294566214\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.42317), ('average_q2', 132.40033), ('average_q_func1_loss', 2.7968148940801623), ('average_q_func2_loss', 2.794631994962692), ('n_updates', 172001), ('average_entropy', -3.1276422), ('temperature', 0.02627067267894745)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:183000 episode:1728 last_R: 399.7913175875348 average_R:363.066132722857\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.49469), ('average_q2', 134.25034), ('average_q_func1_loss', 2.260695347189903), ('average_q_func2_loss', 2.2322503006458283), ('n_updates', 173001), ('average_entropy', -2.8692427), ('temperature', 0.027067016810178757)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:184000 episode:1734 last_R: 247.35400114807177 average_R:363.29715804765254\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.31361), ('average_q2', 132.4012), ('average_q_func1_loss', 2.639766819477081), ('average_q_func2_loss', 2.7850469726324083), ('n_updates', 174001), ('average_entropy', -3.136733), ('temperature', 0.027048762887716293)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:185000 episode:1742 last_R: 247.4204224830368 average_R:359.17553245926763\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 133.57545), ('average_q2', 133.38843), ('average_q_func1_loss', 2.571600821018219), ('average_q_func2_loss', 2.505224162340164), ('n_updates', 175001), ('average_entropy', -2.975396), ('temperature', 0.028093475848436356)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 146 R: 385.23963734347893\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 146 R: 402.5638192029986\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 143 R: 271.260158672937\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 137 R: 248.9296220376748\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 151 R: 422.46243035773927\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 143 R: 268.7602597531282\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 145 R: 383.79620233214166\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 150 R: 411.7356867946895\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 146 R: 400.7239380141257\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 146 R: 414.4195466143588\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 130 R: 223.86385906048605\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 152 R: 287.51149605854215\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 145 R: 394.79568485715026\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 145 R: 400.55452827051545\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 138 R: 250.45551900703504\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 148 R: 405.4895940128462\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 144 R: 380.6481860333707\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 141 R: 382.4768880273053\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 145 R: 420.71887580368576\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 144 R: 394.56364725780406\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 143 R: 267.1097976054639\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 146 R: 265.10441227126455\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 141 R: 260.89233312282886\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 150 R: 284.27865433029524\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 135 R: 236.39899003293604\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 152 R: 440.7799567147028\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 147 R: 407.1197269916845\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 141 R: 382.21780860296644\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 145 R: 385.05915073804624\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 146 R: 402.29093529644945\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:186000 episode:1748 last_R: 273.8860750225327 average_R:358.7735687754593\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 131.54192), ('average_q2', 131.53806), ('average_q_func1_loss', 2.9231795507669447), ('average_q_func2_loss', 3.0592677867412568), ('n_updates', 176001), ('average_entropy', -2.723774), ('temperature', 0.02829790860414505)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:187000 episode:1756 last_R: 409.75734122635254 average_R:356.72733799865773\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.23868), ('average_q2', 136.04965), ('average_q_func1_loss', 2.962100220322609), ('average_q_func2_loss', 2.8984839314222337), ('n_updates', 177001), ('average_entropy', -3.1455348), ('temperature', 0.028590982779860497)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:188000 episode:1763 last_R: 420.47838305396516 average_R:359.1815110242476\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 132.66475), ('average_q2', 132.60907), ('average_q_func1_loss', 2.2704133659601213), ('average_q_func2_loss', 2.2876263654232023), ('n_updates', 178001), ('average_entropy', -2.8742177), ('temperature', 0.02878013253211975)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:189000 episode:1769 last_R: 275.3458629883071 average_R:358.88434471254345\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.72427), ('average_q2', 135.76157), ('average_q_func1_loss', 2.7309253615140916), ('average_q_func2_loss', 2.648127893209457), ('n_updates', 179001), ('average_entropy', -3.0925848), ('temperature', 0.028882555663585663)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:190000 episode:1777 last_R: 421.1648473240863 average_R:358.11931692934854\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.15244), ('average_q2', 136.12619), ('average_q_func1_loss', 3.392705979347229), ('average_q_func2_loss', 3.5924385631084443), ('n_updates', 180001), ('average_entropy', -3.0614805), ('temperature', 0.02865368127822876)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 151 R: 420.11740656067315\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 123 R: 209.0213240937293\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 116 R: 184.44447393916204\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 156 R: 460.6622541588617\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 114 R: 180.03497744474322\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 120 R: 192.89410808146022\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 141 R: 401.3016084839828\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 148 R: 383.3896804986459\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 147 R: 425.2178435880512\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 146 R: 372.6526688153416\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 151 R: 415.5180098952232\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 147 R: 374.05748999018715\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 151 R: 408.4781042581198\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 153 R: 422.6298164166867\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 150 R: 415.6546753655471\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 141 R: 399.8417086223732\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 151 R: 415.8164540551112\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 149 R: 411.56749370384114\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 118 R: 197.8970395454579\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 148 R: 383.7379287508264\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 146 R: 369.0136435091003\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 152 R: 419.51672373411355\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 141 R: 398.0685312651957\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 123 R: 207.27240003512037\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 142 R: 399.8075980565176\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 121 R: 204.5414739102365\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 118 R: 192.58935717369897\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 149 R: 416.96348390331957\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 168 R: 454.0637916973635\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 120 R: 201.91265169359994\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:191000 episode:1784 last_R: 404.2469506394928 average_R:359.7822363226139\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 133.08931), ('average_q2', 133.10118), ('average_q_func1_loss', 2.876683601140976), ('average_q_func2_loss', 3.165531902909279), ('n_updates', 181001), ('average_entropy', -3.0441387), ('temperature', 0.02842874638736248)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:192000 episode:1790 last_R: 251.51495259250603 average_R:360.41646375079324\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.06851), ('average_q2', 135.01874), ('average_q_func1_loss', 2.850521430969238), ('average_q_func2_loss', 3.086188519001007), ('n_updates', 182001), ('average_entropy', -2.8732843), ('temperature', 0.028152192011475563)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:193000 episode:1797 last_R: 412.6877512491272 average_R:359.41948784595274\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.15688), ('average_q2', 135.24463), ('average_q_func1_loss', 2.6793923163414), ('average_q_func2_loss', 2.6975353837013243), ('n_updates', 183001), ('average_entropy', -3.0168238), ('temperature', 0.02845449559390545)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:194000 episode:1804 last_R: 401.25808787001864 average_R:365.6289914408182\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.95485), ('average_q2', 136.63654), ('average_q_func1_loss', 3.5585640513896943), ('average_q_func2_loss', 3.922297487258911), ('n_updates', 184001), ('average_entropy', -2.923098), ('temperature', 0.029269732534885406)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:195000 episode:1811 last_R: 321.12575289872296 average_R:366.56549968287305\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.1587), ('average_q2', 134.25511), ('average_q_func1_loss', 2.8239586663246157), ('average_q_func2_loss', 3.078362276554108), ('n_updates', 185001), ('average_entropy', -2.9860575), ('temperature', 0.029330557212233543)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 152 R: 275.1075840648507\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 144 R: 401.6567146927879\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 142 R: 397.6303755143973\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 169 R: 343.9589465206527\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 149 R: 420.40374287250665\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 138 R: 397.26354639406657\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 137 R: 234.61129379297637\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 153 R: 430.687270813603\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 135 R: 387.2682037112504\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 151 R: 422.6150131453846\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 169 R: 353.51974907618734\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 152 R: 426.9559194103642\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 122 R: 203.22831572296093\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 162 R: 316.4052465210838\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 137 R: 402.0667683614831\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 148 R: 415.64750385475236\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 136 R: 391.09641385191145\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 136 R: 390.82705377455807\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 146 R: 406.5510407614044\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 144 R: 431.9059726715482\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 139 R: 403.7070344210761\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 145 R: 439.0203423354715\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 147 R: 405.5261170761739\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 149 R: 411.7684278572148\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 144 R: 401.8503431513813\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 143 R: 399.5166054461985\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 143 R: 425.81660675199873\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 165 R: 331.56866500701307\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 149 R: 422.25928433360014\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 167 R: 347.2613503914491\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 374.69441210708317 -> 381.2567150766769\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:196000 episode:1817 last_R: 514.9948572425 average_R:371.85394409760806\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.9304), ('average_q2', 135.72757), ('average_q_func1_loss', 2.5663836228847505), ('average_q_func2_loss', 2.69680517077446), ('n_updates', 186001), ('average_entropy', -2.9083862), ('temperature', 0.029688440263271332)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:197000 episode:1824 last_R: 435.50631271042624 average_R:372.1327092019444\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.02908), ('average_q2', 135.04272), ('average_q_func1_loss', 3.4272224390506745), ('average_q_func2_loss', 3.7131598126888274), ('n_updates', 187001), ('average_entropy', -2.9255066), ('temperature', 0.029456770047545433)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:198000 episode:1830 last_R: 417.7434560765726 average_R:371.86898691179107\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.95108), ('average_q2', 137.178), ('average_q_func1_loss', 3.019062513709068), ('average_q_func2_loss', 3.2189270770549774), ('n_updates', 188001), ('average_entropy', -2.8732038), ('temperature', 0.027783134952187538)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:199000 episode:1838 last_R: 423.78272068748726 average_R:364.4080105478833\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.55627), ('average_q2', 134.67334), ('average_q_func1_loss', 3.150293217301369), ('average_q_func2_loss', 3.007710781097412), ('n_updates', 189001), ('average_entropy', -2.9436865), ('temperature', 0.029493024572730064)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:200000 episode:1845 last_R: 409.02624746463323 average_R:366.5347631641754\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.18556), ('average_q2', 137.17078), ('average_q_func1_loss', 2.5513299143314363), ('average_q_func2_loss', 2.7128352117538452), ('n_updates', 190001), ('average_entropy', -3.119161), ('temperature', 0.02901582606136799)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 150 R: 461.4732785670091\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 129 R: 218.80972522320002\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 127 R: 213.5031956372191\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 130 R: 224.51926447354043\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 146 R: 413.83861574864926\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 137 R: 399.1780250316183\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 141 R: 412.8712323353336\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 133 R: 236.99376754669436\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 150 R: 430.3908996876845\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 142 R: 404.39028415793547\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 137 R: 399.8044622699116\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 137 R: 230.87973417261637\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 137 R: 398.59992023037205\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 138 R: 398.9130420201797\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 146 R: 449.29103033222975\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 144 R: 408.1962908659755\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 144 R: 412.23063373827307\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 130 R: 220.66246046432448\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 144 R: 407.5794098028306\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 128 R: 217.58883107549818\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 127 R: 217.17156725056725\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 145 R: 411.8807826873568\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 138 R: 399.8318826365565\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 144 R: 410.335516983594\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 149 R: 427.70852722439935\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 150 R: 427.02760634143203\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 128 R: 217.85723082004043\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 134 R: 229.7670055900828\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 136 R: 395.3105369245956\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 129 R: 215.76243765768731\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:201000 episode:1852 last_R: 271.5602097803045 average_R:371.39425634118896\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.81839), ('average_q2', 134.75972), ('average_q_func1_loss', 2.503936085104942), ('average_q_func2_loss', 2.5799432241916658), ('n_updates', 191001), ('average_entropy', -2.9621015), ('temperature', 0.030642172321677208)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:202000 episode:1859 last_R: 431.8874019515442 average_R:371.08716145985795\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.64372), ('average_q2', 134.58875), ('average_q_func1_loss', 2.7279184758663177), ('average_q_func2_loss', 2.721830833554268), ('n_updates', 192001), ('average_entropy', -2.9489818), ('temperature', 0.03083704598248005)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:203000 episode:1865 last_R: 397.733361181724 average_R:371.42372802260115\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.54181), ('average_q2', 134.8821), ('average_q_func1_loss', 3.5380411565303804), ('average_q_func2_loss', 4.157062747478485), ('n_updates', 193001), ('average_entropy', -2.8669674), ('temperature', 0.030780628323554993)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:204000 episode:1874 last_R: 413.71474825120697 average_R:371.89372793949775\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.76968), ('average_q2', 135.82108), ('average_q_func1_loss', 2.734441993832588), ('average_q_func2_loss', 2.98206344127655), ('n_updates', 194001), ('average_entropy', -2.930233), ('temperature', 0.029105188325047493)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:205000 episode:1880 last_R: 413.35091355790763 average_R:370.39333062978113\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.97882), ('average_q2', 135.82204), ('average_q_func1_loss', 2.3893919438123703), ('average_q_func2_loss', 2.493602575659752), ('n_updates', 195001), ('average_entropy', -2.8678198), ('temperature', 0.029187461361289024)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 140 R: 250.4262339348289\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 162 R: 307.68266663198256\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 120 R: 210.24101999597823\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 148 R: 406.58994557169893\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 154 R: 463.7869928376681\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 146 R: 413.2195859307717\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 139 R: 397.7050633495226\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 145 R: 405.40940183006376\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 143 R: 412.7171740680492\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 144 R: 417.131182343179\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 152 R: 431.7711554020045\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 149 R: 414.96890123819566\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 142 R: 401.9281933210188\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 144 R: 402.2152852114764\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 150 R: 422.8501162091602\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 125 R: 216.04724744967754\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 153 R: 433.77461618647123\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 147 R: 412.93330555288946\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 141 R: 412.6503950027418\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 136 R: 240.625988038732\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 154 R: 461.5616482376308\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 147 R: 413.8465888628512\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 143 R: 402.1096657708671\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 144 R: 405.25017726820323\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 152 R: 432.5761945509428\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 152 R: 427.9895049784311\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 142 R: 414.0157124971858\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 153 R: 437.06091234919364\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 122 R: 212.7692919944575\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 144 R: 403.46207723869486\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 381.2567150766769 -> 382.84387479515226\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:206000 episode:1887 last_R: 414.0666062551039 average_R:369.6616836949083\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.42596), ('average_q2', 137.60863), ('average_q_func1_loss', 2.6941987365484237), ('average_q_func2_loss', 2.7980703204870223), ('n_updates', 196001), ('average_entropy', -2.8552382), ('temperature', 0.029174726456403732)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:207000 episode:1894 last_R: 421.24207515753085 average_R:367.8701241473357\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.58504), ('average_q2', 135.39238), ('average_q_func1_loss', 2.646151708960533), ('average_q_func2_loss', 2.5938343632221224), ('n_updates', 197001), ('average_entropy', -2.7731812), ('temperature', 0.028077490627765656)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:208000 episode:1902 last_R: 201.58691861796754 average_R:362.53610818116044\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.69597), ('average_q2', 135.71713), ('average_q_func1_loss', 2.7257107746601106), ('average_q_func2_loss', 2.6742826533317565), ('n_updates', 198001), ('average_entropy', -3.1476853), ('temperature', 0.028241554275155067)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:209000 episode:1909 last_R: 412.6865829436847 average_R:365.4212668366399\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.21397), ('average_q2', 138.10504), ('average_q_func1_loss', 2.8268335843086243), ('average_q_func2_loss', 2.9330668878555297), ('n_updates', 199001), ('average_entropy', -2.965143), ('temperature', 0.02934226579964161)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:210000 episode:1916 last_R: 418.5623368431978 average_R:363.78071595320654\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.35138), ('average_q2', 136.349), ('average_q_func1_loss', 2.618466727733612), ('average_q_func2_loss', 2.6019130718708037), ('n_updates', 200001), ('average_entropy', -2.9197328), ('temperature', 0.028825057670474052)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 138 R: 403.1465983628734\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 138 R: 401.87634358977834\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 138 R: 401.9402389054936\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 145 R: 406.04003215318727\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 138 R: 405.1822112415713\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 147 R: 446.4088570520225\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 133 R: 269.92204708754775\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 145 R: 274.8720220403426\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 146 R: 440.3870406405597\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 123 R: 258.293912643504\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 150 R: 421.07932555918154\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 132 R: 263.51389553938566\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 129 R: 280.64966228976436\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 138 R: 402.2548171684366\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 147 R: 444.13240134268295\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 146 R: 436.2950279900236\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 143 R: 428.99838012619443\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 147 R: 416.39692316732356\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 145 R: 409.0797899551703\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 146 R: 399.51564820658155\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 144 R: 407.10119088490393\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 149 R: 422.7417438341086\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 137 R: 400.1768677186229\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 144 R: 260.22025163475803\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 135 R: 271.89896583965407\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 128 R: 224.25690130834332\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 146 R: 410.8748321455199\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 139 R: 403.72516347225354\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 147 R: 419.0815824054598\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 147 R: 416.04176145619044\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:211000 episode:1923 last_R: 392.27279642230855 average_R:366.3251447556293\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.62576), ('average_q2', 134.83577), ('average_q_func1_loss', 2.974195083975792), ('average_q_func2_loss', 3.0704545331001283), ('n_updates', 201001), ('average_entropy', -2.8379877), ('temperature', 0.02749027870595455)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:212000 episode:1930 last_R: 410.1317544402634 average_R:365.64393290875415\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.49423), ('average_q2', 135.6515), ('average_q_func1_loss', 2.93647188603878), ('average_q_func2_loss', 3.214649555683136), ('n_updates', 202001), ('average_entropy', -2.9444137), ('temperature', 0.02841595932841301)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:213000 episode:1935 last_R: 266.9845310428513 average_R:368.547025262704\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.06212), ('average_q2', 135.32176), ('average_q_func1_loss', 2.849033413529396), ('average_q_func2_loss', 2.838881729245186), ('n_updates', 203001), ('average_entropy', -3.0669935), ('temperature', 0.028279408812522888)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:214000 episode:1945 last_R: 259.8634497645369 average_R:369.99392326879797\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.19951), ('average_q2', 139.45891), ('average_q_func1_loss', 2.7375042003393175), ('average_q_func2_loss', 2.914807147979736), ('n_updates', 204001), ('average_entropy', -3.1557548), ('temperature', 0.027356630191206932)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:215000 episode:1950 last_R: 412.9051227372511 average_R:367.00581290576986\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.651), ('average_q2', 137.81071), ('average_q_func1_loss', 2.528417972922325), ('average_q_func2_loss', 2.583783769607544), ('n_updates', 205001), ('average_entropy', -3.1517603), ('temperature', 0.027145830914378166)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 138 R: 401.9457607596195\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 141 R: 424.8069566285834\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 245 R: 506.94625075140385\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 123 R: 215.67420191517039\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 141 R: 400.4325878165452\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 122 R: 229.23145915080457\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 137 R: 400.55725807114356\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 141 R: 424.0229104147317\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 120 R: 209.43664434118082\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 144 R: 436.66559097922294\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 138 R: 403.7924202163473\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 138 R: 401.82694389807716\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 138 R: 402.3385788506889\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 137 R: 402.2988803254146\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 138 R: 403.1426159838052\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 121 R: 212.34412553266435\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 151 R: 274.7261340874741\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 138 R: 401.50714669255586\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 138 R: 402.16048861612205\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 137 R: 400.14930520646607\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 141 R: 403.8046405129403\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 140 R: 416.7596709564946\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 191 R: 422.2392519980326\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 141 R: 401.4122469390677\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 145 R: 440.3756010503367\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 120 R: 210.89545946589075\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 147 R: 417.4752399783254\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 150 R: 426.2218670804411\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 146 R: 416.56079713847004\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 138 R: 401.63927334936284\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:216000 episode:1957 last_R: 243.1849474139313 average_R:368.91174494663903\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.09225), ('average_q2', 137.83499), ('average_q_func1_loss', 2.620566154718399), ('average_q_func2_loss', 2.6166709822416307), ('n_updates', 206001), ('average_entropy', -2.9217286), ('temperature', 0.027352286502718925)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:217000 episode:1965 last_R: 199.30357359118676 average_R:365.08589251286946\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.75432), ('average_q2', 137.69159), ('average_q_func1_loss', 3.177819241285324), ('average_q_func2_loss', 3.1617480075359343), ('n_updates', 207001), ('average_entropy', -3.018461), ('temperature', 0.028910795226693153)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:218000 episode:1970 last_R: 463.92097340198814 average_R:365.5752405937935\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.09032), ('average_q2', 137.2586), ('average_q_func1_loss', 2.604787692427635), ('average_q_func2_loss', 2.94881262421608), ('n_updates', 208001), ('average_entropy', -3.1945126), ('temperature', 0.028374318033456802)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:219000 episode:1976 last_R: 412.7522251330064 average_R:368.0647546533015\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.5305), ('average_q2', 134.6329), ('average_q_func1_loss', 3.260026206970215), ('average_q_func2_loss', 3.36621426820755), ('n_updates', 209001), ('average_entropy', -2.9123385), ('temperature', 0.029316838830709457)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:220000 episode:1984 last_R: 426.86310490060544 average_R:370.0845807305515\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.345), ('average_q2', 137.19904), ('average_q_func1_loss', 2.883717169761658), ('average_q_func2_loss', 2.975818548798561), ('n_updates', 210001), ('average_entropy', -2.9587278), ('temperature', 0.029021892696619034)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 140 R: 410.6893864233859\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 142 R: 415.1026379742319\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 137 R: 246.69472810092512\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 137 R: 398.8836948838931\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 141 R: 414.8553512692716\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 138 R: 394.5731826534472\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 141 R: 417.6683543410101\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 140 R: 253.16683697100703\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 139 R: 250.45040135030973\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 137 R: 397.64935025261065\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 136 R: 394.67216005437365\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 141 R: 413.2598399701319\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 139 R: 407.4163173241923\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 141 R: 414.0766088273764\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 137 R: 396.9256123703286\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 137 R: 389.3809234675934\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 139 R: 407.55062068836685\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 135 R: 240.21471027049748\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 138 R: 401.3058641508474\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 136 R: 389.31036206433816\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 134 R: 240.44130023013716\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 152 R: 451.78658163018605\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 136 R: 395.0734601351161\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 137 R: 396.99230796912667\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 141 R: 403.6524221991542\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 142 R: 255.68409142103937\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 140 R: 409.5590431989475\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 136 R: 394.16572754541664\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 136 R: 395.8229677267334\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 146 R: 439.13999066948463\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:221000 episode:1989 last_R: 394.56732719822503 average_R:370.87752063810075\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.93863), ('average_q2', 138.11897), ('average_q_func1_loss', 2.3723768812417982), ('average_q_func2_loss', 2.2444012582302095), ('n_updates', 211001), ('average_entropy', -2.9847085), ('temperature', 0.028502443805336952)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:222000 episode:1997 last_R: 406.3014807551705 average_R:375.85082801097883\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.04903), ('average_q2', 140.33328), ('average_q_func1_loss', 2.741389992237091), ('average_q_func2_loss', 2.6840653121471405), ('n_updates', 212001), ('average_entropy', -2.950334), ('temperature', 0.028512338176369667)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:223000 episode:2002 last_R: 452.5257680784257 average_R:380.37456113717695\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.5613), ('average_q2', 137.69861), ('average_q_func1_loss', 2.36893051981926), ('average_q_func2_loss', 2.8030168795585633), ('n_updates', 213001), ('average_entropy', -3.129592), ('temperature', 0.029542118310928345)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:224000 episode:2008 last_R: 433.4719279623637 average_R:377.445790242158\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.19911), ('average_q2', 136.28632), ('average_q_func1_loss', 2.853052031993866), ('average_q_func2_loss', 2.6741257214546206), ('n_updates', 214001), ('average_entropy', -2.8989692), ('temperature', 0.029151832684874535)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:225000 episode:2016 last_R: 443.8468066887801 average_R:379.4519425540339\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.70398), ('average_q2', 136.6355), ('average_q_func1_loss', 2.533043143749237), ('average_q_func2_loss', 2.4976057744026186), ('n_updates', 215001), ('average_entropy', -2.8927398), ('temperature', 0.02928290143609047)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 223 R: 475.24337892106934\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 150 R: 460.02779928043043\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 138 R: 405.68127697280687\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 205 R: 441.74765860823936\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 146 R: 415.92319248325504\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 189 R: 412.9955620415016\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 140 R: 399.7119498326794\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 191 R: 413.7601630329669\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 202 R: 448.22195400335784\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 138 R: 402.67218573832247\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 137 R: 403.87534624088175\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 138 R: 391.2980918609111\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 153 R: 456.3050010326189\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 179 R: 370.9592367020722\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 144 R: 428.1628864105029\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 145 R: 431.4251028340896\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 148 R: 450.58428497399296\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 151 R: 448.2353979995395\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 199 R: 454.22597533389325\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 138 R: 393.5751097415023\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 196 R: 446.949590769499\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 138 R: 393.739963484605\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 144 R: 408.9168873243695\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 135 R: 385.3017841020176\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 136 R: 385.73240434454834\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 219 R: 476.34811116541715\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 169 R: 345.1921765308984\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 137 R: 401.1110225137021\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 139 R: 404.6786426033775\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 201 R: 455.0559791316994\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 382.84387479515226 -> 420.2552705338256\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:226000 episode:2021 last_R: 215.19694971550868 average_R:377.65160950765573\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.88428), ('average_q2', 135.1849), ('average_q_func1_loss', 2.9251136714220047), ('average_q_func2_loss', 3.076138380765915), ('n_updates', 216001), ('average_entropy', -3.244901), ('temperature', 0.029250385239720345)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:227000 episode:2030 last_R: 426.32634319038044 average_R:377.45252192310517\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.7424), ('average_q2', 135.75346), ('average_q_func1_loss', 2.1661408585309982), ('average_q_func2_loss', 2.2701106572151186), ('n_updates', 217001), ('average_entropy', -2.7986913), ('temperature', 0.029396124184131622)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:228000 episode:2037 last_R: 410.8237882622731 average_R:378.1679724050056\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.63475), ('average_q2', 136.54976), ('average_q_func1_loss', 2.769669778943062), ('average_q_func2_loss', 2.9232927227020262), ('n_updates', 218001), ('average_entropy', -3.1154954), ('temperature', 0.029060501605272293)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:229000 episode:2042 last_R: 407.86421602686016 average_R:378.8536985969739\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.35965), ('average_q2', 137.6153), ('average_q_func1_loss', 2.6730198967456817), ('average_q_func2_loss', 2.6259098875522615), ('n_updates', 219001), ('average_entropy', -2.8513985), ('temperature', 0.028593260794878006)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:230000 episode:2049 last_R: 406.4808254034998 average_R:387.06730814068453\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 133.92184), ('average_q2', 133.88844), ('average_q_func1_loss', 2.2990210485458373), ('average_q_func2_loss', 2.4772045743465423), ('n_updates', 220001), ('average_entropy', -3.1088746), ('temperature', 0.0285708736628294)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 125 R: 243.59037693076368\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 140 R: 399.08258275288966\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 134 R: 389.09778966747416\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 139 R: 402.70233129462025\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 137 R: 266.42936800441134\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 150 R: 455.4292014919933\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 161 R: 456.18452226303054\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 159 R: 459.38570183390163\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 151 R: 455.87535469417503\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 143 R: 410.16342720827646\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 145 R: 412.12015039720575\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 125 R: 245.30241406898114\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 135 R: 232.63480652177705\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 167 R: 323.128905775896\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 143 R: 407.0547462741839\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 136 R: 395.26078825853307\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 135 R: 394.34930840966416\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 143 R: 414.63033097600544\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 135 R: 392.7091014431952\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 144 R: 413.86189037765155\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 145 R: 440.7030398103788\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 137 R: 401.90637832892776\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 119 R: 230.8317592074129\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 146 R: 259.8761442699823\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 135 R: 393.36266572416105\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 164 R: 462.35764213859557\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 143 R: 406.5225810365576\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 136 R: 393.5222094091108\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 122 R: 237.83386684994016\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 123 R: 238.89263147821694\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:231000 episode:2057 last_R: 422.73466989919234 average_R:387.6659590878998\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.44713), ('average_q2', 135.38426), ('average_q_func1_loss', 2.693731921315193), ('average_q_func2_loss', 2.5027429163455963), ('n_updates', 221001), ('average_entropy', -3.2398164), ('temperature', 0.028602709993720055)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:232000 episode:2062 last_R: 409.1603476846261 average_R:387.39398975182104\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.54166), ('average_q2', 139.53477), ('average_q_func1_loss', 2.4876185345649717), ('average_q_func2_loss', 2.715357374548912), ('n_updates', 222001), ('average_entropy', -2.692807), ('temperature', 0.02882559411227703)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:233000 episode:2070 last_R: 413.68572207644604 average_R:386.92089073557173\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 133.28891), ('average_q2', 133.28697), ('average_q_func1_loss', 2.6961472964286806), ('average_q_func2_loss', 2.74389244556427), ('n_updates', 223001), ('average_entropy', -3.1229162), ('temperature', 0.028620894998311996)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:234000 episode:2076 last_R: 425.08701362420095 average_R:386.3692800304336\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.27328), ('average_q2', 137.28052), ('average_q_func1_loss', 2.6420358347892763), ('average_q_func2_loss', 2.6444091528654097), ('n_updates', 224001), ('average_entropy', -2.9290435), ('temperature', 0.028836145997047424)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:235000 episode:2081 last_R: 363.05488055502934 average_R:386.2232458316911\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.42328), ('average_q2', 139.3344), ('average_q_func1_loss', 2.38913289308548), ('average_q_func2_loss', 2.5059533417224884), ('n_updates', 225001), ('average_entropy', -3.0265894), ('temperature', 0.029247060418128967)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 188 R: 351.09814460890766\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 150 R: 428.3868344460339\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 139 R: 403.5812242818\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 187 R: 379.768805920002\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 142 R: 418.0368517947877\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 183 R: 352.21436465817453\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 193 R: 387.64933911240684\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 143 R: 402.428587041129\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 194 R: 382.7623672592487\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 238 R: 451.7642806338402\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 145 R: 433.0216905566064\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 166 R: 501.22931734216206\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 146 R: 411.53905127502225\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 144 R: 404.77496572756684\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 145 R: 405.5789995043412\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 167 R: 326.6155736597738\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 140 R: 411.2771805285878\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 176 R: 341.91625889801213\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 139 R: 407.5854871989921\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 192 R: 421.4124916141667\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 140 R: 407.88284761336433\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 196 R: 371.25884584097633\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 150 R: 419.17496208862536\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 150 R: 427.6788567630869\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 146 R: 408.91032317623535\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 140 R: 406.0826037982999\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 142 R: 396.1038017476176\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 143 R: 402.5741497837272\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 150 R: 428.59021149935774\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 145 R: 407.6165673561486\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:236000 episode:2089 last_R: 405.89062566011467 average_R:387.221726625577\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.67577), ('average_q2', 135.64513), ('average_q_func1_loss', 3.3211173355579375), ('average_q_func2_loss', 3.1689620637893676), ('n_updates', 226001), ('average_entropy', -2.6181304), ('temperature', 0.03024793229997158)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:237000 episode:2095 last_R: 421.44399183929096 average_R:386.92912934906525\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.23875), ('average_q2', 137.17313), ('average_q_func1_loss', 3.1152689629793167), ('average_q_func2_loss', 3.1414222913980483), ('n_updates', 227001), ('average_entropy', -2.9897904), ('temperature', 0.030321629717946053)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:238000 episode:2101 last_R: 402.4283837124496 average_R:385.6908303929711\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 133.47484), ('average_q2', 133.33844), ('average_q_func1_loss', 2.7007572901248933), ('average_q_func2_loss', 2.9370896303653717), ('n_updates', 228001), ('average_entropy', -3.155758), ('temperature', 0.03002406284213066)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:239000 episode:2107 last_R: 402.7101593695672 average_R:388.50824960975353\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.21312), ('average_q2', 135.44553), ('average_q_func1_loss', 3.030313321352005), ('average_q_func2_loss', 3.189256888628006), ('n_updates', 229001), ('average_entropy', -3.0668159), ('temperature', 0.029646048322319984)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:240000 episode:2115 last_R: 394.3343034273053 average_R:385.83479318005834\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.61128), ('average_q2', 134.3978), ('average_q_func1_loss', 3.2116761469841), ('average_q_func2_loss', 3.183298841714859), ('n_updates', 230001), ('average_entropy', -3.11626), ('temperature', 0.030153872445225716)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 90 R: 144.01270043426032\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 88 R: 143.92930475043232\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 143 R: 406.40299608275905\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 94 R: 154.13471040206824\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 92 R: 151.191376893122\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 147 R: 417.7809723942893\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 146 R: 413.2887161470873\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 147 R: 419.42116729154367\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 140 R: 407.6800916957546\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 140 R: 408.7729438502503\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 86 R: 137.94893605728691\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 149 R: 425.82323017767993\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 85 R: 137.80005110590457\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 142 R: 421.617166307827\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 150 R: 457.082979412701\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 92 R: 151.23976760981782\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 85 R: 137.72210765543502\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 92 R: 150.71579175627562\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 91 R: 147.2879754246461\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 89 R: 143.4958000757741\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 89 R: 145.4944096255275\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 152 R: 430.1480499144917\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 141 R: 411.9239157676013\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 89 R: 143.13365394979346\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 140 R: 407.91841054028566\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 94 R: 153.29451061430947\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 87 R: 140.86944483639482\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 147 R: 419.33915803239427\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 140 R: 408.2208098539736\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 146 R: 408.8396142786656\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:241000 episode:2124 last_R: 415.68476860189577 average_R:380.789576815825\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.01878), ('average_q2', 138.97137), ('average_q_func1_loss', 2.8023268818855285), ('average_q_func2_loss', 2.857884247303009), ('n_updates', 231001), ('average_entropy', -2.8761225), ('temperature', 0.029866736382246017)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:242000 episode:2130 last_R: 410.11465355796685 average_R:381.3607339037876\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.34839), ('average_q2', 137.40489), ('average_q_func1_loss', 2.6820386457443237), ('average_q_func2_loss', 3.0277562630176544), ('n_updates', 232001), ('average_entropy', -3.014286), ('temperature', 0.029318733140826225)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:243000 episode:2135 last_R: 416.0671229247665 average_R:383.78856181364324\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.67075), ('average_q2', 139.74931), ('average_q_func1_loss', 2.4097653079032897), ('average_q_func2_loss', 2.572851164937019), ('n_updates', 233001), ('average_entropy', -2.9692018), ('temperature', 0.0289823766797781)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:244000 episode:2144 last_R: 335.9923384590589 average_R:383.93083101931506\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.84859), ('average_q2', 134.7345), ('average_q_func1_loss', 2.552199451327324), ('average_q_func2_loss', 2.467222501039505), ('n_updates', 234001), ('average_entropy', -3.1535368), ('temperature', 0.029719337821006775)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:245000 episode:2150 last_R: 319.74144136661715 average_R:380.5560833805524\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.28357), ('average_q2', 137.39893), ('average_q_func1_loss', 2.3167739158868788), ('average_q_func2_loss', 2.4073303627967833), ('n_updates', 235001), ('average_entropy', -3.094393), ('temperature', 0.02954932302236557)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 152 R: 424.93211969470036\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 139 R: 407.2911644469441\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 144 R: 398.0759995274485\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 145 R: 401.77661321991746\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 140 R: 402.296987690792\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 161 R: 326.9080153381382\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 144 R: 393.7230277595142\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 144 R: 394.2424832309784\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 141 R: 412.1246857533025\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 97 R: 158.84216041507585\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 161 R: 451.7305502859572\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 150 R: 455.62583300512307\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 158 R: 445.63073657973445\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 185 R: 388.8137961595963\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 171 R: 328.75455138921495\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 162 R: 325.6685091710948\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 161 R: 325.45370482584894\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 146 R: 406.9270550199695\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 175 R: 342.05451761923916\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 140 R: 409.2093649591097\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 192 R: 397.6286861721443\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 143 R: 426.9647217415852\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 140 R: 409.87123991552465\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 140 R: 407.5936689493923\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 185 R: 386.95283246176746\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 144 R: 399.2678057453755\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 139 R: 407.4840983024893\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 160 R: 325.7536513472292\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 167 R: 467.20098853784174\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 140 R: 406.9078661991846\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:246000 episode:2155 last_R: 396.1435555473285 average_R:378.57435726861445\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.60013), ('average_q2', 134.56412), ('average_q_func1_loss', 2.434140962958336), ('average_q_func2_loss', 2.6805489718914033), ('n_updates', 236001), ('average_entropy', -3.2044828), ('temperature', 0.029356161132454872)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:247000 episode:2163 last_R: 418.3600345220017 average_R:382.18272313666336\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.86278), ('average_q2', 139.79712), ('average_q_func1_loss', 2.8456106114387514), ('average_q_func2_loss', 2.9012007719278334), ('n_updates', 237001), ('average_entropy', -3.001313), ('temperature', 0.0294848270714283)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:248000 episode:2169 last_R: 315.05665396659384 average_R:382.25452744863264\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.8979), ('average_q2', 137.00719), ('average_q_func1_loss', 3.87426687836647), ('average_q_func2_loss', 3.8618159466981887), ('n_updates', 238001), ('average_entropy', -2.9018896), ('temperature', 0.02887604758143425)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:249000 episode:2175 last_R: 416.9992088357426 average_R:381.59293557872\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.57816), ('average_q2', 137.36867), ('average_q_func1_loss', 2.720590332150459), ('average_q_func2_loss', 2.9166800612211228), ('n_updates', 239001), ('average_entropy', -2.8460615), ('temperature', 0.028065351769328117)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:250000 episode:2183 last_R: 492.35019139276267 average_R:381.13941955514383\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.37646), ('average_q2', 136.59715), ('average_q_func1_loss', 2.9253100997209547), ('average_q_func2_loss', 2.7066846030950544), ('n_updates', 240001), ('average_entropy', -3.016057), ('temperature', 0.028239622712135315)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 141 R: 402.36705576359225\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 150 R: 424.0263353054124\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 155 R: 450.5531480267919\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 155 R: 259.4680158945146\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 214 R: 420.02679473052706\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 148 R: 417.97834054207647\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 141 R: 403.6963865599052\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 163 R: 277.0470819879729\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 202 R: 406.7374477227153\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 151 R: 429.0134332679269\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 140 R: 401.1479972766349\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 148 R: 417.95743105512327\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 176 R: 330.3768063110005\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 141 R: 402.6763640405018\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 177 R: 344.0522518062635\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 148 R: 408.0965186846288\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 148 R: 421.8148160146349\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 141 R: 404.3970749991062\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 157 R: 469.0645166398861\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 172 R: 326.9131648866509\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 215 R: 429.0240197394444\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 169 R: 295.0220533733516\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 170 R: 318.43654892956334\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 212 R: 417.92775161428585\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 203 R: 410.0049834688745\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 179 R: 348.17364401065885\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 148 R: 410.6353914984285\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 145 R: 420.00543477121084\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 177 R: 339.1063921581\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 160 R: 265.1529626535017\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:251000 episode:2189 last_R: 402.25750507188457 average_R:380.9463130858442\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.39253), ('average_q2', 137.18288), ('average_q_func1_loss', 2.9545122390985488), ('average_q_func2_loss', 2.943871202468872), ('n_updates', 241001), ('average_entropy', -2.890433), ('temperature', 0.02818155102431774)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:252000 episode:2195 last_R: 328.2506930787479 average_R:383.0231833098685\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.61075), ('average_q2', 142.99806), ('average_q_func1_loss', 3.1310334289073944), ('average_q_func2_loss', 3.2287293779850006), ('n_updates', 242001), ('average_entropy', -2.7563365), ('temperature', 0.02725081518292427)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:253000 episode:2203 last_R: 408.26442034636415 average_R:380.53210881356034\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.12193), ('average_q2', 137.15054), ('average_q_func1_loss', 2.321406037211418), ('average_q_func2_loss', 2.566707274913788), ('n_updates', 243001), ('average_entropy', -3.1650484), ('temperature', 0.0276303943246603)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:254000 episode:2208 last_R: 324.4453408390207 average_R:380.7017180238296\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.76059), ('average_q2', 136.74197), ('average_q_func1_loss', 2.761873772740364), ('average_q_func2_loss', 2.898735431432724), ('n_updates', 244001), ('average_entropy', -3.1845088), ('temperature', 0.028018398210406303)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:255000 episode:2215 last_R: 338.5028176011946 average_R:382.9332872040388\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.11737), ('average_q2', 138.73392), ('average_q_func1_loss', 2.6079494255781173), ('average_q_func2_loss', 2.691927888393402), ('n_updates', 245001), ('average_entropy', -2.831535), ('temperature', 0.028197357431054115)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 141 R: 407.87397314192486\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 141 R: 410.6574733032883\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 147 R: 412.33823725852443\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 141 R: 411.2625854232386\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 143 R: 418.3193469331444\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 161 R: 239.74869395963418\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 156 R: 230.2767507812986\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 145 R: 413.3716789182426\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 171 R: 244.03512683511198\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 170 R: 244.46226194133095\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 140 R: 401.49510490007214\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 143 R: 401.6162848325638\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 170 R: 242.5096951199595\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 145 R: 410.28072165887045\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 144 R: 404.4973828154518\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 144 R: 402.88340799306917\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 136 R: 223.8427842186688\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 141 R: 409.05667357329787\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 143 R: 405.6240103290307\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 140 R: 404.84239121072113\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 141 R: 409.2730461361968\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 142 R: 408.56620522484104\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 160 R: 236.99552958195883\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 144 R: 419.4525879162755\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 135 R: 223.29815166115804\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 140 R: 405.85191777531946\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 133 R: 223.75766113007373\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 131 R: 217.71196819680938\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 160 R: 248.1126797471499\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 148 R: 238.89777042503403\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:256000 episode:2222 last_R: 446.4661778413974 average_R:388.74997379406955\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.91965), ('average_q2', 134.89227), ('average_q_func1_loss', 2.4709264314174653), ('average_q_func2_loss', 2.4013201498985293), ('n_updates', 246001), ('average_entropy', -3.0734296), ('temperature', 0.027829337865114212)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:257000 episode:2227 last_R: 339.97081345381645 average_R:390.24241187114416\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.90207), ('average_q2', 137.95187), ('average_q_func1_loss', 2.2114126718044282), ('average_q_func2_loss', 2.2626518321037294), ('n_updates', 247001), ('average_entropy', -3.1703799), ('temperature', 0.028745433315634727)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:258000 episode:2236 last_R: 404.131535551505 average_R:389.4425397255001\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.92177), ('average_q2', 139.07744), ('average_q_func1_loss', 2.9379715752601623), ('average_q_func2_loss', 3.0781188017129897), ('n_updates', 248001), ('average_entropy', -2.9608092), ('temperature', 0.027987098321318626)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:259000 episode:2241 last_R: 322.134870607434 average_R:388.6467190207038\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.97565), ('average_q2', 138.25793), ('average_q_func1_loss', 2.6938839274644852), ('average_q_func2_loss', 2.917216191291809), ('n_updates', 249001), ('average_entropy', -2.8223867), ('temperature', 0.027418674901127815)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:260000 episode:2246 last_R: 325.485834468748 average_R:390.38204280144777\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.44272), ('average_q2', 136.43422), ('average_q_func1_loss', 2.7679304033517838), ('average_q_func2_loss', 2.885180978775024), ('n_updates', 250001), ('average_entropy', -2.9190557), ('temperature', 0.02773222327232361)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 157 R: 265.00843377744803\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 161 R: 282.78873863136374\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 139 R: 401.649004987397\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 149 R: 421.8824452944074\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 139 R: 405.2337111953114\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 152 R: 419.16499921304734\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 150 R: 423.72000267720597\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 148 R: 416.81887799713763\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 140 R: 407.91494937604483\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 150 R: 421.43555596533287\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 157 R: 269.27526041954565\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 158 R: 266.19866528176124\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 151 R: 251.63572727039994\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 156 R: 266.64167071522024\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 160 R: 282.7831228144892\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 139 R: 405.3326202016544\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 140 R: 410.4804911268177\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 164 R: 288.67367531229553\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 159 R: 275.6243751706562\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 148 R: 414.1014697350924\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 140 R: 407.63425326178543\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 139 R: 406.8073169031589\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 140 R: 405.1287894224091\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 150 R: 424.8678343540582\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 161 R: 284.05555597253857\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 139 R: 402.9064904125025\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 158 R: 275.41042493407616\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 162 R: 284.2750145551787\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 149 R: 420.0962479727631\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 152 R: 460.4239379035331\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:261000 episode:2255 last_R: 407.37602420544454 average_R:392.77792695569946\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.18561), ('average_q2', 140.21893), ('average_q_func1_loss', 2.87632978618145), ('average_q_func2_loss', 3.0274820733070373), ('n_updates', 251001), ('average_entropy', -3.0473242), ('temperature', 0.026520291343331337)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:262000 episode:2260 last_R: 416.56044781865705 average_R:392.8395612463018\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.36327), ('average_q2', 144.41602), ('average_q_func1_loss', 2.5819082939624787), ('average_q_func2_loss', 2.8450562179088594), ('n_updates', 252001), ('average_entropy', -2.9632828), ('temperature', 0.02799443155527115)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:263000 episode:2266 last_R: 404.6166366798062 average_R:395.118808853204\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.6315), ('average_q2', 140.51831), ('average_q_func1_loss', 2.82519625723362), ('average_q_func2_loss', 2.827426974773407), ('n_updates', 253001), ('average_entropy', -3.018048), ('temperature', 0.02854754775762558)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:264000 episode:2272 last_R: 403.6969602836858 average_R:396.8306984538807\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.73463), ('average_q2', 138.8405), ('average_q_func1_loss', 2.726982660293579), ('average_q_func2_loss', 2.701628576517105), ('n_updates', 254001), ('average_entropy', -3.150434), ('temperature', 0.02817314676940441)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:265000 episode:2280 last_R: 472.2935589842743 average_R:395.4647318072662\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.26294), ('average_q2', 137.2768), ('average_q_func1_loss', 3.4037986838817598), ('average_q_func2_loss', 3.3965057796239853), ('n_updates', 255001), ('average_entropy', -3.3518047), ('temperature', 0.026980701833963394)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 145 R: 423.04687694294773\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 148 R: 415.73251242293406\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 148 R: 412.8319171625583\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 141 R: 405.03753708632513\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 150 R: 261.9802482171993\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 141 R: 404.6656972414208\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 148 R: 413.717911195689\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 148 R: 416.8217173091669\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 318 R: 560.9997339102426\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 142 R: 408.6142324108885\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 142 R: 407.961151648699\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 149 R: 443.8447249597963\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 141 R: 404.2292499163801\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 152 R: 451.50100217195103\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 142 R: 407.9643256591909\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 152 R: 433.4356518030938\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 274 R: 518.6443889669109\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 222 R: 426.9717696535202\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 152 R: 256.88072977082794\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 145 R: 401.63755355568685\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 141 R: 402.3387610400777\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 154 R: 434.02230869637214\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 298 R: 533.8336831907769\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 148 R: 410.7381760220144\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 149 R: 255.04179625069233\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 216 R: 419.76251093136017\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 144 R: 400.1677597306549\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 143 R: 244.4441353679757\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 142 R: 409.09427283912544\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 157 R: 471.96050258921497\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:266000 episode:2286 last_R: 413.8469654927061 average_R:395.9749548016558\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.93578), ('average_q2', 138.14748), ('average_q_func1_loss', 2.6647526013851164), ('average_q_func2_loss', 2.854508055448532), ('n_updates', 256001), ('average_entropy', -3.2051175), ('temperature', 0.02796495519578457)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:267000 episode:2293 last_R: 249.9620149810763 average_R:391.08113090637517\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.9304), ('average_q2', 136.86551), ('average_q_func1_loss', 2.600884287953377), ('average_q_func2_loss', 2.71688450217247), ('n_updates', 257001), ('average_entropy', -2.9478333), ('temperature', 0.026693636551499367)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:268000 episode:2299 last_R: 418.6957474967456 average_R:392.99750352215625\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.48866), ('average_q2', 140.38103), ('average_q_func1_loss', 3.213929522037506), ('average_q_func2_loss', 3.428524722456932), ('n_updates', 258001), ('average_entropy', -2.8902457), ('temperature', 0.02738817036151886)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:269000 episode:2304 last_R: 414.4796874324983 average_R:395.05990967941557\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.17648), ('average_q2', 138.08475), ('average_q_func1_loss', 2.4767328661680224), ('average_q_func2_loss', 2.857832731604576), ('n_updates', 259001), ('average_entropy', -2.9137611), ('temperature', 0.026521537452936172)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:270000 episode:2311 last_R: 322.04359289237277 average_R:393.5899123289492\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.19505), ('average_q2', 139.2579), ('average_q_func1_loss', 2.540613917708397), ('average_q_func2_loss', 2.683194839954376), ('n_updates', 260001), ('average_entropy', -3.0759566), ('temperature', 0.026527229696512222)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 147 R: 415.63590686716833\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 165 R: 335.97330202288634\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 151 R: 452.2435389867406\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 150 R: 432.2660548021184\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 140 R: 405.08893884603594\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 150 R: 431.35522367249445\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 148 R: 417.8948616100659\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 147 R: 416.3823163374677\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 178 R: 346.61323069552816\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 140 R: 408.1247337313598\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 136 R: 395.82454570535197\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 140 R: 406.27045564933417\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 161 R: 353.97231683918096\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 178 R: 353.30144960552786\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 145 R: 409.7209069118451\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 147 R: 415.0439896534799\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 185 R: 361.7041560355395\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 164 R: 359.7587160888373\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 138 R: 403.13673318111023\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 164 R: 345.8291977259866\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 177 R: 348.717845304987\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 160 R: 353.18384316679465\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 139 R: 404.43584506828444\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 148 R: 419.2480225197926\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 182 R: 359.53394056159084\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 154 R: 344.33158745374243\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 166 R: 366.37796462401013\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 140 R: 406.5923913312474\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 166 R: 331.3334739696563\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 148 R: 416.60847577215003\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:271000 episode:2318 last_R: 455.40690823429253 average_R:396.4124115461805\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.31163), ('average_q2', 141.50671), ('average_q_func1_loss', 2.488807672858238), ('average_q_func2_loss', 2.653842053413391), ('n_updates', 261001), ('average_entropy', -3.2261012), ('temperature', 0.027663761749863625)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:272000 episode:2324 last_R: 423.1659693481117 average_R:394.52577497914695\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.29369), ('average_q2', 142.2728), ('average_q_func1_loss', 2.324569616317749), ('average_q_func2_loss', 2.2628121161460877), ('n_updates', 262001), ('average_entropy', -2.862567), ('temperature', 0.028368020430207253)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:273000 episode:2332 last_R: 404.3116381532881 average_R:390.23786558885445\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.3266), ('average_q2', 140.23349), ('average_q_func1_loss', 3.450775001645088), ('average_q_func2_loss', 3.4902982634305952), ('n_updates', 263001), ('average_entropy', -2.9519627), ('temperature', 0.027576817199587822)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:274000 episode:2338 last_R: 184.12149969654044 average_R:387.49208714161085\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 134.31676), ('average_q2', 134.46344), ('average_q_func1_loss', 2.823540015220642), ('average_q_func2_loss', 2.9887518846988677), ('n_updates', 264001), ('average_entropy', -3.1618512), ('temperature', 0.02723151445388794)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:275000 episode:2343 last_R: 449.09469525765513 average_R:386.82023575050704\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.46031), ('average_q2', 137.54816), ('average_q_func1_loss', 2.188286625146866), ('average_q_func2_loss', 2.3860744082927705), ('n_updates', 265001), ('average_entropy', -3.170018), ('temperature', 0.026774056255817413)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 144 R: 413.5185688166832\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 136 R: 375.21650079284484\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 144 R: 415.80199740563353\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 133 R: 370.1516353050162\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 134 R: 373.98588785809517\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 144 R: 412.27450806808116\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 119 R: 183.39954751299248\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 134 R: 374.6471342260044\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 145 R: 276.3802132504502\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 145 R: 414.7627197262865\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 144 R: 416.1852683048804\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 145 R: 416.69286272117665\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 144 R: 414.60846254232126\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 145 R: 260.08886275056307\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 140 R: 397.32168588604605\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 145 R: 412.95749298270243\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 144 R: 414.90760780737526\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 165 R: 389.17716848474464\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 143 R: 415.0323141668275\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 144 R: 415.57188579496574\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 143 R: 412.6353279887861\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 121 R: 185.06183312015676\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 144 R: 273.5735158904808\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 144 R: 413.05805254736686\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 135 R: 377.19665785214363\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 144 R: 417.0676001010568\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 134 R: 370.69351029464497\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 124 R: 188.78704628224935\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 150 R: 442.43121502512435\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 121 R: 184.61193224771884\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:276000 episode:2353 last_R: 370.7317367549195 average_R:387.3605437027059\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.76106), ('average_q2', 140.06253), ('average_q_func1_loss', 3.099006564617157), ('average_q_func2_loss', 3.197330869436264), ('n_updates', 266001), ('average_entropy', -3.0617537), ('temperature', 0.027084067463874817)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:277000 episode:2358 last_R: 384.93128843579194 average_R:387.7886649251535\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.79619), ('average_q2', 140.77728), ('average_q_func1_loss', 2.912877086400986), ('average_q_func2_loss', 3.0738437008857726), ('n_updates', 267001), ('average_entropy', -2.883736), ('temperature', 0.02675151266157627)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:278000 episode:2363 last_R: 415.7156862726697 average_R:387.82981805464266\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.34613), ('average_q2', 141.2239), ('average_q_func1_loss', 2.4528384691476823), ('average_q_func2_loss', 2.604291145801544), ('n_updates', 268001), ('average_entropy', -2.985618), ('temperature', 0.027179641649127007)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:279000 episode:2372 last_R: 409.8839376086425 average_R:388.3831027327973\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.9629), ('average_q2', 137.84724), ('average_q_func1_loss', 2.35559823513031), ('average_q_func2_loss', 2.523258522748947), ('n_updates', 269001), ('average_entropy', -3.1467135), ('temperature', 0.0266522616147995)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:280000 episode:2378 last_R: 409.2101565355749 average_R:385.0466785382955\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.00638), ('average_q2', 139.1618), ('average_q_func1_loss', 2.0604012912511824), ('average_q_func2_loss', 2.105603654384613), ('n_updates', 270001), ('average_entropy', -2.8769987), ('temperature', 0.026208441704511642)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 163 R: 310.892952304287\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 168 R: 335.9275169224005\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 141 R: 395.5091433786136\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 150 R: 452.5069414854765\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 144 R: 420.9297023161363\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 142 R: 414.7717235199749\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 143 R: 399.13366317624997\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 160 R: 295.6790245738268\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 142 R: 417.446704956592\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 164 R: 333.0282361664629\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 159 R: 332.61763669855355\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 142 R: 416.1524047793892\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 153 R: 461.28952682900103\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 141 R: 390.92963365405336\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 146 R: 418.6700848451366\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 148 R: 443.8042279828757\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 168 R: 341.09342950998854\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 153 R: 465.3493102903148\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 143 R: 400.5689185372919\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 171 R: 340.9958728189624\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 157 R: 323.9704987849704\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 165 R: 334.5589767855426\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 157 R: 325.04637207082675\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 140 R: 391.9071957823382\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 139 R: 389.0736671133068\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 140 R: 249.0537879379147\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 160 R: 321.9622324123289\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 167 R: 336.64432505858434\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 142 R: 396.15120547068915\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 143 R: 421.9601150123817\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:281000 episode:2385 last_R: 424.8731042704925 average_R:384.9019681951653\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.92284), ('average_q2', 136.82785), ('average_q_func1_loss', 2.3616224193572997), ('average_q_func2_loss', 2.3665668016672132), ('n_updates', 271001), ('average_entropy', -2.9278522), ('temperature', 0.025780089199543)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:282000 episode:2391 last_R: 451.0953371335773 average_R:385.6499217842281\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.25104), ('average_q2', 138.3473), ('average_q_func1_loss', 2.5655019313097), ('average_q_func2_loss', 2.632740272283554), ('n_updates', 272001), ('average_entropy', -3.1424592), ('temperature', 0.025309434160590172)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:283000 episode:2398 last_R: 459.60633075403075 average_R:387.20425458128875\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.67325), ('average_q2', 138.0491), ('average_q_func1_loss', 2.8617872536182403), ('average_q_func2_loss', 3.0263135093450546), ('n_updates', 273001), ('average_entropy', -3.1474054), ('temperature', 0.025820273905992508)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:284000 episode:2406 last_R: 396.3468281813919 average_R:383.89053343437934\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.46542), ('average_q2', 140.45844), ('average_q_func1_loss', 2.152928879261017), ('average_q_func2_loss', 2.2869683599472044), ('n_updates', 274001), ('average_entropy', -3.0504384), ('temperature', 0.026072194799780846)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:285000 episode:2413 last_R: 235.0434964530208 average_R:379.12688331345294\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.27287), ('average_q2', 140.16498), ('average_q_func1_loss', 2.127838976383209), ('average_q_func2_loss', 2.1998597884178164), ('n_updates', 275001), ('average_entropy', -2.9111564), ('temperature', 0.02546551823616028)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 144 R: 282.48136837519843\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 147 R: 438.2762107640722\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 148 R: 446.1158277561454\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 154 R: 307.1268140180985\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 144 R: 425.34858405893806\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 147 R: 415.0708855784951\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 164 R: 353.80356368899504\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 145 R: 427.9099024446007\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 143 R: 417.6813617992132\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 154 R: 311.2772685848591\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 146 R: 411.40106127840835\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 149 R: 297.7571838502388\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 146 R: 412.0077916893912\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 179 R: 396.12433844276677\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 143 R: 417.8522791033785\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 143 R: 421.16819817861034\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 147 R: 402.17070626650724\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 153 R: 313.38561672384384\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 150 R: 299.00509830029694\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 143 R: 416.2124537856274\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 161 R: 335.5895878973551\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 144 R: 419.76615091074353\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 158 R: 323.7812711631027\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 142 R: 415.67359199786506\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 159 R: 451.62111034358105\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 159 R: 307.99695424175457\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 152 R: 311.95784167846006\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 146 R: 410.8530158876512\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 147 R: 407.6856076639224\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 153 R: 315.7535498709426\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:286000 episode:2418 last_R: 419.903862183449 average_R:377.21845916190165\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.22176), ('average_q2', 139.16219), ('average_q_func1_loss', 2.1349730402231217), ('average_q_func2_loss', 2.2867475551366807), ('n_updates', 276001), ('average_entropy', -3.0760498), ('temperature', 0.026243887841701508)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:287000 episode:2426 last_R: 394.30749422278626 average_R:376.14265209041776\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.07286), ('average_q2', 136.99673), ('average_q_func1_loss', 2.6562147092819215), ('average_q_func2_loss', 2.7206667459011076), ('n_updates', 277001), ('average_entropy', -2.8391082), ('temperature', 0.02544950135052204)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:288000 episode:2433 last_R: 412.9155814148407 average_R:378.13346288264603\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.64104), ('average_q2', 141.63237), ('average_q_func1_loss', 2.4209792947769166), ('average_q_func2_loss', 2.380830335021019), ('n_updates', 278001), ('average_entropy', -2.9466004), ('temperature', 0.02575080655515194)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:289000 episode:2439 last_R: 405.3395779141067 average_R:380.147071228594\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.43893), ('average_q2', 138.38046), ('average_q_func1_loss', 2.2868010199069975), ('average_q_func2_loss', 2.3532937854528426), ('n_updates', 279001), ('average_entropy', -2.9326026), ('temperature', 0.02581474743783474)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:290000 episode:2445 last_R: 268.3570950377877 average_R:381.9145020446473\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.66884), ('average_q2', 140.48068), ('average_q_func1_loss', 2.7630036652088164), ('average_q_func2_loss', 2.773467101454735), ('n_updates', 280001), ('average_entropy', -2.906002), ('temperature', 0.02469233237206936)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 141 R: 399.4529855431559\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 147 R: 427.75256516448644\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 145 R: 418.15345121632305\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 145 R: 240.98506010083884\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 148 R: 433.3463620670965\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 144 R: 389.73678068710564\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 143 R: 406.74561334541994\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 152 R: 268.32677486014995\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 144 R: 398.0652758310173\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 145 R: 404.20601601271795\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 144 R: 417.56792143519755\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 142 R: 388.9198145517767\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 146 R: 417.37658261357\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 138 R: 232.15404440233195\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 144 R: 415.76502791305154\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 145 R: 416.9579795919013\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 142 R: 402.23863056242476\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 143 R: 247.25394911424934\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 144 R: 398.1139208312553\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 150 R: 445.9923323995682\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 133 R: 378.8015834222577\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 143 R: 400.0161429486211\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 148 R: 257.69126852593774\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 143 R: 392.98095472141796\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 142 R: 246.26274914762078\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 145 R: 418.24814386129145\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 144 R: 415.2317167368288\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 144 R: 416.70886825887686\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 141 R: 389.8265011822011\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 140 R: 397.2450764388307\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:291000 episode:2452 last_R: 410.90716310997306 average_R:384.1493246352943\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.47552), ('average_q2', 138.49683), ('average_q_func1_loss', 2.5113855600357056), ('average_q_func2_loss', 2.773976256847382), ('n_updates', 281001), ('average_entropy', -2.8856025), ('temperature', 0.025915946811437607)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:292000 episode:2459 last_R: 415.58755161995236 average_R:384.6248662357901\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.48863), ('average_q2', 137.69006), ('average_q_func1_loss', 2.203018350601196), ('average_q_func2_loss', 2.3580772519111632), ('n_updates', 282001), ('average_entropy', -2.9870281), ('temperature', 0.02545784041285515)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:293000 episode:2465 last_R: 394.4517629284078 average_R:380.6012724674191\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.24474), ('average_q2', 144.34952), ('average_q_func1_loss', 2.058239313364029), ('average_q_func2_loss', 2.2017436057329176), ('n_updates', 283001), ('average_entropy', -2.9010565), ('temperature', 0.0248123612254858)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:294000 episode:2471 last_R: 413.79540877687856 average_R:382.4539143713675\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.7399), ('average_q2', 143.51456), ('average_q_func1_loss', 2.7808771473169327), ('average_q_func2_loss', 2.9623850202560424), ('n_updates', 284001), ('average_entropy', -2.779421), ('temperature', 0.024469977244734764)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:295000 episode:2478 last_R: 309.6038697544828 average_R:381.68656077868843\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.83502), ('average_q2', 140.71938), ('average_q_func1_loss', 2.931262105703354), ('average_q_func2_loss', 2.926825052499771), ('n_updates', 285001), ('average_entropy', -3.1022456), ('temperature', 0.02484489232301712)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 143 R: 417.57676625400416\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 140 R: 254.5946611240823\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 153 R: 464.69266871777\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 134 R: 237.43763496718336\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 142 R: 414.97833605757563\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 147 R: 419.03273441462676\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 158 R: 324.77132087301834\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 150 R: 323.2813038791124\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 146 R: 415.2164203151682\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 148 R: 424.469145002602\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 142 R: 293.5430834960982\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 143 R: 290.02136811671323\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 142 R: 416.5380628155984\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 143 R: 407.350460144095\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 141 R: 400.53110324152277\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 143 R: 418.43607909296895\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 158 R: 324.37431276454083\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 148 R: 424.6333547743261\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 152 R: 461.3087272719861\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 148 R: 424.386380360515\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 142 R: 416.49752109596676\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 142 R: 417.37268953986757\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 143 R: 402.5761648444807\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 152 R: 461.53238376976293\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 147 R: 419.6745321062773\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 150 R: 434.993147928749\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 158 R: 323.7601720982038\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 144 R: 419.16413618423013\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 144 R: 420.9433976797606\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 142 R: 414.61244382770167\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:296000 episode:2485 last_R: 412.7516278267904 average_R:380.54186534969847\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.14955), ('average_q2', 138.91547), ('average_q_func1_loss', 2.2194802075624467), ('average_q_func2_loss', 2.245627916455269), ('n_updates', 286001), ('average_entropy', -3.002739), ('temperature', 0.024267073720693588)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:297000 episode:2492 last_R: 256.2717357700757 average_R:377.92380400606953\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.57413), ('average_q2', 141.53984), ('average_q_func1_loss', 2.2499191534519194), ('average_q_func2_loss', 2.67365657389164), ('n_updates', 287001), ('average_entropy', -3.0331547), ('temperature', 0.024321794509887695)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:298000 episode:2497 last_R: 414.5099760155453 average_R:379.67827630518985\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.02066), ('average_q2', 141.21184), ('average_q_func1_loss', 2.0726721453666688), ('average_q_func2_loss', 2.2835451012849806), ('n_updates', 288001), ('average_entropy', -2.9700894), ('temperature', 0.024453146383166313)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:299000 episode:2503 last_R: 415.00901923795476 average_R:382.9365437871809\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.69452), ('average_q2', 138.7381), ('average_q_func1_loss', 2.2099288511276245), ('average_q_func2_loss', 2.3393573528528213), ('n_updates', 289001), ('average_entropy', -3.1165004), ('temperature', 0.02472858875989914)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:300000 episode:2511 last_R: 413.1440113076102 average_R:384.8732711409997\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.9974), ('average_q2', 140.72116), ('average_q_func1_loss', 2.4967803925275804), ('average_q_func2_loss', 2.6419113874435425), ('n_updates', 290001), ('average_entropy', -3.006934), ('temperature', 0.0243870597332716)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 144 R: 416.07966479353155\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 145 R: 419.52287428309137\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 144 R: 416.7176209646357\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 150 R: 442.4522687455632\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 192 R: 414.36761959741506\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 148 R: 422.4503882368783\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 146 R: 409.47824462407385\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 147 R: 409.1764965183797\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 144 R: 404.1643914280198\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 151 R: 431.8520215464325\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 145 R: 418.5389293413024\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 183 R: 416.7038602592688\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 231 R: 479.9323740293294\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 190 R: 407.54233923793964\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 148 R: 417.9156046279599\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 177 R: 392.6238807576444\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 241 R: 497.4047523147081\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 145 R: 407.5523078063124\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 147 R: 421.39972167203274\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 144 R: 413.4018188164168\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 144 R: 404.6107834967957\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 201 R: 451.85544445295596\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 131 R: 351.8058064284252\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 144 R: 417.6527734704841\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 145 R: 410.1257434190593\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 147 R: 420.15376149032494\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 143 R: 402.9479065618225\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 145 R: 418.52855345978384\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 149 R: 441.8215002294342\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 173 R: 383.04664641763196\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:301000 episode:2517 last_R: 430.4957101887642 average_R:389.1141121403079\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.73169), ('average_q2', 139.76816), ('average_q_func1_loss', 2.217666766047478), ('average_q_func2_loss', 2.355903369188309), ('n_updates', 291001), ('average_entropy', -3.1014755), ('temperature', 0.024631459265947342)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:302000 episode:2524 last_R: 419.04305047094175 average_R:391.95912846928485\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.18352), ('average_q2', 139.31142), ('average_q_func1_loss', 2.3305649542808533), ('average_q_func2_loss', 2.086529591679573), ('n_updates', 292001), ('average_entropy', -3.0529966), ('temperature', 0.024905122816562653)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:303000 episode:2531 last_R: 405.91916195325194 average_R:391.6598619537911\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.52892), ('average_q2', 142.6236), ('average_q_func1_loss', 2.2714294278621674), ('average_q_func2_loss', 2.407723830342293), ('n_updates', 293001), ('average_entropy', -3.0720649), ('temperature', 0.024751871824264526)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:304000 episode:2537 last_R: 419.2087983784653 average_R:394.4212871821192\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.55678), ('average_q2', 140.64526), ('average_q_func1_loss', 2.5355627411603927), ('average_q_func2_loss', 2.63106231033802), ('n_updates', 294001), ('average_entropy', -2.9188664), ('temperature', 0.02506466582417488)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:305000 episode:2543 last_R: 415.25297321155097 average_R:390.38958119558464\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.52997), ('average_q2', 142.37354), ('average_q_func1_loss', 2.3240578812360764), ('average_q_func2_loss', 2.2537872552871705), ('n_updates', 295001), ('average_entropy', -2.929039), ('temperature', 0.024938277900218964)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 153 R: 456.80995488294275\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 146 R: 424.1359615187256\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 210 R: 446.9049413591145\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 139 R: 395.8222301288672\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 147 R: 426.21263680238116\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 153 R: 460.39372238333715\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 156 R: 328.58138907958164\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 138 R: 391.37177256022017\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 202 R: 432.87551673746543\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 137 R: 386.61139044563663\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 139 R: 394.24781569480257\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 146 R: 423.09071093925166\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 140 R: 390.501076770714\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 129 R: 350.80420639386085\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 145 R: 421.69620560543746\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 128 R: 346.894558597978\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 130 R: 227.13773129669258\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 140 R: 398.17087025842676\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 145 R: 419.0910367881065\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 156 R: 469.9156322632706\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 127 R: 352.8562753197559\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 131 R: 221.71167192201656\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 201 R: 472.06463651721504\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 139 R: 394.26475503532384\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 137 R: 402.16610930448854\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 138 R: 390.28563369051614\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 146 R: 425.1045254855331\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 146 R: 424.57073763464876\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 138 R: 389.0381354814332\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 127 R: 210.8617618566823\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:306000 episode:2550 last_R: 350.1769735041757 average_R:388.4172826435846\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.73221), ('average_q2', 141.70381), ('average_q_func1_loss', 2.445926412343979), ('average_q_func2_loss', 2.653599611520767), ('n_updates', 296001), ('average_entropy', -2.9009757), ('temperature', 0.024808179587125778)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:307000 episode:2556 last_R: 412.99867773272723 average_R:389.84663026632353\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.97668), ('average_q2', 141.10095), ('average_q_func1_loss', 2.209091650247574), ('average_q_func2_loss', 2.292086121439934), ('n_updates', 297001), ('average_entropy', -3.0762734), ('temperature', 0.025422781705856323)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:308000 episode:2563 last_R: 319.0691162678656 average_R:392.6651871139587\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.30064), ('average_q2', 137.42586), ('average_q_func1_loss', 2.2649408811330796), ('average_q_func2_loss', 2.6104417890310287), ('n_updates', 298001), ('average_entropy', -2.8091896), ('temperature', 0.026111498475074768)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:309000 episode:2570 last_R: 268.0880082637742 average_R:390.9514259410844\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.51202), ('average_q2', 143.37231), ('average_q_func1_loss', 1.985576508641243), ('average_q_func2_loss', 2.104497946500778), ('n_updates', 299001), ('average_entropy', -2.8887117), ('temperature', 0.024832909926772118)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:310000 episode:2574 last_R: 486.4681108696681 average_R:393.6700357860544\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.13841), ('average_q2', 141.13322), ('average_q_func1_loss', 2.4363531005382537), ('average_q_func2_loss', 2.398443316817284), ('n_updates', 300001), ('average_entropy', -2.768107), ('temperature', 0.023889794945716858)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 146 R: 417.0737276893955\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 144 R: 422.80324834121086\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 145 R: 416.89903730129606\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 145 R: 415.7036458043057\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 162 R: 328.31573043121347\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 144 R: 413.96097687147244\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 210 R: 446.31074280264943\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 129 R: 221.3466242850199\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 145 R: 415.7206057452153\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 145 R: 417.30139796671045\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 141 R: 252.61188342965366\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 145 R: 415.63865195419856\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 148 R: 436.8064804864691\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 144 R: 411.26707678824295\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 146 R: 422.91024684878556\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 143 R: 408.08947467421785\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 154 R: 434.97568536741653\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 147 R: 415.39265345676756\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 143 R: 415.71392013519875\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 143 R: 413.9137000546414\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 153 R: 465.8000817193559\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 145 R: 415.4488215162524\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 207 R: 471.0683431587609\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 144 R: 413.16877834831485\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 145 R: 415.53455507152444\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 145 R: 416.6531248570699\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 147 R: 422.1503484259641\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 146 R: 418.6899402570898\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 136 R: 242.2151475255843\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 158 R: 482.8849695245152\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:311000 episode:2583 last_R: 475.52352050139416 average_R:397.41194878913296\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.30913), ('average_q2', 141.1665), ('average_q_func1_loss', 2.5376198041439055), ('average_q_func2_loss', 2.59011000931263), ('n_updates', 301001), ('average_entropy', -2.860969), ('temperature', 0.024072377011179924)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:312000 episode:2588 last_R: 406.138891339064 average_R:396.1448227386664\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.59229), ('average_q2', 144.62842), ('average_q_func1_loss', 2.7558969402313234), ('average_q_func2_loss', 2.646509313583374), ('n_updates', 302001), ('average_entropy', -2.8605337), ('temperature', 0.024243831634521484)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:313000 episode:2593 last_R: 414.06018123002724 average_R:397.6098071557405\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.54822), ('average_q2', 139.614), ('average_q_func1_loss', 2.3377122670412063), ('average_q_func2_loss', 2.3298304933309555), ('n_updates', 303001), ('average_entropy', -2.9862285), ('temperature', 0.02319853939116001)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:314000 episode:2599 last_R: 408.98563557883153 average_R:397.8866967919972\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.72285), ('average_q2', 138.73946), ('average_q_func1_loss', 2.465745684504509), ('average_q_func2_loss', 2.5360338348150253), ('n_updates', 304001), ('average_entropy', -3.0893965), ('temperature', 0.023337461054325104)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:315000 episode:2607 last_R: 422.2015572278626 average_R:394.4002016889738\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.92778), ('average_q2', 142.10056), ('average_q_func1_loss', 2.2262064057588575), ('average_q_func2_loss', 2.246347873210907), ('n_updates', 305001), ('average_entropy', -2.8702583), ('temperature', 0.024185694754123688)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 142 R: 411.65225662658054\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 145 R: 417.9635474430844\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 147 R: 421.4161156786059\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 142 R: 410.5447118352235\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 144 R: 413.2557016373118\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 143 R: 413.91169183344584\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 142 R: 276.1251431735252\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 146 R: 428.27621618257245\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 139 R: 274.35131340952637\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 143 R: 413.4039603865627\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 144 R: 413.98300537409415\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 131 R: 247.69825283441435\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 144 R: 416.2555111160221\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 148 R: 424.33569242994434\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 147 R: 438.9607054803688\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 146 R: 420.9515395815389\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 141 R: 277.62680269395133\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 146 R: 416.9311863616992\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 130 R: 244.79667825706161\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 144 R: 416.9844912134388\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 147 R: 425.44152281620734\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 128 R: 237.11712307271878\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 144 R: 416.47117042195356\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 160 R: 482.9937379361581\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 132 R: 247.0378505178567\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 131 R: 246.6947950402249\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 155 R: 463.809040713241\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 144 R: 414.60063444536195\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 142 R: 412.92435741377307\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 129 R: 239.51608392569727\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:316000 episode:2612 last_R: 417.4995876486081 average_R:392.3756274147652\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.12013), ('average_q2', 138.89328), ('average_q_func1_loss', 2.860066755414009), ('average_q_func2_loss', 2.994050731062889), ('n_updates', 306001), ('average_entropy', -3.0468285), ('temperature', 0.024210814386606216)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:317000 episode:2619 last_R: 422.9045311656492 average_R:391.3144249130245\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.62738), ('average_q2', 140.81274), ('average_q_func1_loss', 2.3404959148168563), ('average_q_func2_loss', 2.3005489319562913), ('n_updates', 307001), ('average_entropy', -2.7778556), ('temperature', 0.023297972977161407)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:318000 episode:2625 last_R: 411.2990030848425 average_R:392.40364513286016\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.5264), ('average_q2', 139.42282), ('average_q_func1_loss', 2.360163138508797), ('average_q_func2_loss', 2.480309019088745), ('n_updates', 308001), ('average_entropy', -3.0470207), ('temperature', 0.023807665333151817)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:319000 episode:2633 last_R: 404.72996634306867 average_R:396.46810133570546\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.89864), ('average_q2', 142.51309), ('average_q_func1_loss', 2.5811602544784544), ('average_q_func2_loss', 2.7606377029418945), ('n_updates', 309001), ('average_entropy', -3.0013812), ('temperature', 0.02470559999346733)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:320000 episode:2640 last_R: 402.1714909643916 average_R:393.5902069309442\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.335), ('average_q2', 138.52596), ('average_q_func1_loss', 2.022393190860748), ('average_q_func2_loss', 2.040779872536659), ('n_updates', 310001), ('average_entropy', -2.9673214), ('temperature', 0.024874527007341385)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 172 R: 366.77530574809293\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 134 R: 400.54081095408657\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 136 R: 254.20279860337925\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 131 R: 384.7775299307986\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 128 R: 235.28745707376507\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 135 R: 253.02999461821443\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 135 R: 404.8747597982174\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 126 R: 232.26132653727782\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 128 R: 236.091428902935\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 140 R: 408.41124549338815\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 139 R: 407.2837349619328\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 131 R: 382.0272703120032\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 140 R: 408.53744573164977\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 134 R: 390.6268884578808\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 130 R: 243.84976557095945\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 133 R: 387.37821803896804\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 138 R: 412.2826670306992\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 129 R: 240.38676890615793\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 136 R: 253.6462185653332\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 159 R: 493.67873658371724\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 146 R: 313.2816673828679\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 143 R: 411.25709650041847\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 137 R: 410.3599598132272\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 138 R: 268.3635753507811\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 134 R: 391.28820730683015\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 128 R: 234.60559430126128\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 132 R: 391.0975176432638\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 136 R: 396.1980217703789\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 131 R: 383.82083660822934\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 119 R: 210.79418518599263\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:321000 episode:2645 last_R: 401.0890156949045 average_R:397.4077502637788\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.84738), ('average_q2', 141.8447), ('average_q_func1_loss', 2.075998472571373), ('average_q_func2_loss', 2.1938670921325683), ('n_updates', 311001), ('average_entropy', -2.9306562), ('temperature', 0.024251671507954597)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:322000 episode:2654 last_R: 389.84095605949744 average_R:394.36096313493965\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.7314), ('average_q2', 140.6626), ('average_q_func1_loss', 2.4231715887784957), ('average_q_func2_loss', 2.6146401530504226), ('n_updates', 312001), ('average_entropy', -3.0022788), ('temperature', 0.024357140064239502)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:323000 episode:2660 last_R: 250.17482654958278 average_R:393.1151146881003\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.1605), ('average_q2', 141.0208), ('average_q_func1_loss', 2.2534087336063386), ('average_q_func2_loss', 2.212347569465637), ('n_updates', 313001), ('average_entropy', -2.9915886), ('temperature', 0.02323831617832184)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:324000 episode:2667 last_R: 414.0034932607617 average_R:395.22213032614883\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.45445), ('average_q2', 142.60667), ('average_q_func1_loss', 2.2946952068805695), ('average_q_func2_loss', 2.6413327193260194), ('n_updates', 314001), ('average_entropy', -2.8330822), ('temperature', 0.023512134328484535)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:325000 episode:2674 last_R: 239.35791206177493 average_R:392.5489624339019\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.83443), ('average_q2', 139.67836), ('average_q_func1_loss', 1.9580955743789672), ('average_q_func2_loss', 1.9941988646984101), ('n_updates', 315001), ('average_entropy', -2.7915537), ('temperature', 0.02374296449124813)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 145 R: 439.5237850382548\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 151 R: 309.95528517827205\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 137 R: 397.9674607453929\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 150 R: 310.003810678351\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 138 R: 402.819908820173\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 139 R: 400.9821807719455\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 141 R: 412.9633038823987\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 143 R: 420.2690114795064\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 154 R: 322.7834951383112\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 139 R: 404.31828004576084\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 137 R: 398.0279580406986\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 151 R: 467.11095516316556\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 139 R: 403.819832515963\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 145 R: 436.6867232752534\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 138 R: 410.96337403076353\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 138 R: 400.91907439696496\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 140 R: 412.7082042356113\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 140 R: 410.12970317757373\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 139 R: 399.8202566717293\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 141 R: 410.0232822806159\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 139 R: 403.160048864852\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 141 R: 411.82999763805265\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 141 R: 414.11967307959316\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 129 R: 232.98777304356682\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 207 R: 495.4647013142447\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 128 R: 237.53375465158518\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 140 R: 406.69568360262645\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 157 R: 483.54363708610117\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 146 R: 293.35012209413065\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 155 R: 487.4640514385643\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:326000 episode:2680 last_R: 392.29455726264985 average_R:394.92099619443024\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.30075), ('average_q2', 138.34885), ('average_q_func1_loss', 2.3784428143501284), ('average_q_func2_loss', 2.3566589015722275), ('n_updates', 316001), ('average_entropy', -3.126566), ('temperature', 0.023609764873981476)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:327000 episode:2685 last_R: 394.4821187400993 average_R:395.40119057524475\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.0673), ('average_q2', 137.78796), ('average_q_func1_loss', 2.357647801041603), ('average_q_func2_loss', 2.5812032294273375), ('n_updates', 317001), ('average_entropy', -2.917714), ('temperature', 0.023921821266412735)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:328000 episode:2694 last_R: 388.43100346328583 average_R:397.3351817691746\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.33571), ('average_q2', 139.41129), ('average_q_func1_loss', 2.3823267084360125), ('average_q_func2_loss', 2.401671179533005), ('n_updates', 318001), ('average_entropy', -3.013476), ('temperature', 0.023605898022651672)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:329000 episode:2699 last_R: 254.13777383112105 average_R:394.45257314097944\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.45442), ('average_q2', 140.49854), ('average_q_func1_loss', 1.9887877517938615), ('average_q_func2_loss', 2.1443455851078035), ('n_updates', 319001), ('average_entropy', -3.1305315), ('temperature', 0.023718738928437233)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:330000 episode:2705 last_R: 413.05645189905243 average_R:395.5919425741181\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.20528), ('average_q2', 139.07634), ('average_q_func1_loss', 2.12310966193676), ('average_q_func2_loss', 2.1549445098638533), ('n_updates', 320001), ('average_entropy', -2.7391875), ('temperature', 0.024410873651504517)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 144 R: 414.04090274540476\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 136 R: 396.4734008064376\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 137 R: 257.3926773983636\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 144 R: 412.6024832116141\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 130 R: 383.4365581005091\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 147 R: 431.0278540637424\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 133 R: 389.22272814086097\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 172 R: 442.25918896558835\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 142 R: 278.1354523893422\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 130 R: 383.45953958077865\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 145 R: 421.6081089627144\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 136 R: 414.4418883189749\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 154 R: 464.154807224996\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 143 R: 408.3431471811965\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 164 R: 412.50517751377464\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 143 R: 410.817610341086\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 138 R: 262.4525641543541\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 136 R: 417.5502007806808\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 170 R: 402.3819197976595\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 144 R: 412.96648680992524\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 134 R: 249.83157531779503\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 150 R: 303.83713085117375\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 160 R: 331.69445961102855\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 143 R: 413.63741863563797\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 138 R: 257.9458722105238\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 145 R: 419.3986429484551\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 163 R: 361.5015444021652\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 131 R: 388.73712348876353\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 144 R: 411.357119712313\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 165 R: 384.44940362878583\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:331000 episode:2712 last_R: 310.1369944414369 average_R:399.76078558403526\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.62387), ('average_q2', 138.70769), ('average_q_func1_loss', 2.347648333311081), ('average_q_func2_loss', 2.506844809651375), ('n_updates', 321001), ('average_entropy', -2.8581598), ('temperature', 0.02334016002714634)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:332000 episode:2719 last_R: 431.14550316718737 average_R:401.4392574120487\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.02736), ('average_q2', 143.05441), ('average_q_func1_loss', 1.7728767585754395), ('average_q_func2_loss', 1.8459820002317429), ('n_updates', 322001), ('average_entropy', -2.8239872), ('temperature', 0.02343675121665001)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:333000 episode:2724 last_R: 397.4560318514022 average_R:400.92539421899477\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.03407), ('average_q2', 141.99722), ('average_q_func1_loss', 2.0452300906181335), ('average_q_func2_loss', 2.200813421010971), ('n_updates', 323001), ('average_entropy', -2.897254), ('temperature', 0.023531531915068626)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:334000 episode:2731 last_R: 424.3628784369564 average_R:400.98488183485443\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.7432), ('average_q2', 142.49937), ('average_q_func1_loss', 2.267879986166954), ('average_q_func2_loss', 2.440544122457504), ('n_updates', 324001), ('average_entropy', -3.0862284), ('temperature', 0.024002740159630775)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:335000 episode:2739 last_R: 459.4156163791296 average_R:404.29488573756026\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.60922), ('average_q2', 137.53404), ('average_q_func1_loss', 2.0482685345411302), ('average_q_func2_loss', 2.193989310860634), ('n_updates', 325001), ('average_entropy', -2.9399858), ('temperature', 0.023426711559295654)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 141 R: 421.87219992593754\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 204 R: 516.0816574942628\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 177 R: 406.1377146846084\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 150 R: 458.1951820640947\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 146 R: 449.81255981085974\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 141 R: 418.6506112353909\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 171 R: 382.7839684771956\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 144 R: 439.6263161551699\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 200 R: 480.14108428543244\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 140 R: 413.8205180393648\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 140 R: 422.54442390812505\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 141 R: 419.0950192947233\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 148 R: 457.49660744395004\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 206 R: 541.2323044473188\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 138 R: 414.06888056929256\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 149 R: 460.24904091362447\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 141 R: 427.8102130922227\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 130 R: 384.8788904666369\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 199 R: 491.44085005922324\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 137 R: 387.7388215614176\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 133 R: 388.3592383556301\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 146 R: 452.42229108127685\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 154 R: 473.07404536034267\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 140 R: 418.42549774800943\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 204 R: 502.05273864548974\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 145 R: 445.6218067192602\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 139 R: 409.80136104736357\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 130 R: 383.0505426811914\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 194 R: 485.5775587375719\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 150 R: 461.5413970806732\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 420.2552705338256 -> 440.45344471285534\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:336000 episode:2746 last_R: 479.65920860520544 average_R:406.64365804496373\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.34799), ('average_q2', 142.13292), ('average_q_func1_loss', 2.196705139875412), ('average_q_func2_loss', 2.186802214384079), ('n_updates', 326001), ('average_entropy', -3.0183432), ('temperature', 0.02271977998316288)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:337000 episode:2752 last_R: 378.79929597331864 average_R:407.4607958123603\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.52493), ('average_q2', 140.30684), ('average_q_func1_loss', 2.1663096261024477), ('average_q_func2_loss', 2.5259809201955794), ('n_updates', 327001), ('average_entropy', -3.2714562), ('temperature', 0.023429332301020622)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:338000 episode:2758 last_R: 382.5268497267716 average_R:406.470955814356\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.18047), ('average_q2', 143.29778), ('average_q_func1_loss', 2.2639153563976286), ('average_q_func2_loss', 2.3946106004714967), ('n_updates', 328001), ('average_entropy', -2.9513035), ('temperature', 0.02303137630224228)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:339000 episode:2766 last_R: 386.5922819856588 average_R:407.1044609243517\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.05307), ('average_q2', 140.0905), ('average_q_func1_loss', 1.7815328055620194), ('average_q_func2_loss', 1.8279441481828689), ('n_updates', 329001), ('average_entropy', -3.1128833), ('temperature', 0.024396857246756554)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:340000 episode:2772 last_R: 518.4754289273993 average_R:408.6087408406106\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.06647), ('average_q2', 143.13914), ('average_q_func1_loss', 2.359368164539337), ('average_q_func2_loss', 2.4247919803857805), ('n_updates', 330001), ('average_entropy', -3.0388052), ('temperature', 0.0238946583122015)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 152 R: 463.8675650149318\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 141 R: 394.9359348461481\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 151 R: 451.9372442125566\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 176 R: 416.91160916592577\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 189 R: 420.51881571815994\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 140 R: 399.9230854799494\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 139 R: 398.33332593462364\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 145 R: 424.73961646626157\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 139 R: 392.6561020680138\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 139 R: 398.08617354958136\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 138 R: 398.0763193901157\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 145 R: 425.1107354652783\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 203 R: 540.3000768011675\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 140 R: 394.94895229661495\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 152 R: 414.59696477006116\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 147 R: 428.8748647942742\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 171 R: 311.12826432490334\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 145 R: 412.0990788161828\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 138 R: 395.8431165283607\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 153 R: 458.9438949784786\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 206 R: 471.5331736119815\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 145 R: 423.7289616289426\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 195 R: 479.7372606455514\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 150 R: 445.9960006519387\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 182 R: 446.99169651770023\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 150 R: 440.7695671688692\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 141 R: 398.17122389773215\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 204 R: 489.72728735605125\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 139 R: 396.70088284376493\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 144 R: 425.39822831974385\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:341000 episode:2779 last_R: 418.69188071315165 average_R:409.95411912680436\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.81009), ('average_q2', 143.7891), ('average_q_func1_loss', 2.921146879196167), ('average_q_func2_loss', 3.257462772130966), ('n_updates', 331001), ('average_entropy', -2.8215175), ('temperature', 0.02394251525402069)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:342000 episode:2785 last_R: 390.1489958727767 average_R:410.051700743765\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.1581), ('average_q2', 141.21063), ('average_q_func1_loss', 2.2438939601182937), ('average_q_func2_loss', 2.198796489238739), ('n_updates', 332001), ('average_entropy', -3.2485564), ('temperature', 0.024924220517277718)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:343000 episode:2791 last_R: 395.0323187689239 average_R:409.72697557403694\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.99312), ('average_q2', 141.07443), ('average_q_func1_loss', 2.214327992796898), ('average_q_func2_loss', 2.1436056500673293), ('n_updates', 333001), ('average_entropy', -2.9249194), ('temperature', 0.024792766198515892)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:344000 episode:2797 last_R: 407.2693663509946 average_R:410.911036256078\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.63077), ('average_q2', 139.71123), ('average_q_func1_loss', 2.447165314555168), ('average_q_func2_loss', 2.846962130665779), ('n_updates', 334001), ('average_entropy', -3.0602236), ('temperature', 0.025233745574951172)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:345000 episode:2803 last_R: 422.6121348173974 average_R:414.24717044172013\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.47731), ('average_q2', 141.50201), ('average_q_func1_loss', 2.3687834483385086), ('average_q_func2_loss', 2.647694202065468), ('n_updates', 335001), ('average_entropy', -3.0891826), ('temperature', 0.02396642230451107)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 207 R: 461.07762221008227\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 143 R: 417.8349506577275\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 140 R: 405.9338100447468\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 213 R: 461.773974693033\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 138 R: 397.1903350068784\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 138 R: 409.2015333630776\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 160 R: 303.7902356502283\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 211 R: 470.3743521719601\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 139 R: 399.1243471527705\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 154 R: 294.26228615892853\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 139 R: 408.5740044940414\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 137 R: 402.74729670528995\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 208 R: 537.2681559845116\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 210 R: 545.683838407025\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 139 R: 399.8720217188315\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 209 R: 450.1661112776876\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 140 R: 415.55939757869953\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 204 R: 448.5970997071904\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 141 R: 413.94231107413793\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 203 R: 454.0116829891354\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 140 R: 412.76275653942065\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 138 R: 404.1183473391138\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 206 R: 497.6101492301641\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 139 R: 399.01812706991126\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 138 R: 406.24940949122686\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 138 R: 404.81881664430813\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 139 R: 404.3316139767363\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 139 R: 403.7649496697595\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 148 R: 456.1723363029006\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 143 R: 420.3940702461269\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:346000 episode:2810 last_R: 416.48617416122255 average_R:416.25468287277886\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.74753), ('average_q2', 142.83743), ('average_q_func1_loss', 2.0898859202861786), ('average_q_func2_loss', 2.037551503777504), ('n_updates', 336001), ('average_entropy', -3.1140807), ('temperature', 0.024924855679273605)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:347000 episode:2816 last_R: 410.1474442777489 average_R:418.3046210813455\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.0732), ('average_q2', 140.11746), ('average_q_func1_loss', 1.711555911898613), ('average_q_func2_loss', 1.8668428307771683), ('n_updates', 337001), ('average_entropy', -3.1028998), ('temperature', 0.02483437955379486)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:348000 episode:2823 last_R: 503.55855396040596 average_R:417.0097288156627\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.17491), ('average_q2', 142.0305), ('average_q_func1_loss', 2.1775929498672486), ('average_q_func2_loss', 2.168943123817444), ('n_updates', 338001), ('average_entropy', -2.8272936), ('temperature', 0.02445233054459095)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:349000 episode:2831 last_R: 434.65839323979714 average_R:418.3794079103273\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.84251), ('average_q2', 139.61374), ('average_q_func1_loss', 2.382175862789154), ('average_q_func2_loss', 2.4727884870767594), ('n_updates', 339001), ('average_entropy', -3.0729406), ('temperature', 0.02552756480872631)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:350000 episode:2837 last_R: 393.6505259224716 average_R:418.6136753140247\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.84825), ('average_q2', 139.87683), ('average_q_func1_loss', 2.450458301305771), ('average_q_func2_loss', 2.5846634066104888), ('n_updates', 340001), ('average_entropy', -3.1522412), ('temperature', 0.025347895920276642)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 137 R: 403.8723137023785\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 136 R: 390.63778514562983\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 142 R: 401.8032476570557\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 185 R: 469.34366793517427\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 187 R: 476.7759369428848\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 173 R: 410.7102734308787\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 142 R: 418.18273067622266\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 140 R: 419.88651070179594\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 150 R: 463.3356125268228\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 136 R: 394.5637606211208\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 141 R: 420.2447716616165\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 153 R: 272.2621207277815\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 139 R: 400.17755656365335\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 186 R: 476.0132575534505\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 145 R: 440.0768159973659\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 143 R: 403.6681209930823\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 166 R: 383.1083100245351\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 139 R: 401.92882325202487\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 189 R: 471.1745258056425\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 141 R: 403.65345251097943\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 142 R: 403.5205930851454\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 190 R: 474.54991953439236\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 149 R: 430.4009106745141\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 143 R: 403.2675749887996\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 190 R: 476.56104088167757\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 187 R: 466.2180601036709\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 160 R: 436.2720658281638\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 148 R: 455.29813171499995\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 187 R: 472.39370058350784\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 147 R: 449.73365408841613\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:351000 episode:2843 last_R: 387.8486344846975 average_R:418.82584743352373\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.36049), ('average_q2', 141.37221), ('average_q_func1_loss', 2.2512635773420335), ('average_q_func2_loss', 2.189574247598648), ('n_updates', 341001), ('average_entropy', -2.9014237), ('temperature', 0.025548718869686127)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:352000 episode:2851 last_R: 461.1795005769016 average_R:418.26522358490814\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.10585), ('average_q2', 146.15154), ('average_q_func1_loss', 2.5142071300745013), ('average_q_func2_loss', 2.5602481037378313), ('n_updates', 342001), ('average_entropy', -3.1084642), ('temperature', 0.025579089298844337)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:353000 episode:2857 last_R: 474.3079418003919 average_R:420.1989652219984\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.55254), ('average_q2', 145.6169), ('average_q_func1_loss', 2.660553384423256), ('average_q_func2_loss', 2.983894080519676), ('n_updates', 343001), ('average_entropy', -3.076171), ('temperature', 0.02581154741346836)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:354000 episode:2863 last_R: 381.3486999987648 average_R:420.2183113225652\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.68826), ('average_q2', 141.72395), ('average_q_func1_loss', 1.9559285521507264), ('average_q_func2_loss', 2.107834560871124), ('n_updates', 344001), ('average_entropy', -2.8125665), ('temperature', 0.025062041357159615)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:355000 episode:2870 last_R: 392.69356205849294 average_R:422.25619330459887\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.65047), ('average_q2', 144.59944), ('average_q_func1_loss', 2.904959196448326), ('average_q_func2_loss', 2.9610852009057997), ('n_updates', 345001), ('average_entropy', -3.0199013), ('temperature', 0.026269201189279556)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 142 R: 416.38894629229833\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 134 R: 390.79049775999647\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 230 R: 540.464744976123\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 135 R: 393.90450387369833\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 221 R: 455.8235665939683\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 147 R: 447.32858718894596\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 235 R: 547.01637314357\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 141 R: 412.354122893827\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 139 R: 409.29285414268156\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 141 R: 417.69109693523365\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 140 R: 410.0321960308243\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 144 R: 436.55827545719285\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 250 R: 521.686885404273\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 134 R: 392.25870016695603\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 141 R: 404.252969186964\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 137 R: 396.9711253923361\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 184 R: 431.5554256211205\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 130 R: 384.8878833119813\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 134 R: 396.49304562126133\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 143 R: 429.88942543539036\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 176 R: 341.229715626503\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 227 R: 558.1722683750884\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 141 R: 416.0100315567802\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 233 R: 549.6202623669657\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 229 R: 479.1260140782653\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 130 R: 380.95948434811964\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 144 R: 434.3306684579108\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 210 R: 525.941664573205\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 245 R: 616.299886680088\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 140 R: 412.2685335949011\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 440.45344471285534 -> 444.98665850288234\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:356000 episode:2876 last_R: 390.4403495519525 average_R:422.2907708152012\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.8638), ('average_q2', 142.84235), ('average_q_func1_loss', 2.020670673251152), ('average_q_func2_loss', 2.19946503341198), ('n_updates', 346001), ('average_entropy', -2.8821924), ('temperature', 0.02623027376830578)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:357000 episode:2884 last_R: 407.6796144572356 average_R:422.12487560579893\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.74178), ('average_q2', 142.96254), ('average_q_func1_loss', 2.329463331699371), ('average_q_func2_loss', 2.6258819782733918), ('n_updates', 347001), ('average_entropy', -2.9417486), ('temperature', 0.02560487948358059)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:358000 episode:2890 last_R: 393.1730366103215 average_R:423.04850788878264\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.83484), ('average_q2', 141.914), ('average_q_func1_loss', 2.153600500226021), ('average_q_func2_loss', 2.25713488817215), ('n_updates', 348001), ('average_entropy', -3.032577), ('temperature', 0.02457418665289879)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:359000 episode:2896 last_R: 405.40118086978333 average_R:422.28395118140855\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.27675), ('average_q2', 145.35144), ('average_q_func1_loss', 2.3050980126857756), ('average_q_func2_loss', 2.453296625614166), ('n_updates', 349001), ('average_entropy', -3.1132488), ('temperature', 0.023954181000590324)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:360000 episode:2902 last_R: 415.2701116359605 average_R:421.9995216280944\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.6719), ('average_q2', 142.6363), ('average_q_func1_loss', 2.5070144003629684), ('average_q_func2_loss', 2.6536703836917876), ('n_updates', 350001), ('average_entropy', -3.05246), ('temperature', 0.024561306461691856)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 150 R: 453.582626946962\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 141 R: 422.56264504717063\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 138 R: 394.5461674684825\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 135 R: 387.95639665850535\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 188 R: 471.6640574182313\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 140 R: 411.2341066646215\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 137 R: 392.80135404586196\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 137 R: 393.441451806917\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 186 R: 408.4325767132863\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 149 R: 455.29463768600425\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 187 R: 460.52624318280317\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 150 R: 453.41887463612295\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 136 R: 391.20434498095267\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 171 R: 383.46929208329385\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 139 R: 399.5043433686554\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 181 R: 415.6251371320688\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 179 R: 404.7082815883076\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 139 R: 397.70580514419765\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 181 R: 461.8962009074574\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 169 R: 383.83959911343914\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 141 R: 416.0274915109582\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 137 R: 394.7414617476814\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 151 R: 449.88378783661307\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 175 R: 396.248465970355\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 208 R: 532.2258453391523\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 187 R: 450.6866743325796\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 141 R: 406.4920138638154\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 208 R: 484.02345287724773\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 215 R: 519.523662241889\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 195 R: 434.48889515273305\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:361000 episode:2911 last_R: 411.6948267001275 average_R:419.4383591319987\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.18118), ('average_q2', 142.31172), ('average_q_func1_loss', 2.2315200209617614), ('average_q_func2_loss', 2.336308685541153), ('n_updates', 351001), ('average_entropy', -2.9982638), ('temperature', 0.024411560967564583)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:362000 episode:2916 last_R: 489.7618363979874 average_R:419.28972954260485\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.69154), ('average_q2', 143.78658), ('average_q_func1_loss', 1.846435906291008), ('average_q_func2_loss', 2.0727057665586472), ('n_updates', 352001), ('average_entropy', -3.120904), ('temperature', 0.024871809408068657)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:363000 episode:2922 last_R: 449.4245954693762 average_R:421.27491149843786\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.02115), ('average_q2', 144.15253), ('average_q_func1_loss', 2.221262200474739), ('average_q_func2_loss', 2.430906330943108), ('n_updates', 353001), ('average_entropy', -2.7195067), ('temperature', 0.02574010007083416)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:364000 episode:2929 last_R: 387.3559423318284 average_R:419.498946011496\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.00615), ('average_q2', 142.07408), ('average_q_func1_loss', 2.250496904850006), ('average_q_func2_loss', 2.370449633598328), ('n_updates', 354001), ('average_entropy', -3.0244327), ('temperature', 0.02556031197309494)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:365000 episode:2936 last_R: 415.74794200625877 average_R:418.5573467496218\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.8889), ('average_q2', 143.93729), ('average_q_func1_loss', 2.0360790276527405), ('average_q_func2_loss', 2.7930261993408205), ('n_updates', 355001), ('average_entropy', -3.020963), ('temperature', 0.024648524820804596)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 135 R: 401.87289534454817\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 126 R: 244.02580636431423\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 125 R: 238.37225610818118\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 141 R: 417.3829611324202\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 139 R: 407.85354272755086\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 135 R: 393.41546771233567\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 142 R: 420.70116694996454\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 137 R: 399.958890384219\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 143 R: 427.42920279615294\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 138 R: 404.16329983983053\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 139 R: 412.0091902417262\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 133 R: 380.35285659418304\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 123 R: 234.11807822778619\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 135 R: 396.2774786427035\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 125 R: 236.2753671978375\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 124 R: 238.0142121318245\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 138 R: 403.53721002587326\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 123 R: 235.36906462354068\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 149 R: 445.1781111553077\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 141 R: 415.72009077260884\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 137 R: 395.934508090975\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 121 R: 235.35563829716446\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 124 R: 239.9172619812419\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 124 R: 240.76902723011187\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 137 R: 406.27586578751885\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 136 R: 397.46366895740937\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 127 R: 242.0918656434406\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 123 R: 238.46365897008005\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 136 R: 396.8832115862395\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 141 R: 416.54918499084494\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:366000 episode:2942 last_R: 390.49334200305526 average_R:417.1234683476043\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.38506), ('average_q2', 144.24567), ('average_q_func1_loss', 2.133742878437042), ('average_q_func2_loss', 2.1750158166885374), ('n_updates', 356001), ('average_entropy', -2.942305), ('temperature', 0.025821391493082047)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:367000 episode:2948 last_R: 458.6672017073321 average_R:417.6828760394439\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.70146), ('average_q2', 144.73294), ('average_q_func1_loss', 2.5198626989126205), ('average_q_func2_loss', 3.206942934393883), ('n_updates', 357001), ('average_entropy', -2.7038229), ('temperature', 0.02561553567647934)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:368000 episode:2955 last_R: 416.4179806291227 average_R:418.7838735569518\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.14682), ('average_q2', 146.05576), ('average_q_func1_loss', 1.8320934438705445), ('average_q_func2_loss', 2.033897544145584), ('n_updates', 358001), ('average_entropy', -2.7146816), ('temperature', 0.025264151394367218)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:369000 episode:2962 last_R: 312.4773878684873 average_R:417.20596101582674\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.82896), ('average_q2', 144.67447), ('average_q_func1_loss', 1.9108482545614243), ('average_q_func2_loss', 1.9824881917238235), ('n_updates', 359001), ('average_entropy', -3.0337691), ('temperature', 0.025498921051621437)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:370000 episode:2969 last_R: 335.10657176301555 average_R:412.1386891381401\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.76047), ('average_q2', 142.73059), ('average_q_func1_loss', 2.030664854645729), ('average_q_func2_loss', 2.067432472705841), ('n_updates', 360001), ('average_entropy', -2.8828228), ('temperature', 0.025165554136037827)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 184 R: 416.963793100807\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 141 R: 417.6860902178571\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 129 R: 374.31545718921535\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 142 R: 418.4071801573634\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 134 R: 390.7912213180843\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 135 R: 401.6242745263946\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 134 R: 396.40846899335156\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 141 R: 416.5765862597847\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 135 R: 395.05648802571585\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 132 R: 381.16469793670495\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 128 R: 367.18096388944684\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 185 R: 421.9609484853431\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 140 R: 414.57716437917105\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 141 R: 414.3711476166303\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 127 R: 366.02355829600714\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 196 R: 470.64470086580087\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 134 R: 391.88314562909784\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 135 R: 394.73008957472433\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 140 R: 407.1869522706983\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 156 R: 478.7882896652353\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 135 R: 390.89295683483516\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 134 R: 390.61425935915406\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 142 R: 422.6192429579868\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 155 R: 478.06706577090563\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 208 R: 503.80847598451453\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 146 R: 445.59817335211477\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 134 R: 389.61547864683394\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 148 R: 455.49288099389076\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 187 R: 405.37060736887395\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 141 R: 418.0546850127873\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:371000 episode:2975 last_R: 421.6702282690952 average_R:409.00602969515444\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.09042), ('average_q2', 144.08722), ('average_q_func1_loss', 2.100393170118332), ('average_q_func2_loss', 1.980163015127182), ('n_updates', 361001), ('average_entropy', -3.157216), ('temperature', 0.025738297030329704)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:372000 episode:2982 last_R: 430.09931667154245 average_R:408.7851910923135\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.53185), ('average_q2', 143.3756), ('average_q_func1_loss', 2.1421210426092148), ('average_q_func2_loss', 2.1210505884885786), ('n_updates', 362001), ('average_entropy', -2.960355), ('temperature', 0.024658875539898872)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:373000 episode:2988 last_R: 454.0516783799813 average_R:403.0506655628312\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.35855), ('average_q2', 146.37401), ('average_q_func1_loss', 1.7968930619955064), ('average_q_func2_loss', 1.8362952893972397), ('n_updates', 363001), ('average_entropy', -2.910148), ('temperature', 0.024869123473763466)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:374000 episode:2998 last_R: 478.45967822349735 average_R:399.70749805596375\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.62547), ('average_q2', 145.5439), ('average_q_func1_loss', 2.9838612473011015), ('average_q_func2_loss', 2.92230162024498), ('n_updates', 364001), ('average_entropy', -2.814813), ('temperature', 0.025115547701716423)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:375000 episode:3003 last_R: 277.9579987948937 average_R:394.0691936690476\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.31622), ('average_q2', 146.19635), ('average_q_func1_loss', 3.5915868002176286), ('average_q_func2_loss', 3.5973320811986924), ('n_updates', 365001), ('average_entropy', -2.907532), ('temperature', 0.02610924281179905)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 137 R: 351.27589679311956\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 130 R: 315.40553803183184\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 144 R: 418.1191513126867\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 123 R: 290.13239342327785\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 126 R: 291.9432795183031\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 135 R: 396.64904808268693\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 136 R: 396.5309229442132\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 138 R: 412.0395314667868\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 138 R: 336.461683635067\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 136 R: 394.5912340202652\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 142 R: 412.6985680011182\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 116 R: 244.3419339616462\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 153 R: 464.215123554272\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 137 R: 353.1984501038888\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 144 R: 414.8309535348751\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 144 R: 423.84378005960565\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 136 R: 400.1761313220314\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 147 R: 432.44964295585345\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 114 R: 235.2159322654192\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 137 R: 340.6310536941988\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 141 R: 409.82338920140273\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 138 R: 403.50639051069885\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 138 R: 351.7830001903818\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 136 R: 398.08592146928794\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 142 R: 414.8609909892536\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 141 R: 411.85500585433283\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 137 R: 336.5667289596019\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 136 R: 350.94141810816313\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 141 R: 413.28063931512787\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 138 R: 346.6360841750505\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:376000 episode:3012 last_R: 348.8439853659049 average_R:392.64124615028516\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.492), ('average_q2', 145.20273), ('average_q_func1_loss', 1.9418907707929611), ('average_q_func2_loss', 2.0013292974233625), ('n_updates', 366001), ('average_entropy', -2.7689736), ('temperature', 0.0263594388961792)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:377000 episode:3018 last_R: 309.6085701435757 average_R:390.1534871311623\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.88457), ('average_q2', 142.76082), ('average_q_func1_loss', 2.5049233055114746), ('average_q_func2_loss', 2.2441302627325057), ('n_updates', 367001), ('average_entropy', -2.9932778), ('temperature', 0.025740979239344597)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:378000 episode:3024 last_R: 314.3288596609533 average_R:388.32234160243513\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.96559), ('average_q2', 144.02106), ('average_q_func1_loss', 2.2306801509857177), ('average_q_func2_loss', 2.1063045912981035), ('n_updates', 368001), ('average_entropy', -2.9772432), ('temperature', 0.02506278268992901)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:379000 episode:3033 last_R: 345.3032428729525 average_R:386.6541961190386\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.64662), ('average_q2', 137.52731), ('average_q_func1_loss', 2.1683759766817094), ('average_q_func2_loss', 2.198803489804268), ('n_updates', 369001), ('average_entropy', -2.8454666), ('temperature', 0.02545439824461937)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:380000 episode:3038 last_R: 424.6105806943759 average_R:387.5468725896277\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.9765), ('average_q2', 144.03114), ('average_q_func1_loss', 2.3601891458034516), ('average_q_func2_loss', 2.385097026228905), ('n_updates', 370001), ('average_entropy', -3.005098), ('temperature', 0.02587387152016163)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 196 R: 501.1932707169596\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 204 R: 480.80974859844497\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 138 R: 404.4269358687416\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 206 R: 538.815550154628\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 231 R: 605.569249727996\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 164 R: 314.33595584215055\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 142 R: 412.63783739490134\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 189 R: 435.12543442349914\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 138 R: 405.9062694784614\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 135 R: 397.9620429389246\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 196 R: 454.0122904989739\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 176 R: 432.1690818642933\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 142 R: 424.94902794581986\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 133 R: 386.5654714885946\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 147 R: 437.2081930986621\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 141 R: 409.81769141714216\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 193 R: 440.8571550863446\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 141 R: 424.0373453824839\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 144 R: 419.28557936048287\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 215 R: 519.43572690187\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 182 R: 424.3065390758921\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 184 R: 435.3604635085642\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 142 R: 424.55960689353356\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 133 R: 396.17095051135715\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 142 R: 411.43003934710447\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 143 R: 428.28183714031013\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 144 R: 424.10346678216354\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 143 R: 414.30027701402315\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 191 R: 423.4702931645692\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 189 R: 417.51895466231144\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:381000 episode:3044 last_R: 488.6013123777161 average_R:388.23423536058624\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.3365), ('average_q2', 143.30983), ('average_q_func1_loss', 2.501343305706978), ('average_q_func2_loss', 2.385712537765503), ('n_updates', 371001), ('average_entropy', -2.970535), ('temperature', 0.026666734367609024)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:382000 episode:3053 last_R: 430.48692227092863 average_R:384.85965269273527\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.1796), ('average_q2', 144.01663), ('average_q_func1_loss', 2.419055234193802), ('average_q_func2_loss', 2.1531568068265914), ('n_updates', 372001), ('average_entropy', -2.992136), ('temperature', 0.02701767534017563)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:383000 episode:3058 last_R: 409.29678351646066 average_R:382.8768093390048\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.45827), ('average_q2', 143.52344), ('average_q_func1_loss', 2.366322532892227), ('average_q_func2_loss', 2.479669157266617), ('n_updates', 373001), ('average_entropy', -3.107304), ('temperature', 0.027090169489383698)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:384000 episode:3067 last_R: 424.672921232147 average_R:385.40752467297096\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.8692), ('average_q2', 142.9297), ('average_q_func1_loss', 2.208995090723038), ('average_q_func2_loss', 2.6519899022579194), ('n_updates', 374001), ('average_entropy', -3.1053696), ('temperature', 0.026955213397741318)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:385000 episode:3073 last_R: 432.52049103973997 average_R:386.92302920830184\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.41405), ('average_q2', 143.43164), ('average_q_func1_loss', 2.6145404213666916), ('average_q_func2_loss', 2.501053903698921), ('n_updates', 375001), ('average_entropy', -2.8556597), ('temperature', 0.026692871004343033)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 142 R: 411.050085428736\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 143 R: 422.5841104163295\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 142 R: 412.19688861486026\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 143 R: 419.8199595023126\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 143 R: 420.1358993536951\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 144 R: 422.1305302321479\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 147 R: 314.9144044934871\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 162 R: 354.1048684104982\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 144 R: 423.32844982893425\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 161 R: 358.09001026427603\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 140 R: 408.83090940038517\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 159 R: 353.7742645668016\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 140 R: 400.63285228775044\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 149 R: 448.69781330976764\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 145 R: 429.44806831185275\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 146 R: 442.1474531290282\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 144 R: 424.4741558354992\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 141 R: 412.9351466225951\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 129 R: 231.7834152236148\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 140 R: 410.381532146071\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 156 R: 345.02074841582254\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 143 R: 415.4154276044972\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 141 R: 402.8483802537418\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 158 R: 487.40085666892674\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 164 R: 374.88597507996\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 159 R: 351.3242448179213\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 164 R: 359.6793169004428\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 165 R: 368.4838402667619\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 144 R: 422.6608339224748\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 149 R: 320.32969037872584\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:386000 episode:3080 last_R: 420.7617972316712 average_R:387.36802753731166\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.36334), ('average_q2', 144.48566), ('average_q_func1_loss', 2.342203760743141), ('average_q_func2_loss', 2.4079308891296387), ('n_updates', 376001), ('average_entropy', -3.1323538), ('temperature', 0.026921866461634636)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:387000 episode:3086 last_R: 422.74793749659347 average_R:392.29665748585313\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.87285), ('average_q2', 141.73314), ('average_q_func1_loss', 2.3149475187063215), ('average_q_func2_loss', 2.396113345623016), ('n_updates', 377001), ('average_entropy', -2.7812862), ('temperature', 0.026436928659677505)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:388000 episode:3092 last_R: 515.4407980645955 average_R:394.8128992732003\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.05693), ('average_q2', 141.11629), ('average_q_func1_loss', 2.6039115780591966), ('average_q_func2_loss', 2.506796868443489), ('n_updates', 378001), ('average_entropy', -3.1022635), ('temperature', 0.02717859297990799)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:389000 episode:3101 last_R: 371.4307291883041 average_R:399.9214979058088\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.28122), ('average_q2', 145.31116), ('average_q_func1_loss', 2.208448650240898), ('average_q_func2_loss', 2.322670856118202), ('n_updates', 379001), ('average_entropy', -3.0692806), ('temperature', 0.027974657714366913)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:390000 episode:3106 last_R: 467.14516971274844 average_R:404.7624254367235\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.90703), ('average_q2', 145.90791), ('average_q_func1_loss', 2.512044351696968), ('average_q_func2_loss', 2.521846371889114), ('n_updates', 380001), ('average_entropy', -2.8696406), ('temperature', 0.02738076075911522)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 151 R: 427.2103782500567\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 149 R: 436.9688921510903\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 154 R: 461.60489943846926\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 238 R: 505.78273940314756\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 144 R: 421.2018015928325\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 145 R: 421.14462992326133\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 147 R: 421.2032296502439\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 153 R: 436.8987299425304\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 210 R: 526.0981670070591\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 151 R: 432.3158451928955\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 149 R: 433.2335906441782\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 147 R: 427.36769194949477\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 145 R: 415.8553156552557\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 146 R: 424.7067083901379\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 226 R: 495.2947171953675\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 145 R: 424.6468530987625\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 151 R: 458.0805090806387\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 148 R: 431.8093848699594\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 146 R: 418.593011066147\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 210 R: 499.04016044822197\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 159 R: 485.5471638357583\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 148 R: 430.92109163523173\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 147 R: 428.41184990664726\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 216 R: 475.6159351564984\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 149 R: 433.20521221252835\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 240 R: 515.1065808497166\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 148 R: 431.6313352198949\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 143 R: 417.436266696259\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 228 R: 504.03551226021574\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 149 R: 433.4248113939285\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 444.98665850288234 -> 449.146433803881\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:391000 episode:3111 last_R: 550.9001418467031 average_R:408.70459879567846\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.73671), ('average_q2', 147.90018), ('average_q_func1_loss', 2.4769613856077193), ('average_q_func2_loss', 2.5638009762763976), ('n_updates', 381001), ('average_entropy', -3.009056), ('temperature', 0.027423350140452385)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:392000 episode:3116 last_R: 510.1903973333293 average_R:411.29670453509925\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.6451), ('average_q2', 144.72774), ('average_q_func1_loss', 2.105838884115219), ('average_q_func2_loss', 2.1459941202402115), ('n_updates', 382001), ('average_entropy', -2.9633608), ('temperature', 0.027529211714863777)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:393000 episode:3123 last_R: 418.38836608301347 average_R:415.9865017260614\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.46732), ('average_q2', 146.64151), ('average_q_func1_loss', 1.9516203373670578), ('average_q_func2_loss', 1.8382769453525543), ('n_updates', 383001), ('average_entropy', -2.8669658), ('temperature', 0.026934104040265083)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:394000 episode:3129 last_R: 423.55678666984244 average_R:420.3865655213887\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.89464), ('average_q2', 145.8473), ('average_q_func1_loss', 2.2608346462249758), ('average_q_func2_loss', 2.394701038599014), ('n_updates', 384001), ('average_entropy', -3.0191686), ('temperature', 0.02678559720516205)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:395000 episode:3135 last_R: 436.00447341841647 average_R:426.4432284979033\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.77197), ('average_q2', 144.85281), ('average_q_func1_loss', 2.9838156235218047), ('average_q_func2_loss', 3.1919518184661864), ('n_updates', 385001), ('average_entropy', -2.9375331), ('temperature', 0.027597550302743912)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 156 R: 458.51365827660214\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 156 R: 478.10227376344164\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 134 R: 347.9504955398252\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 153 R: 458.49944204227177\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 144 R: 413.3179109344592\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 258 R: 547.390808400237\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 171 R: 486.62342305882987\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 145 R: 418.3010106726091\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 234 R: 532.1125334668678\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 143 R: 412.01980171959053\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 231 R: 512.7168840002652\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 260 R: 531.7702088177612\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 155 R: 428.9154043485289\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 208 R: 418.2213206663168\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 132 R: 352.22645165074056\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 145 R: 410.5214288446988\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 144 R: 408.94180836574054\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 148 R: 437.40537977374703\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 215 R: 499.0937630407514\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 154 R: 430.8753014106269\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 132 R: 353.2866043068266\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 143 R: 410.44464273456447\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 227 R: 493.930393644508\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 164 R: 456.79275428697554\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 144 R: 414.02459896216715\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 137 R: 360.0478011578614\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 144 R: 413.715217209033\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 227 R: 487.86776487504795\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 132 R: 353.13464132432495\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 158 R: 449.82988857192817\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:396000 episode:3141 last_R: 327.7928696223875 average_R:424.79259927083893\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.709), ('average_q2', 142.6114), ('average_q_func1_loss', 2.5414737647771837), ('average_q_func2_loss', 2.4747123712301256), ('n_updates', 386001), ('average_entropy', -3.278509), ('temperature', 0.02676447480916977)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:397000 episode:3146 last_R: 501.2160853314242 average_R:427.4778238770196\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.19489), ('average_q2', 142.33844), ('average_q_func1_loss', 2.574586466550827), ('average_q_func2_loss', 2.6335243356227873), ('n_updates', 387001), ('average_entropy', -2.953795), ('temperature', 0.027873661369085312)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:398000 episode:3152 last_R: 369.1486772088102 average_R:431.5753008660316\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.05519), ('average_q2', 146.0944), ('average_q_func1_loss', 2.3921980822086333), ('average_q_func2_loss', 2.661634840965271), ('n_updates', 388001), ('average_entropy', -2.8469155), ('temperature', 0.028721265494823456)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:399000 episode:3157 last_R: 559.9847840589808 average_R:435.306473663222\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.08667), ('average_q2', 146.02835), ('average_q_func1_loss', 2.8065932059288023), ('average_q_func2_loss', 3.0225609838962555), ('n_updates', 389001), ('average_entropy', -2.9340947), ('temperature', 0.02706347592175007)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:400000 episode:3163 last_R: 516.0514327119387 average_R:442.33192298051597\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.78134), ('average_q2', 146.74982), ('average_q_func1_loss', 2.6847434276342392), ('average_q_func2_loss', 2.661133737564087), ('n_updates', 390001), ('average_entropy', -2.8577187), ('temperature', 0.027893133461475372)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 212 R: 543.1141448529597\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 153 R: 452.43592727233766\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 231 R: 554.2280244494846\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 158 R: 482.87428836574986\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 154 R: 481.6774685449877\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 215 R: 536.7193021643321\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 207 R: 493.8503632845641\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 142 R: 413.3811592028617\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 159 R: 487.0068841152408\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 168 R: 535.5042994089323\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 223 R: 507.6877576814686\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 144 R: 423.35166982064027\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 234 R: 564.4308475954499\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 166 R: 506.6459627429697\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 143 R: 413.89108003128996\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 144 R: 424.87452010277514\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 153 R: 458.32690608386673\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 159 R: 485.2007857850713\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 154 R: 463.11419917843693\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 203 R: 476.6159437864798\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 146 R: 432.166747003803\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 216 R: 482.6755249146971\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 144 R: 425.24633417614916\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 144 R: 413.87156489594577\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 220 R: 479.06912682402515\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 207 R: 492.9609966531692\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 158 R: 485.096590417523\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 143 R: 419.273090129914\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 205 R: 493.3077290138154\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 136 R: 374.9029767357029\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 449.146433803881 -> 473.4500738411548\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:401000 episode:3170 last_R: 450.878786287858 average_R:449.5919072770477\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.40688), ('average_q2', 143.29784), ('average_q_func1_loss', 2.439661982655525), ('average_q_func2_loss', 2.2922660833597184), ('n_updates', 391001), ('average_entropy', -2.8593054), ('temperature', 0.028818344697356224)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:402000 episode:3175 last_R: 571.0380385985201 average_R:453.71481736871203\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.08707), ('average_q2', 141.96153), ('average_q_func1_loss', 2.0477766609191894), ('average_q_func2_loss', 2.3351364916563035), ('n_updates', 392001), ('average_entropy', -3.2300181), ('temperature', 0.029368573799729347)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:403000 episode:3181 last_R: 428.99506781834197 average_R:456.01035273636023\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.2536), ('average_q2', 146.10706), ('average_q_func1_loss', 2.60164203286171), ('average_q_func2_loss', 2.8274319475889205), ('n_updates', 393001), ('average_entropy', -2.9258096), ('temperature', 0.02991924062371254)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:404000 episode:3186 last_R: 427.9977238401543 average_R:456.7323982732073\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.80003), ('average_q2', 147.0769), ('average_q_func1_loss', 2.561272830367088), ('average_q_func2_loss', 2.4265613806247712), ('n_updates', 394001), ('average_entropy', -3.0091605), ('temperature', 0.02889712154865265)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:405000 episode:3193 last_R: 428.41005047003705 average_R:460.45774613862244\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.53275), ('average_q2', 145.64166), ('average_q_func1_loss', 2.5506356024742125), ('average_q_func2_loss', 2.6216224920749664), ('n_updates', 395001), ('average_entropy', -3.057899), ('temperature', 0.028559964150190353)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 151 R: 446.8297616093162\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 152 R: 448.2583186990887\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 171 R: 540.294125330633\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 165 R: 490.1065205581992\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 147 R: 420.4239764041329\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 146 R: 421.96363823594965\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 151 R: 445.9434059011998\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 177 R: 550.2909672488893\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 283 R: 586.2899170543764\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 162 R: 470.608674555311\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 213 R: 554.3478518961572\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 171 R: 536.0642660658215\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 162 R: 496.8748832933552\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 176 R: 550.6659286439827\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 218 R: 454.3078550108324\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 146 R: 421.72367773149585\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 150 R: 440.08495307783664\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 151 R: 444.9945519443001\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 246 R: 524.8816051672832\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 168 R: 521.5241017915438\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 147 R: 427.6085129632244\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 281 R: 574.4301130595009\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 152 R: 450.2136223807049\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 145 R: 422.27104669885665\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 172 R: 543.5392927080358\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 236 R: 591.0153081821936\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 166 R: 499.7552707571768\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 182 R: 560.937099891744\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 245 R: 542.4538437650032\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 145 R: 420.91376256668144\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 473.4500738411548 -> 493.3205617730942\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:406000 episode:3198 last_R: 531.115195519813 average_R:463.5699134607215\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.13756), ('average_q2', 141.20444), ('average_q_func1_loss', 2.2985697692632674), ('average_q_func2_loss', 2.319042653441429), ('n_updates', 396001), ('average_entropy', -2.895123), ('temperature', 0.028855182230472565)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:407000 episode:3203 last_R: 448.99703566191613 average_R:467.099028957038\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.57016), ('average_q2', 147.65494), ('average_q_func1_loss', 2.4170029735565186), ('average_q_func2_loss', 2.4701611411571505), ('n_updates', 397001), ('average_entropy', -2.9807856), ('temperature', 0.02943231351673603)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:408000 episode:3210 last_R: 420.75933531218215 average_R:466.2654191738304\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.59396), ('average_q2', 147.48238), ('average_q_func1_loss', 2.276433030962944), ('average_q_func2_loss', 2.2936558228731156), ('n_updates', 398001), ('average_entropy', -3.033887), ('temperature', 0.02948049083352089)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:409000 episode:3215 last_R: 490.38634989546756 average_R:467.16140807922756\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.28981), ('average_q2', 143.37897), ('average_q_func1_loss', 2.309234935641289), ('average_q_func2_loss', 2.252054203748703), ('n_updates', 399001), ('average_entropy', -2.8765178), ('temperature', 0.02934723161160946)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:410000 episode:3220 last_R: 423.90937594581015 average_R:465.2063415239014\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.27359), ('average_q2', 146.28827), ('average_q_func1_loss', 2.1359089142084122), ('average_q_func2_loss', 2.1296300888061523), ('n_updates', 400001), ('average_entropy', -3.0059633), ('temperature', 0.029318850487470627)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 148 R: 427.30648405551364\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 210 R: 488.4127636730609\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 167 R: 477.73681242085905\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 147 R: 435.54050641628265\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 147 R: 428.8168296642846\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 169 R: 523.0960466595005\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 156 R: 478.8467503889993\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 147 R: 426.23831101327875\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 158 R: 483.0706211219704\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 239 R: 573.1624669056989\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 217 R: 513.951233699831\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 250 R: 567.5603910982151\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 212 R: 493.8943377967911\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 222 R: 506.0810233167942\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 212 R: 501.89498358300574\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 207 R: 488.3966282361534\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 169 R: 524.4386751226798\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 147 R: 427.75363229749286\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 147 R: 428.6808607939474\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 169 R: 544.0139179400463\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 172 R: 553.5264592616235\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 146 R: 428.5277143429567\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 152 R: 456.77710118863274\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 241 R: 570.5159308797997\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 168 R: 524.1360738024443\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 235 R: 497.33447600066944\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 147 R: 424.93445831127417\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 168 R: 533.445566957852\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 163 R: 494.86549246624463\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 204 R: 482.9928431029331\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:411000 episode:3228 last_R: 439.6262999360555 average_R:465.92238351788075\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.543), ('average_q2', 145.27612), ('average_q_func1_loss', 1.8845380216836929), ('average_q_func2_loss', 2.0940109449625015), ('n_updates', 401001), ('average_entropy', -3.012265), ('temperature', 0.030311981216073036)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:412000 episode:3234 last_R: 517.2574507828829 average_R:465.65488315413216\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.22209), ('average_q2', 146.30449), ('average_q_func1_loss', 2.6716689997911454), ('average_q_func2_loss', 2.7584883654117585), ('n_updates', 402001), ('average_entropy', -3.0073364), ('temperature', 0.029781010001897812)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:413000 episode:3238 last_R: 192.4168869094261 average_R:462.2961960878206\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.32646), ('average_q2', 145.38538), ('average_q_func1_loss', 2.1932317388057707), ('average_q_func2_loss', 2.322400408387184), ('n_updates', 403001), ('average_entropy', -2.9495966), ('temperature', 0.03018893115222454)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:414000 episode:3245 last_R: 565.3598902465146 average_R:468.8330296653097\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.19594), ('average_q2', 145.10406), ('average_q_func1_loss', 1.9577437180280686), ('average_q_func2_loss', 2.064179941415787), ('n_updates', 404001), ('average_entropy', -3.0088851), ('temperature', 0.029612895101308823)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:415000 episode:3251 last_R: 494.67223111065596 average_R:466.9339247026776\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.15726), ('average_q2', 146.11865), ('average_q_func1_loss', 2.3611513167619704), ('average_q_func2_loss', 2.3906804090738296), ('n_updates', 405001), ('average_entropy', -2.9365623), ('temperature', 0.02858375944197178)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 184 R: 557.8855655859029\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 185 R: 512.961758228644\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 145 R: 409.6040456190343\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 159 R: 476.56460684108225\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 172 R: 475.8086932092207\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 183 R: 430.5055356827436\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 157 R: 451.5528817579751\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 173 R: 413.2268006104536\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 201 R: 484.0785405903234\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 163 R: 520.5732890823938\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 146 R: 422.32051635011624\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 180 R: 434.1027891591433\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 145 R: 416.22102462442575\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 156 R: 452.53544078144506\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 200 R: 482.33096619508274\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 199 R: 600.1659815328472\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 169 R: 476.82838918906634\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 138 R: 369.7511908698332\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 214 R: 503.01772396611227\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 182 R: 415.4511125627383\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 146 R: 419.2402173801647\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 144 R: 412.7423124319178\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 139 R: 369.1019707237038\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 191 R: 585.2306110615187\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 192 R: 483.5417559041079\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 180 R: 577.8327952573121\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 177 R: 374.4684690494458\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 192 R: 587.1451255847953\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 144 R: 414.0276485349894\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 146 R: 421.002960618433\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:416000 episode:3257 last_R: 535.6765012938405 average_R:468.206726213352\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.88129), ('average_q2', 147.14671), ('average_q_func1_loss', 2.9391966438293458), ('average_q_func2_loss', 3.113485199213028), ('n_updates', 406001), ('average_entropy', -3.0571957), ('temperature', 0.029574019834399223)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:417000 episode:3262 last_R: 534.3467100748431 average_R:469.6873335604563\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.9665), ('average_q2', 147.99838), ('average_q_func1_loss', 2.3402974933385847), ('average_q_func2_loss', 2.2384704023599626), ('n_updates', 407001), ('average_entropy', -2.770498), ('temperature', 0.029583357274532318)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:418000 episode:3268 last_R: 397.72703218122257 average_R:466.77844093833954\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.29718), ('average_q2', 146.25102), ('average_q_func1_loss', 2.447578254342079), ('average_q_func2_loss', 2.582656432390213), ('n_updates', 408001), ('average_entropy', -2.9763455), ('temperature', 0.03135072812438011)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:419000 episode:3274 last_R: 407.0624588281733 average_R:468.0104884741\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.41937), ('average_q2', 145.29584), ('average_q_func1_loss', 3.071635429263115), ('average_q_func2_loss', 2.978508258461952), ('n_updates', 409001), ('average_entropy', -2.9256938), ('temperature', 0.03163382038474083)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:420000 episode:3279 last_R: 267.7218591645002 average_R:465.9744609483234\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.06068), ('average_q2', 145.0505), ('average_q_func1_loss', 2.4826790845394133), ('average_q_func2_loss', 2.4123480623960494), ('n_updates', 410001), ('average_entropy', -3.094623), ('temperature', 0.030979793518781662)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 131 R: 275.30146961324476\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 196 R: 449.91722590733656\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 177 R: 540.8496803050025\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 178 R: 540.1581770814569\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 140 R: 389.677983879528\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 168 R: 459.17553193825836\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 186 R: 456.9635990134842\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 186 R: 559.6862122870831\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 140 R: 391.61933081400315\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 132 R: 239.25247605112133\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 121 R: 216.45897696300915\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 190 R: 577.0541692163613\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 180 R: 548.2280494925819\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 130 R: 232.08072632911995\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 159 R: 482.78794833696884\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 153 R: 339.84453523731406\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 159 R: 470.4363237561161\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 153 R: 433.5019288027818\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 170 R: 396.18742157640185\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 190 R: 488.8773158108946\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 130 R: 241.3091297651286\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 185 R: 563.8256570974212\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 190 R: 570.6929435854546\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 184 R: 564.8384336659974\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 167 R: 521.3199124364048\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 162 R: 487.0711067088625\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 140 R: 388.40927946880834\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 191 R: 448.7142645683864\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 184 R: 563.3680936382934\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 179 R: 546.9760502616025\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:421000 episode:3286 last_R: 409.4285594038479 average_R:467.01359809257934\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.82469), ('average_q2', 146.6569), ('average_q_func1_loss', 2.681641924381256), ('average_q_func2_loss', 2.4845264464616776), ('n_updates', 411001), ('average_entropy', -3.049273), ('temperature', 0.029574887827038765)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:422000 episode:3291 last_R: 537.1125219809646 average_R:463.94878830863905\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.4049), ('average_q2', 146.45692), ('average_q_func1_loss', 2.789486380815506), ('average_q_func2_loss', 2.8927580922842027), ('n_updates', 412001), ('average_entropy', -2.8857312), ('temperature', 0.029562557116150856)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:423000 episode:3297 last_R: 403.49953972572536 average_R:466.94298472122085\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.52278), ('average_q2', 147.45299), ('average_q_func1_loss', 2.3239954620599748), ('average_q_func2_loss', 2.246240959763527), ('n_updates', 413001), ('average_entropy', -2.9349887), ('temperature', 0.029367482289671898)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:424000 episode:3304 last_R: 413.42678143198935 average_R:467.1129172471943\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.89546), ('average_q2', 150.83907), ('average_q_func1_loss', 2.999562321305275), ('average_q_func2_loss', 3.153135607242584), ('n_updates', 414001), ('average_entropy', -3.0550277), ('temperature', 0.030093450099229813)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:425000 episode:3309 last_R: 586.8271849900918 average_R:469.3147355316768\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.43149), ('average_q2', 146.41357), ('average_q_func1_loss', 2.08096963763237), ('average_q_func2_loss', 2.1151908552646637), ('n_updates', 415001), ('average_entropy', -2.9378242), ('temperature', 0.029995055869221687)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 166 R: 448.8440694517997\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 144 R: 417.90450952265667\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 144 R: 413.9958301136544\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 170 R: 466.9951293749389\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 183 R: 480.4989470117346\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 163 R: 445.406773451909\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 145 R: 422.44939262913965\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 146 R: 425.1918292607826\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 200 R: 523.955174742925\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 144 R: 416.4652153573676\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 146 R: 432.4879167917481\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 174 R: 434.3842016005233\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 144 R: 421.37984730484783\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 171 R: 478.9103611921045\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 196 R: 514.3565765196817\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 178 R: 481.98722206154434\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 144 R: 414.2400559435386\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 167 R: 409.0730621579469\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 168 R: 415.42906064618165\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 142 R: 413.5188635374493\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 199 R: 534.4267966868321\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 144 R: 413.83601630643295\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 145 R: 417.6656484012783\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 144 R: 417.42227672422354\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 150 R: 353.8642198421017\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 203 R: 580.6326840559581\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 151 R: 447.5902120021811\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 143 R: 419.4480547217314\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 143 R: 414.08966154944085\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 183 R: 466.3407942159123\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:426000 episode:3314 last_R: 588.8910474111166 average_R:468.2797533453757\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.11919), ('average_q2', 145.99785), ('average_q_func1_loss', 2.6434405314922333), ('average_q_func2_loss', 2.5906187176704405), ('n_updates', 416001), ('average_entropy', -2.957274), ('temperature', 0.029275566339492798)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:427000 episode:3320 last_R: 427.6627831324661 average_R:468.93586775290265\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.66104), ('average_q2', 149.81862), ('average_q_func1_loss', 2.2754342657327653), ('average_q_func2_loss', 2.2421481412649156), ('n_updates', 417001), ('average_entropy', -2.898315), ('temperature', 0.03009430319070816)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:428000 episode:3327 last_R: 499.64836272634653 average_R:467.7255452940892\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.41225), ('average_q2', 145.34274), ('average_q_func1_loss', 2.7015811401605605), ('average_q_func2_loss', 2.7621806609630584), ('n_updates', 418001), ('average_entropy', -3.0868807), ('temperature', 0.029628178104758263)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:429000 episode:3334 last_R: 428.9226013343871 average_R:463.7232580125577\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.85402), ('average_q2', 146.9075), ('average_q_func1_loss', 2.5050999414920807), ('average_q_func2_loss', 2.475491386651993), ('n_updates', 419001), ('average_entropy', -2.7031047), ('temperature', 0.02917342819273472)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:430000 episode:3339 last_R: 303.56409962180965 average_R:461.0737756399467\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.6751), ('average_q2', 147.77454), ('average_q_func1_loss', 2.3660095393657685), ('average_q_func2_loss', 2.536099408864975), ('n_updates', 420001), ('average_entropy', -2.9937925), ('temperature', 0.02910364232957363)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 150 R: 397.6166642919584\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 163 R: 500.27237494243116\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 144 R: 300.0206044288087\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 154 R: 469.63496472912396\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 164 R: 456.4035903479509\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 162 R: 446.14979850848334\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 162 R: 431.7066432914385\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 153 R: 331.07676307625155\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 144 R: 425.26548391325287\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 180 R: 383.55321803139634\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 158 R: 429.8594874452174\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 144 R: 425.64826068348486\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 161 R: 443.2596076088552\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 156 R: 417.00435113898993\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 162 R: 505.7847756724668\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 170 R: 387.63983190012624\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 138 R: 302.0996830330607\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 165 R: 459.376762472176\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 168 R: 380.9079372898359\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 145 R: 427.6624999727426\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 162 R: 376.48780076070966\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 177 R: 422.5253419295183\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 147 R: 388.74401173873133\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 160 R: 373.18657004113413\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 147 R: 429.8358313781972\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 147 R: 433.38848457761827\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 148 R: 439.9156918900422\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 186 R: 459.48684710700246\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 177 R: 453.70526817728853\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 161 R: 440.4104997069977\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:431000 episode:3345 last_R: 439.0276065066705 average_R:460.2136841892975\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.46745), ('average_q2', 148.42398), ('average_q_func1_loss', 3.210477131009102), ('average_q_func2_loss', 3.203323423266411), ('n_updates', 421001), ('average_entropy', -3.112408), ('temperature', 0.02992325648665428)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:432000 episode:3351 last_R: 494.66424154168675 average_R:463.90103733342636\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.06726), ('average_q2', 147.03488), ('average_q_func1_loss', 1.9006516593694687), ('average_q_func2_loss', 2.0138411062955854), ('n_updates', 422001), ('average_entropy', -2.968591), ('temperature', 0.02896168828010559)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:433000 episode:3357 last_R: 448.0753129000579 average_R:465.60270216278775\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.256), ('average_q2', 146.17525), ('average_q_func1_loss', 2.2081805860996244), ('average_q_func2_loss', 2.084202691912651), ('n_updates', 423001), ('average_entropy', -3.022246), ('temperature', 0.029300376772880554)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:434000 episode:3362 last_R: 593.4951673774907 average_R:463.8021007132313\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.29498), ('average_q2', 147.19206), ('average_q_func1_loss', 2.3940452498197557), ('average_q_func2_loss', 2.3504611480236055), ('n_updates', 424001), ('average_entropy', -3.119047), ('temperature', 0.028587568551301956)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:435000 episode:3367 last_R: 516.7701256828435 average_R:467.0319760246236\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.36987), ('average_q2', 149.23831), ('average_q_func1_loss', 2.2258919495344163), ('average_q_func2_loss', 2.405851812362671), ('n_updates', 425001), ('average_entropy', -2.7876728), ('temperature', 0.029619794338941574)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 168 R: 338.44238634435027\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 165 R: 521.4983268685263\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 174 R: 409.33588731598996\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 182 R: 394.6629544511552\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 153 R: 473.0917866790862\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 169 R: 529.2143251174097\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 170 R: 321.3647229836352\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 163 R: 504.6628924784629\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 147 R: 432.1298861697169\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 173 R: 350.01264748237986\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 143 R: 423.4375529173004\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 144 R: 424.55009991390415\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 143 R: 423.560204735234\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 158 R: 494.94936936243016\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 166 R: 516.326997859744\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 162 R: 501.789022742337\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 147 R: 428.7328154711111\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 163 R: 305.0329437638204\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 160 R: 306.6541686282623\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 143 R: 422.4018640982113\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 190 R: 398.52823489060137\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 189 R: 401.4968276364125\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 163 R: 506.15457796536316\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 161 R: 503.2710700249038\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 151 R: 449.9921383604564\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 203 R: 450.3394251284813\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 164 R: 284.07411605220733\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 157 R: 490.93163633100977\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 189 R: 408.24703824697576\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 181 R: 522.9775043398103\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:436000 episode:3373 last_R: 499.01096410636 average_R:468.1157655626764\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.37563), ('average_q2', 147.28534), ('average_q_func1_loss', 2.201288722157478), ('average_q_func2_loss', 2.209005956053734), ('n_updates', 426001), ('average_entropy', -3.111849), ('temperature', 0.030177852138876915)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:437000 episode:3381 last_R: 375.69172887349134 average_R:465.57552757540753\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.35031), ('average_q2', 147.43747), ('average_q_func1_loss', 2.4036993879079818), ('average_q_func2_loss', 2.562606366276741), ('n_updates', 427001), ('average_entropy', -2.794377), ('temperature', 0.029927274212241173)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:438000 episode:3386 last_R: 463.4703887506549 average_R:463.9728576105205\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.50269), ('average_q2', 150.15157), ('average_q_func1_loss', 2.2573473060131075), ('average_q_func2_loss', 2.375155689716339), ('n_updates', 428001), ('average_entropy', -2.7358487), ('temperature', 0.029864685609936714)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:439000 episode:3393 last_R: 483.1455001163496 average_R:460.04941508174505\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.01031), ('average_q2', 148.80588), ('average_q_func1_loss', 3.454865770339966), ('average_q_func2_loss', 3.5270696610212324), ('n_updates', 429001), ('average_entropy', -2.6170812), ('temperature', 0.0295422300696373)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:440000 episode:3399 last_R: 533.3455516835303 average_R:456.7653586760615\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.54276), ('average_q2', 147.51566), ('average_q_func1_loss', 2.1656624084711074), ('average_q_func2_loss', 2.1592812979221345), ('n_updates', 430001), ('average_entropy', -3.162527), ('temperature', 0.029276682063937187)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 150 R: 432.1737150876462\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 168 R: 532.8843680549418\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 148 R: 424.2242289475543\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 150 R: 442.12577150872767\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 152 R: 450.78125963480636\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 168 R: 535.9268836656728\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 147 R: 423.60300037429914\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 162 R: 491.119342133116\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 142 R: 276.28963514432087\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 168 R: 534.2290173358045\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 159 R: 360.3805240703867\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 159 R: 491.70487656582804\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 194 R: 524.8278068140487\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 150 R: 445.0457759480922\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 136 R: 240.59657791037526\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 155 R: 286.6199550204038\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 166 R: 402.30795283109495\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 132 R: 234.2474184862574\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 150 R: 447.6299162002453\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 142 R: 407.38055037254304\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 161 R: 372.3911304142759\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 151 R: 452.04022437449936\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 147 R: 425.2908995552626\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 172 R: 338.5646737031063\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 167 R: 533.7101166463223\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 164 R: 510.7967641688767\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 159 R: 489.82226888787216\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 159 R: 491.98036822373143\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 146 R: 421.96092482629246\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 147 R: 428.87932385359653\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:441000 episode:3405 last_R: 383.2635106898278 average_R:454.5140947454554\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.403), ('average_q2', 149.41838), ('average_q_func1_loss', 2.974612250328064), ('average_q_func2_loss', 2.705664802789688), ('n_updates', 431001), ('average_entropy', -3.078747), ('temperature', 0.02902430109679699)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:442000 episode:3411 last_R: 449.9427042116653 average_R:454.28138991715116\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.54619), ('average_q2', 146.59804), ('average_q_func1_loss', 2.3701993834972384), ('average_q_func2_loss', 2.5915521812438964), ('n_updates', 432001), ('average_entropy', -2.9189649), ('temperature', 0.030006594955921173)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:443000 episode:3417 last_R: 417.25758076920187 average_R:450.72399851191403\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.86237), ('average_q2', 144.83603), ('average_q_func1_loss', 2.603039186000824), ('average_q_func2_loss', 2.698737832903862), ('n_updates', 433001), ('average_entropy', -2.9649115), ('temperature', 0.029303114861249924)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:444000 episode:3422 last_R: 431.68502808946334 average_R:448.9903897096125\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.52412), ('average_q2', 148.57162), ('average_q_func1_loss', 2.241505095362663), ('average_q_func2_loss', 2.251709555387497), ('n_updates', 434001), ('average_entropy', -3.153206), ('temperature', 0.02901568077504635)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:445000 episode:3430 last_R: 271.319951409834 average_R:447.42794095874814\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.02466), ('average_q2', 146.08002), ('average_q_func1_loss', 2.4964898544549943), ('average_q_func2_loss', 2.6891132199764254), ('n_updates', 435001), ('average_entropy', -3.2970772), ('temperature', 0.02963375300168991)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 164 R: 503.73869940602657\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 146 R: 421.040189357106\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 255 R: 524.3182080829666\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 175 R: 540.0434549902457\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 258 R: 544.5506554109382\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 212 R: 428.57908061299054\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 166 R: 514.056605586252\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 147 R: 419.9053588588119\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 148 R: 419.8615213354538\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 170 R: 528.9991311096339\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 145 R: 418.0585175515991\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 149 R: 435.73503626843666\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 146 R: 418.0353233174151\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 236 R: 504.97781048397155\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 246 R: 498.67732626360896\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 182 R: 584.8293678392347\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 169 R: 524.8634477434885\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 156 R: 453.02545089884376\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 217 R: 432.36904167219507\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 152 R: 445.7404003138929\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 148 R: 404.7901648111881\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 181 R: 581.309478646206\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 255 R: 507.03938653821126\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 251 R: 518.2715801641108\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 158 R: 464.85769442442455\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 235 R: 490.6588577552632\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 210 R: 423.89988041475567\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 155 R: 448.3297043745419\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 157 R: 454.5706876511015\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 260 R: 550.5137171040037\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:446000 episode:3436 last_R: 444.87859828671947 average_R:448.3536631558834\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.52477), ('average_q2', 145.62428), ('average_q_func1_loss', 3.0248149639368056), ('average_q_func2_loss', 3.0116612178087236), ('n_updates', 436001), ('average_entropy', -3.018788), ('temperature', 0.029897237196564674)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:447000 episode:3441 last_R: 563.8304658522176 average_R:450.9008424239462\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.2056), ('average_q2', 144.07153), ('average_q_func1_loss', 2.3700028860569002), ('average_q_func2_loss', 2.341202576160431), ('n_updates', 437001), ('average_entropy', -2.9496963), ('temperature', 0.029782645404338837)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:448000 episode:3448 last_R: 416.4501206953252 average_R:448.3466130896795\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.9887), ('average_q2', 144.96657), ('average_q_func1_loss', 2.208225426077843), ('average_q_func2_loss', 2.3321748566627503), ('n_updates', 438001), ('average_entropy', -3.0180593), ('temperature', 0.02909710630774498)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:449000 episode:3454 last_R: 413.58682372079966 average_R:443.99346723161\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.3244), ('average_q2', 143.19438), ('average_q_func1_loss', 2.2230074828863144), ('average_q_func2_loss', 2.2404264682531356), ('n_updates', 439001), ('average_entropy', -3.2846098), ('temperature', 0.028431694954633713)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:450000 episode:3459 last_R: 417.3406472084034 average_R:442.3622961892515\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.00107), ('average_q2', 145.86707), ('average_q_func1_loss', 2.177095396518707), ('average_q_func2_loss', 2.4306365561485292), ('n_updates', 440001), ('average_entropy', -3.1501222), ('temperature', 0.028840936720371246)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 161 R: 493.6763431048216\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 146 R: 423.54019207886506\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 169 R: 537.143786071827\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 147 R: 438.63311164266315\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 150 R: 390.98734080905473\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 161 R: 496.52033435659473\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 145 R: 421.82204174159966\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 171 R: 550.2327326657783\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 190 R: 460.0704902040346\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 169 R: 543.3375037675511\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 173 R: 475.06353671365304\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 161 R: 497.68829871762017\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 196 R: 479.12182792327957\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 146 R: 426.66101933301337\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 167 R: 528.2338591043921\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 235 R: 586.7750755826536\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 147 R: 427.7786273817279\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 148 R: 438.87849872213667\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 150 R: 395.1389340078341\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 194 R: 484.95384567456233\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 158 R: 493.70972991894695\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 171 R: 547.3041100928916\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 149 R: 446.3681080955723\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 155 R: 388.8368868951264\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 145 R: 422.3108611727415\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 149 R: 299.12707966896244\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 171 R: 551.8238903784371\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 149 R: 440.72465206768294\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 252 R: 613.828131629796\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 176 R: 482.34221072467085\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:451000 episode:3465 last_R: 422.5420896616514 average_R:438.12033796129464\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.63167), ('average_q2', 150.52884), ('average_q_func1_loss', 2.0963659352064132), ('average_q_func2_loss', 2.1376042771339416), ('n_updates', 441001), ('average_entropy', -3.0548518), ('temperature', 0.029354553669691086)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:452000 episode:3471 last_R: 402.4044930596009 average_R:436.5355999908682\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.51978), ('average_q2', 148.39766), ('average_q_func1_loss', 2.461563172340393), ('average_q_func2_loss', 2.72153754234314), ('n_updates', 442001), ('average_entropy', -2.9800541), ('temperature', 0.03074765019118786)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:453000 episode:3478 last_R: 433.6568491374199 average_R:435.9048363075209\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.42436), ('average_q2', 143.42026), ('average_q_func1_loss', 2.5561953032016755), ('average_q_func2_loss', 2.8525578284263613), ('n_updates', 443001), ('average_entropy', -3.1987224), ('temperature', 0.029766956344246864)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:454000 episode:3483 last_R: 534.3227903770205 average_R:440.76126979778195\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.37338), ('average_q2', 144.1818), ('average_q_func1_loss', 2.697817943096161), ('average_q_func2_loss', 2.7314790040254593), ('n_updates', 444001), ('average_entropy', -3.1111143), ('temperature', 0.030467038974165916)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:455000 episode:3489 last_R: 440.2250083399447 average_R:444.687757386433\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.43965), ('average_q2', 149.5829), ('average_q_func1_loss', 2.703659527897835), ('average_q_func2_loss', 2.8231278324127196), ('n_updates', 445001), ('average_entropy', -2.8837726), ('temperature', 0.03050544671714306)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 160 R: 499.60100248512117\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 172 R: 552.3554509932861\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 172 R: 554.7323660896982\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 168 R: 544.9623726949477\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 160 R: 501.84690762085825\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 144 R: 420.0270143753543\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 146 R: 421.83309744547006\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 158 R: 477.5303901102292\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 157 R: 458.667751053773\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 148 R: 446.71118000694906\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 145 R: 423.3738361279586\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 227 R: 537.1529647122106\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 166 R: 354.77975137954434\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 147 R: 426.62392592964426\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 151 R: 464.86603782226024\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 212 R: 478.1343200656618\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 170 R: 539.0839220500742\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 161 R: 502.3666082854741\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 164 R: 528.3150017148217\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 146 R: 423.1388617610453\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 153 R: 461.48284962227456\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 145 R: 422.0607245727042\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 165 R: 520.6135409543275\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 149 R: 439.2509766287571\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 156 R: 468.6129181968981\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 160 R: 501.6750527255325\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 206 R: 527.6758374269889\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 167 R: 539.8330618764309\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 154 R: 470.9341159560003\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 151 R: 464.3567163317206\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:456000 episode:3496 last_R: 462.424115165096 average_R:452.58307123764905\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.44249), ('average_q2', 146.4329), ('average_q_func1_loss', 3.0725636321306227), ('average_q_func2_loss', 3.111657953262329), ('n_updates', 446001), ('average_entropy', -3.0088563), ('temperature', 0.03035547584295273)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:457000 episode:3502 last_R: 474.06080083585067 average_R:450.86331046712286\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.13382), ('average_q2', 149.10625), ('average_q_func1_loss', 2.1238885134458543), ('average_q_func2_loss', 2.226531165242195), ('n_updates', 447001), ('average_entropy', -3.1335974), ('temperature', 0.02989785000681877)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:458000 episode:3507 last_R: 610.7584582386788 average_R:454.44555051128145\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.76659), ('average_q2', 145.91096), ('average_q_func1_loss', 2.814304890036583), ('average_q_func2_loss', 2.774579167962074), ('n_updates', 448001), ('average_entropy', -3.011066), ('temperature', 0.030976906418800354)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:459000 episode:3513 last_R: 467.2436095708065 average_R:455.5989713676501\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.10197), ('average_q2', 150.16066), ('average_q_func1_loss', 2.933634460568428), ('average_q_func2_loss', 2.945590611696243), ('n_updates', 449001), ('average_entropy', -2.766428), ('temperature', 0.029586967080831528)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:460000 episode:3518 last_R: 477.0427637005126 average_R:456.8737625026268\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.20139), ('average_q2', 150.34428), ('average_q_func1_loss', 2.8281408274173736), ('average_q_func2_loss', 2.8805429059267045), ('n_updates', 450001), ('average_entropy', -2.8221114), ('temperature', 0.02966899424791336)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 237 R: 596.430043466881\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 189 R: 608.6821661131631\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 186 R: 586.6801279806781\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 155 R: 471.29606002556386\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 187 R: 590.2973483514488\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 147 R: 435.4919511164441\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 233 R: 552.412981225413\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 189 R: 593.0872636206564\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 153 R: 465.3090520486474\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 168 R: 487.9977911253109\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 213 R: 734.517708548058\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 168 R: 502.70486658760655\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 168 R: 474.5096411955476\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 148 R: 278.66523515274184\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 146 R: 432.7008913810433\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 152 R: 461.8394571371832\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 190 R: 594.7021350359877\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 236 R: 590.007867645775\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 202 R: 633.5144058922251\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 195 R: 610.2464442976639\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 148 R: 276.15997857996865\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 146 R: 432.3786716056756\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 236 R: 584.345682075284\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 148 R: 438.0574949633451\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 150 R: 451.29012487663067\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 212 R: 459.1840194981425\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 147 R: 436.13208402246653\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 238 R: 583.800926424329\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 146 R: 429.34396495685684\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 187 R: 589.5094721384037\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 493.3205617730942 -> 512.7098619029714\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:461000 episode:3524 last_R: 473.6297327260383 average_R:459.49619389302353\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.47269), ('average_q2', 149.58241), ('average_q_func1_loss', 2.763424661755562), ('average_q_func2_loss', 3.041512740254402), ('n_updates', 451001), ('average_entropy', -2.7716875), ('temperature', 0.030714567750692368)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:462000 episode:3530 last_R: 424.9071963595018 average_R:461.47136216450696\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.67436), ('average_q2', 149.28183), ('average_q_func1_loss', 2.5596253299713134), ('average_q_func2_loss', 2.6014778172969817), ('n_updates', 452001), ('average_entropy', -2.857444), ('temperature', 0.029352422803640366)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:463000 episode:3535 last_R: 586.8129466859215 average_R:466.1258703622578\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.30838), ('average_q2', 147.39334), ('average_q_func1_loss', 3.333661959171295), ('average_q_func2_loss', 3.3578310942649843), ('n_updates', 453001), ('average_entropy', -3.0338788), ('temperature', 0.030246077105402946)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:464000 episode:3542 last_R: 442.56168106018316 average_R:471.2764040449694\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.55937), ('average_q2', 146.66064), ('average_q_func1_loss', 2.097798789739609), ('average_q_func2_loss', 2.0422535163164137), ('n_updates', 454001), ('average_entropy', -3.2191143), ('temperature', 0.03054732456803322)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:465000 episode:3548 last_R: 481.8831200565299 average_R:469.1177626721485\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.10649), ('average_q2', 150.13391), ('average_q_func1_loss', 3.0326218891143797), ('average_q_func2_loss', 3.0529048269987107), ('n_updates', 455001), ('average_entropy', -3.2273037), ('temperature', 0.030511585995554924)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 145 R: 432.4349027656217\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 217 R: 537.5667042573089\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 201 R: 615.1575896058822\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 159 R: 489.7717598393058\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 146 R: 432.9648237277341\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 185 R: 569.1378026769947\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 155 R: 454.51896704343073\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 153 R: 434.5644105849897\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 151 R: 427.97371231277873\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 146 R: 427.6849506490901\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 138 R: 280.14393970495274\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 147 R: 438.16886419917853\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 147 R: 428.641215496375\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 147 R: 429.9772851737196\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 155 R: 329.66756116707984\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 158 R: 430.41954853685195\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 161 R: 360.13514387873676\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 146 R: 430.14000537132523\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 164 R: 504.1955636894707\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 209 R: 521.6566346344654\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 153 R: 452.9460449473117\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 159 R: 355.45566730606015\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 146 R: 429.11752044010706\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 145 R: 426.5051860092174\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 192 R: 581.6290986819357\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 145 R: 428.0279614378255\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 147 R: 429.73095586358875\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 195 R: 602.2835868282435\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 222 R: 584.1327246531291\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 145 R: 430.7796356352356\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:466000 episode:3553 last_R: 569.5034802580115 average_R:471.82774070460124\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.00203), ('average_q2', 146.05269), ('average_q_func1_loss', 2.265779581665993), ('average_q_func2_loss', 2.6178305953741074), ('n_updates', 456001), ('average_entropy', -3.1847591), ('temperature', 0.03042651154100895)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:467000 episode:3557 last_R: 578.0579347307628 average_R:475.23943043813705\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.51678), ('average_q2', 147.53316), ('average_q_func1_loss', 2.4823014336824416), ('average_q_func2_loss', 2.4472215294837953), ('n_updates', 457001), ('average_entropy', -2.8977852), ('temperature', 0.0313425250351429)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:468000 episode:3564 last_R: 395.971183995309 average_R:477.87467645142357\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.87093), ('average_q2', 147.75375), ('average_q_func1_loss', 2.9817924815416337), ('average_q_func2_loss', 3.611453176736832), ('n_updates', 458001), ('average_entropy', -2.8652334), ('temperature', 0.030574209988117218)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:469000 episode:3570 last_R: 451.603174732722 average_R:473.86850262499496\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.21967), ('average_q2', 148.9358), ('average_q_func1_loss', 2.3120516967773437), ('average_q_func2_loss', 2.2366547644138337), ('n_updates', 459001), ('average_entropy', -2.8855839), ('temperature', 0.029174121096730232)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:470000 episode:3575 last_R: 632.3533558669931 average_R:479.6437071795936\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.49745), ('average_q2', 149.57855), ('average_q_func1_loss', 2.283303450345993), ('average_q_func2_loss', 2.488742324709892), ('n_updates', 460001), ('average_entropy', -3.0262814), ('temperature', 0.0292680487036705)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 203 R: 436.00976164678565\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 199 R: 605.4057721743774\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 155 R: 454.8571725525645\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 156 R: 466.7231221074121\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 160 R: 485.13608607357617\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 212 R: 658.1077215657201\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 204 R: 429.4908446890475\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 196 R: 605.0935877607848\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 220 R: 487.8426724254915\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 199 R: 600.6978085417836\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 145 R: 422.5784724581723\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 210 R: 454.50786466518207\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 187 R: 598.2994230039517\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 185 R: 360.05934697063924\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 145 R: 426.3396793564265\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 146 R: 428.3251251429565\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 201 R: 410.195847147363\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 146 R: 426.07560463493036\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 192 R: 398.2772251284872\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 236 R: 584.4025914064333\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 153 R: 441.44765288451975\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 146 R: 425.56083451261924\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 154 R: 448.11329794611987\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 165 R: 486.2206661550287\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 220 R: 477.83057183562994\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 148 R: 437.90151768446924\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 210 R: 449.7704429760991\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 200 R: 623.4579704368989\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 215 R: 474.0903531305399\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 203 R: 614.4822519909113\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:471000 episode:3580 last_R: 428.22150183227564 average_R:480.8191504153914\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.17017), ('average_q2', 145.1016), ('average_q_func1_loss', 2.569623007774353), ('average_q_func2_loss', 2.6088460302352905), ('n_updates', 461001), ('average_entropy', -3.0583568), ('temperature', 0.03048034943640232)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:472000 episode:3585 last_R: 594.1031226399327 average_R:481.40400914836886\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.64671), ('average_q2', 147.9469), ('average_q_func1_loss', 3.796527078151703), ('average_q_func2_loss', 3.134137404561043), ('n_updates', 462001), ('average_entropy', -2.947459), ('temperature', 0.029263896867632866)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:473000 episode:3590 last_R: 580.5728447757766 average_R:486.1381529475343\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.06691), ('average_q2', 146.3434), ('average_q_func1_loss', 2.4999895811080934), ('average_q_func2_loss', 2.4128250908851623), ('n_updates', 463001), ('average_entropy', -3.107993), ('temperature', 0.02973492443561554)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:474000 episode:3596 last_R: 410.8144547233059 average_R:484.9837265394788\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.23503), ('average_q2', 149.08528), ('average_q_func1_loss', 2.954995933175087), ('average_q_func2_loss', 2.9209167897701263), ('n_updates', 464001), ('average_entropy', -2.8251047), ('temperature', 0.030016383156180382)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:475000 episode:3601 last_R: 570.0465581933793 average_R:491.60557600223575\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 146.8125), ('average_q2', 146.67325), ('average_q_func1_loss', 1.957352357506752), ('average_q_func2_loss', 2.1648102647066114), ('n_updates', 465001), ('average_entropy', -3.0876536), ('temperature', 0.030073875561356544)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 149 R: 433.45834927976574\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 208 R: 535.8195722580042\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 153 R: 446.254387144772\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 192 R: 585.4139872880351\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 151 R: 436.56850299347786\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 183 R: 572.1711279021758\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 159 R: 480.5625731526878\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 212 R: 566.067852067768\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 225 R: 560.5785982324098\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 219 R: 523.2548894721465\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 194 R: 586.9927950575468\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 150 R: 439.37465580084665\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 191 R: 607.7606975318445\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 223 R: 521.5367628536457\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 225 R: 600.2618011145847\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 190 R: 611.3967961479676\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 191 R: 580.2282585925736\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 223 R: 567.0865675438467\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 189 R: 578.1886465443628\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 189 R: 591.5566456577869\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 149 R: 432.6725365643018\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 229 R: 567.0570453429584\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 147 R: 428.70630244762947\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 149 R: 429.8760867365155\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 160 R: 512.7340602713705\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 149 R: 433.77585106083586\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 148 R: 430.717493753448\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 153 R: 469.4382913389423\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 148 R: 430.5319178740034\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 227 R: 547.7305439335242\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 512.7098619029714 -> 516.9257865319926\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:476000 episode:3607 last_R: 428.84672949423225 average_R:486.1975226793761\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.97145), ('average_q2', 151.02655), ('average_q_func1_loss', 2.50722628057003), ('average_q_func2_loss', 2.598720704317093), ('n_updates', 466001), ('average_entropy', -2.842315), ('temperature', 0.030009856447577477)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:477000 episode:3613 last_R: 594.3617089051565 average_R:490.07550381352064\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.61775), ('average_q2', 147.51566), ('average_q_func1_loss', 3.4689492702484133), ('average_q_func2_loss', 3.7184536337852476), ('n_updates', 467001), ('average_entropy', -2.8454897), ('temperature', 0.02969677932560444)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:478000 episode:3618 last_R: 399.4313749078352 average_R:490.70207883760827\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.2842), ('average_q2', 149.43707), ('average_q_func1_loss', 2.730431783795357), ('average_q_func2_loss', 2.7535075795650483), ('n_updates', 468001), ('average_entropy', -2.9527864), ('temperature', 0.029523318633437157)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:479000 episode:3624 last_R: 445.0363076614984 average_R:488.7672900200705\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.52019), ('average_q2', 149.29173), ('average_q_func1_loss', 2.210782807469368), ('average_q_func2_loss', 2.3339021027088167), ('n_updates', 469001), ('average_entropy', -2.8472733), ('temperature', 0.02972552925348282)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:480000 episode:3630 last_R: 435.7476187042014 average_R:490.0612627310746\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.33566), ('average_q2', 149.42853), ('average_q_func1_loss', 2.2637453019618987), ('average_q_func2_loss', 2.629564046263695), ('n_updates', 470001), ('average_entropy', -2.8622897), ('temperature', 0.029424594715237617)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 187 R: 585.1424797086306\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 231 R: 601.4970934314543\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 234 R: 635.899044257628\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 186 R: 581.0124261287106\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 146 R: 430.9382683555453\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 153 R: 463.1928174886584\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 228 R: 603.2220821089833\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 148 R: 442.5445410668462\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 230 R: 604.6516937522691\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 216 R: 519.0326233025786\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 187 R: 584.3076667129798\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 154 R: 456.05708644101355\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 148 R: 439.55678314142637\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 230 R: 605.1775315762914\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 154 R: 471.9264606257037\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 152 R: 447.7546181818547\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 219 R: 554.8012996382677\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 147 R: 427.73800379244057\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 152 R: 456.9994606382024\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 186 R: 581.6416339873931\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 234 R: 619.3656592328213\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 218 R: 541.025170644828\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 241 R: 643.6155830517688\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 148 R: 441.3799196865627\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 152 R: 455.97554867911043\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 223 R: 578.3448540317798\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 239 R: 608.5267145025813\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 221 R: 556.1675120920555\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 148 R: 442.29600884155167\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 217 R: 541.298819083298\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 516.9257865319926 -> 530.7029801394411\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:481000 episode:3636 last_R: 466.2939465314916 average_R:488.5588537074359\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.2118), ('average_q2', 148.2097), ('average_q_func1_loss', 2.3989797073602674), ('average_q_func2_loss', 2.4610735177993774), ('n_updates', 471001), ('average_entropy', -2.7914476), ('temperature', 0.029082613065838814)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:482000 episode:3640 last_R: 661.817898945676 average_R:490.6293772973874\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.61378), ('average_q2', 148.55574), ('average_q_func1_loss', 2.287557796239853), ('average_q_func2_loss', 2.6487035262584686), ('n_updates', 472001), ('average_entropy', -2.951741), ('temperature', 0.030652664601802826)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:483000 episode:3645 last_R: 423.5027303263372 average_R:495.3670865300436\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.86072), ('average_q2', 148.79205), ('average_q_func1_loss', 2.4001468348503114), ('average_q_func2_loss', 2.545794548392296), ('n_updates', 473001), ('average_entropy', -3.1245728), ('temperature', 0.02999204769730568)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:484000 episode:3651 last_R: 428.03361462198006 average_R:497.3950123577251\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.29561), ('average_q2', 148.63426), ('average_q_func1_loss', 2.2320543986558916), ('average_q_func2_loss', 2.3877369701862334), ('n_updates', 474001), ('average_entropy', -3.125997), ('temperature', 0.03005203604698181)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:485000 episode:3657 last_R: 438.48222474966144 average_R:498.2035153206991\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.15863), ('average_q2', 150.01097), ('average_q_func1_loss', 2.6373209500312806), ('average_q_func2_loss', 3.03772125184536), ('n_updates', 475001), ('average_entropy', -3.0555832), ('temperature', 0.029739586636424065)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 147 R: 429.59461790354555\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 180 R: 565.7667064867732\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 159 R: 504.02142306602286\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 159 R: 507.0173312906789\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 154 R: 383.30173458483443\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 144 R: 425.8772014874519\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 177 R: 529.9082517604958\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 157 R: 502.40735607154\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 151 R: 371.84928122285544\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 143 R: 421.2658997121196\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 139 R: 306.00870606246286\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 156 R: 482.4934860688212\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 181 R: 567.0213872089251\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 150 R: 427.3246472944299\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 149 R: 363.61416149244434\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 173 R: 518.5771920202435\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 145 R: 426.42275311161836\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 148 R: 445.50897816856315\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 150 R: 353.93251465433514\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 156 R: 400.00203944016874\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 144 R: 423.9198609949272\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 151 R: 359.261343330858\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 145 R: 425.4262049556816\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 148 R: 431.12821243415266\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 150 R: 441.4492271268613\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 145 R: 425.3812113358641\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 145 R: 318.18922073438347\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 175 R: 551.6898557330395\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 148 R: 435.70166573343266\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 173 R: 542.6711124989404\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:486000 episode:3663 last_R: 483.33034245192096 average_R:496.85411362090025\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.77919), ('average_q2', 149.90154), ('average_q_func1_loss', 2.6536254197359086), ('average_q_func2_loss', 2.544043456315994), ('n_updates', 476001), ('average_entropy', -3.042703), ('temperature', 0.029576735571026802)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:487000 episode:3669 last_R: 421.3349317516857 average_R:500.10607051133263\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 152.8633), ('average_q2', 153.08687), ('average_q_func1_loss', 2.2847449773550035), ('average_q_func2_loss', 2.2853177160024645), ('n_updates', 477001), ('average_entropy', -2.954183), ('temperature', 0.029400240629911423)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:488000 episode:3675 last_R: 249.11697058757014 average_R:496.19124822305594\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.74377), ('average_q2', 148.94469), ('average_q_func1_loss', 2.2438059604167937), ('average_q_func2_loss', 2.3406368219852447), ('n_updates', 478001), ('average_entropy', -3.1753926), ('temperature', 0.02873653918504715)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:489000 episode:3683 last_R: 507.5580685869601 average_R:494.9195205931111\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.13751), ('average_q2', 151.24194), ('average_q_func1_loss', 2.4154208910465242), ('average_q_func2_loss', 2.5546810007095337), ('n_updates', 479001), ('average_entropy', -2.8090694), ('temperature', 0.029767045751214027)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:490000 episode:3688 last_R: 617.3962402999081 average_R:492.79775782390783\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.40329), ('average_q2', 148.1513), ('average_q_func1_loss', 2.8455655753612517), ('average_q_func2_loss', 3.0491160881519317), ('n_updates', 480001), ('average_entropy', -2.8295534), ('temperature', 0.030285440385341644)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 145 R: 430.9929337043237\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 173 R: 421.30963571239187\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 150 R: 444.5818529128281\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 146 R: 433.40035033011577\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 205 R: 653.2381515101654\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 144 R: 425.0725904265382\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 174 R: 441.3406695619405\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 144 R: 426.74602080290066\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 176 R: 392.1002152813558\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 193 R: 603.6008174712093\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 187 R: 480.60464774385156\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 199 R: 635.4491341959097\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 151 R: 458.9376492722458\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 196 R: 613.223063771603\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 146 R: 338.8731545328486\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 182 R: 589.3033426319558\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 128 R: 264.5020470160787\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 195 R: 610.7685948550771\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 147 R: 426.69219137227225\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 144 R: 424.637422131963\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 177 R: 428.39014489161275\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 144 R: 425.6219945640284\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 200 R: 637.9400821724657\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 146 R: 380.75056064446073\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 154 R: 462.64546470482264\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 152 R: 361.6684873308361\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 192 R: 595.9440164080659\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 161 R: 426.4071247668056\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 145 R: 329.2935786801814\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 177 R: 574.3782845786334\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:491000 episode:3693 last_R: 439.392012087079 average_R:490.99732470312995\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.85358), ('average_q2', 151.75557), ('average_q_func1_loss', 2.409732230305672), ('average_q_func2_loss', 2.653703454732895), ('n_updates', 481001), ('average_entropy', -2.8852322), ('temperature', 0.030721696093678474)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:492000 episode:3699 last_R: 428.77917286105463 average_R:490.1287021351135\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.00107), ('average_q2', 148.93806), ('average_q_func1_loss', 2.9120830303430556), ('average_q_func2_loss', 2.67130556166172), ('n_updates', 482001), ('average_entropy', -2.9873598), ('temperature', 0.029400745406746864)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:493000 episode:3707 last_R: 432.23991062309807 average_R:486.5390260545927\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.5774), ('average_q2', 148.74852), ('average_q_func1_loss', 2.564891201257706), ('average_q_func2_loss', 2.7554900151491166), ('n_updates', 483001), ('average_entropy', -3.2631693), ('temperature', 0.029524443671107292)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:494000 episode:3713 last_R: 565.1159101085054 average_R:486.6678695750333\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 148.47847), ('average_q2', 148.20876), ('average_q_func1_loss', 2.2834161454439164), ('average_q_func2_loss', 2.634845367670059), ('n_updates', 484001), ('average_entropy', -3.0227547), ('temperature', 0.029574308544397354)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:495000 episode:3718 last_R: 525.3354586790925 average_R:487.69883730509764\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.74615), ('average_q2', 150.54253), ('average_q_func1_loss', 2.4448703598976134), ('average_q_func2_loss', 2.8448221665620803), ('n_updates', 485001), ('average_entropy', -2.850182), ('temperature', 0.028827449306845665)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 260 R: 614.9758508240246\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 157 R: 482.0502496313607\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 143 R: 413.7053629534431\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 221 R: 506.9044470063018\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 158 R: 485.71454433134755\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 142 R: 412.6180174903245\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 259 R: 597.5337123941508\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 274 R: 635.0260355594795\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 170 R: 542.5061572530768\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 144 R: 418.79138019554244\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 179 R: 579.0642389542005\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 256 R: 600.1780977586433\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 143 R: 418.76927854652627\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 221 R: 505.6077954734213\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 217 R: 505.0315115441734\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 265 R: 651.6378870149761\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 160 R: 493.34243173910875\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 227 R: 599.023033589701\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 255 R: 593.7515162472413\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 170 R: 534.6728221233553\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 176 R: 572.5776374353864\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 229 R: 604.1541018671248\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 164 R: 524.1224247786827\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 233 R: 530.403218657923\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 262 R: 625.6609890253903\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 227 R: 500.6687061243694\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 270 R: 602.8014283498777\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 176 R: 569.3263217552188\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 142 R: 413.73084096421417\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 230 R: 613.4648552191385\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 530.7029801394411 -> 538.2604964935908\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:496000 episode:3725 last_R: 468.26134134133173 average_R:488.15141073970983\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.42755), ('average_q2', 151.47504), ('average_q_func1_loss', 2.6203300392627717), ('average_q_func2_loss', 2.6511077231168745), ('n_updates', 486001), ('average_entropy', -2.8957608), ('temperature', 0.029228566214442253)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:497000 episode:3732 last_R: 443.58819763015515 average_R:489.84955790811773\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 151.66867), ('average_q2', 151.71358), ('average_q_func1_loss', 2.4982676029205324), ('average_q_func2_loss', 2.550747867822647), ('n_updates', 487001), ('average_entropy', -2.9877853), ('temperature', 0.029377272352576256)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:498000 episode:3736 last_R: 435.9542488646416 average_R:489.7415230150176\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.69347), ('average_q2', 149.62807), ('average_q_func1_loss', 3.299210923910141), ('average_q_func2_loss', 3.3612372732162474), ('n_updates', 488001), ('average_entropy', -3.020742), ('temperature', 0.029657183215022087)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:499000 episode:3741 last_R: 396.0508145877677 average_R:485.45270217757485\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 153.72223), ('average_q2', 153.76935), ('average_q_func1_loss', 2.6467401188611985), ('average_q_func2_loss', 2.7023702335357664), ('n_updates', 489001), ('average_entropy', -3.0122035), ('temperature', 0.02925574965775013)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/mixed step:500000 episode:3749 last_R: 423.67835654396566 average_R:481.95484055036184\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 150.9219), ('average_q2', 150.9959), ('average_q_func1_loss', 2.7044841969013214), ('average_q_func2_loss', 2.836162796020508), ('n_updates', 490001), ('average_entropy', -3.1638026), ('temperature', 0.02952825278043747)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 149 R: 431.5095480513759\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 147 R: 429.7626773563116\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 154 R: 463.5874397130576\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 223 R: 555.1293510502555\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 148 R: 429.59630483467754\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 153 R: 454.0328568233265\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 213 R: 556.5457575285031\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 236 R: 574.807282762484\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 147 R: 429.6722364948251\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 148 R: 432.23043097045786\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 160 R: 452.82019943755125\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 182 R: 575.3252449856852\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 157 R: 476.65308058835103\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 237 R: 618.2543408836792\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 259 R: 642.484370248157\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 194 R: 588.8579066155958\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 230 R: 585.0085013567942\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 193 R: 458.80224774441973\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 172 R: 412.7980922805529\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 224 R: 588.0039826752806\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 168 R: 368.91362912329475\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 148 R: 430.4490134037507\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 241 R: 628.3634993129397\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 155 R: 469.3589278326399\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 147 R: 427.81039071911783\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 239 R: 586.4411106171423\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 208 R: 544.481058665908\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 148 R: 428.3202761416752\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 148 R: 420.785489409583\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 148 R: 430.9740060980522\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/mixed/500000_finish\n"]},{"output_type":"execute_result","data":{"text/plain":["(<pfrl.agents.soft_actor_critic.SoftActorCritic at 0x7d4ef8c0ebc0>,\n"," [{'average_q1': nan,\n","   'average_q2': nan,\n","   'average_q_func1_loss': nan,\n","   'average_q_func2_loss': nan,\n","   'n_updates': 0,\n","   'average_entropy': nan,\n","   'temperature': 1.0,\n","   'eval_score': 11.623751329837196},\n","  {'average_q1': -0.17558007,\n","   'average_q2': -0.23542228,\n","   'average_q_func1_loss': 3.1768946647644043,\n","   'average_q_func2_loss': 3.3060193061828613,\n","   'n_updates': 1,\n","   'average_entropy': 1.5813969,\n","   'temperature': 0.9997000694274902,\n","   'eval_score': 12.293270344066988},\n","  {'average_q1': 46.819084,\n","   'average_q2': 46.875404,\n","   'average_q_func1_loss': 3.7903609001636505,\n","   'average_q_func2_loss': 3.8519053626060487,\n","   'n_updates': 5001,\n","   'average_entropy': -0.41934192,\n","   'temperature': 0.32523313164711,\n","   'eval_score': 167.74740440768662},\n","  {'average_q1': 80.97967,\n","   'average_q2': 81.0113,\n","   'average_q_func1_loss': 7.401262819766998,\n","   'average_q_func2_loss': 7.123835413455963,\n","   'n_updates': 10001,\n","   'average_entropy': -1.921351,\n","   'temperature': 0.13190008699893951,\n","   'eval_score': 220.10071363208425},\n","  {'average_q1': 104.10988,\n","   'average_q2': 104.013954,\n","   'average_q_func1_loss': 9.586803798675538,\n","   'average_q_func2_loss': 9.440652923583984,\n","   'n_updates': 15001,\n","   'average_entropy': -2.9215157,\n","   'temperature': 0.08157888799905777,\n","   'eval_score': 297.3891227170098},\n","  {'average_q1': 114.342445,\n","   'average_q2': 114.15969,\n","   'average_q_func1_loss': 9.052737379074097,\n","   'average_q_func2_loss': 8.876459584236144,\n","   'n_updates': 20001,\n","   'average_entropy': -3.1683161,\n","   'temperature': 0.0845397561788559,\n","   'eval_score': 299.3162995038213},\n","  {'average_q1': 124.21359,\n","   'average_q2': 124.17542,\n","   'average_q_func1_loss': 10.370877277851104,\n","   'average_q_func2_loss': 10.110443425178527,\n","   'n_updates': 25001,\n","   'average_entropy': -2.8208227,\n","   'temperature': 0.08254407346248627,\n","   'eval_score': 275.12024379147454},\n","  {'average_q1': 128.00124,\n","   'average_q2': 128.50717,\n","   'average_q_func1_loss': 9.480453732013702,\n","   'average_q_func2_loss': 9.263550608158111,\n","   'n_updates': 30001,\n","   'average_entropy': -3.0707881,\n","   'temperature': 0.07162618637084961,\n","   'eval_score': 298.6977132066535},\n","  {'average_q1': 124.457375,\n","   'average_q2': 124.4998,\n","   'average_q_func1_loss': 8.409743783473969,\n","   'average_q_func2_loss': 8.320825901031494,\n","   'n_updates': 35001,\n","   'average_entropy': -3.2050128,\n","   'temperature': 0.07797486335039139,\n","   'eval_score': 317.46875101028536},\n","  {'average_q1': 125.938484,\n","   'average_q2': 125.71501,\n","   'average_q_func1_loss': 6.86320969581604,\n","   'average_q_func2_loss': 6.664995210170746,\n","   'n_updates': 40001,\n","   'average_entropy': -3.0765018,\n","   'temperature': 0.06769993156194687,\n","   'eval_score': 318.92098774672826},\n","  {'average_q1': 124.089,\n","   'average_q2': 124.15546,\n","   'average_q_func1_loss': 7.6001570868492125,\n","   'average_q_func2_loss': 7.814398910999298,\n","   'n_updates': 45001,\n","   'average_entropy': -3.0278785,\n","   'temperature': 0.05726976320147514,\n","   'eval_score': 298.15518875210904},\n","  {'average_q1': 122.78739,\n","   'average_q2': 123.10428,\n","   'average_q_func1_loss': 5.6164488077163695,\n","   'average_q_func2_loss': 5.289942495822906,\n","   'n_updates': 50001,\n","   'average_entropy': -2.8908067,\n","   'temperature': 0.052786312997341156,\n","   'eval_score': 323.36925169598675},\n","  {'average_q1': 121.29489,\n","   'average_q2': 121.24704,\n","   'average_q_func1_loss': 5.059508438110352,\n","   'average_q_func2_loss': 5.151977744102478,\n","   'n_updates': 55001,\n","   'average_entropy': -3.1606257,\n","   'temperature': 0.054202355444431305,\n","   'eval_score': 326.5095516353058},\n","  {'average_q1': 122.391014,\n","   'average_q2': 122.50424,\n","   'average_q_func1_loss': 6.526731491088867,\n","   'average_q_func2_loss': 6.317715957164764,\n","   'n_updates': 60001,\n","   'average_entropy': -3.0337033,\n","   'temperature': 0.04775659367442131,\n","   'eval_score': 110.96887850661429},\n","  {'average_q1': 122.93949,\n","   'average_q2': 123.109985,\n","   'average_q_func1_loss': 8.743790230751038,\n","   'average_q_func2_loss': 8.843002891540527,\n","   'n_updates': 65001,\n","   'average_entropy': -2.9866068,\n","   'temperature': 0.049122344702482224,\n","   'eval_score': 200.7128415474124},\n","  {'average_q1': 121.51975,\n","   'average_q2': 121.386185,\n","   'average_q_func1_loss': 6.483057370185852,\n","   'average_q_func2_loss': 6.565099052190781,\n","   'n_updates': 70001,\n","   'average_entropy': -3.1841202,\n","   'temperature': 0.0493968203663826,\n","   'eval_score': 336.02095736705127},\n","  {'average_q1': 122.905464,\n","   'average_q2': 123.02214,\n","   'average_q_func1_loss': 5.355314695835114,\n","   'average_q_func2_loss': 5.552821037769317,\n","   'n_updates': 75001,\n","   'average_entropy': -3.0706518,\n","   'temperature': 0.04644804075360298,\n","   'eval_score': 323.78849506767574},\n","  {'average_q1': 122.26673,\n","   'average_q2': 122.516884,\n","   'average_q_func1_loss': 5.607533845901489,\n","   'average_q_func2_loss': 5.621132576465607,\n","   'n_updates': 80001,\n","   'average_entropy': -3.0149727,\n","   'temperature': 0.05050103738903999,\n","   'eval_score': 334.11875916559904},\n","  {'average_q1': 121.09882,\n","   'average_q2': 120.987656,\n","   'average_q_func1_loss': 4.524137527942657,\n","   'average_q_func2_loss': 4.4901178467273715,\n","   'n_updates': 85001,\n","   'average_entropy': -2.7568157,\n","   'temperature': 0.04485511779785156,\n","   'eval_score': 374.59941031913354},\n","  {'average_q1': 124.731995,\n","   'average_q2': 124.7424,\n","   'average_q_func1_loss': 3.789105545282364,\n","   'average_q_func2_loss': 3.6626888871192933,\n","   'n_updates': 90001,\n","   'average_entropy': -3.1919453,\n","   'temperature': 0.04494844749569893,\n","   'eval_score': 336.2851126068845},\n","  {'average_q1': 122.594986,\n","   'average_q2': 122.79432,\n","   'average_q_func1_loss': 4.34410706281662,\n","   'average_q_func2_loss': 3.9274955415725707,\n","   'n_updates': 95001,\n","   'average_entropy': -2.9433465,\n","   'temperature': 0.04007555916905403,\n","   'eval_score': 357.35977928422005},\n","  {'average_q1': 124.31445,\n","   'average_q2': 124.39586,\n","   'average_q_func1_loss': 4.010192065238953,\n","   'average_q_func2_loss': 3.5577271962165833,\n","   'n_updates': 100001,\n","   'average_entropy': -3.12465,\n","   'temperature': 0.039695121347904205,\n","   'eval_score': 343.23527316832326},\n","  {'average_q1': 124.40661,\n","   'average_q2': 124.55679,\n","   'average_q_func1_loss': 3.0845156574249266,\n","   'average_q_func2_loss': 3.0997548377513886,\n","   'n_updates': 105001,\n","   'average_entropy': -2.869399,\n","   'temperature': 0.038742225617170334,\n","   'eval_score': 355.39585468564},\n","  {'average_q1': 126.161156,\n","   'average_q2': 126.08895,\n","   'average_q_func1_loss': 3.003091285228729,\n","   'average_q_func2_loss': 3.3248129427433013,\n","   'n_updates': 110001,\n","   'average_entropy': -2.9622731,\n","   'temperature': 0.036003243178129196,\n","   'eval_score': 362.22900413124034},\n","  {'average_q1': 129.2514,\n","   'average_q2': 128.89598,\n","   'average_q_func1_loss': 3.283387280702591,\n","   'average_q_func2_loss': 3.2576371169090272,\n","   'n_updates': 115001,\n","   'average_entropy': -2.916414,\n","   'temperature': 0.03390228748321533,\n","   'eval_score': 328.8604861698316},\n","  {'average_q1': 130.16127,\n","   'average_q2': 130.15392,\n","   'average_q_func1_loss': 3.856102274656296,\n","   'average_q_func2_loss': 3.686848638057709,\n","   'n_updates': 120001,\n","   'average_entropy': -2.935104,\n","   'temperature': 0.034408748149871826,\n","   'eval_score': 332.63599175242416},\n","  {'average_q1': 127.64702,\n","   'average_q2': 127.84229,\n","   'average_q_func1_loss': 3.139889750480652,\n","   'average_q_func2_loss': 3.2599162459373474,\n","   'n_updates': 125001,\n","   'average_entropy': -3.1534848,\n","   'temperature': 0.033919814974069595,\n","   'eval_score': 351.00187422776236},\n","  {'average_q1': 128.30444,\n","   'average_q2': 127.90416,\n","   'average_q_func1_loss': 2.8420016288757326,\n","   'average_q_func2_loss': 2.688931342959404,\n","   'n_updates': 130001,\n","   'average_entropy': -2.966671,\n","   'temperature': 0.03364962339401245,\n","   'eval_score': 358.9929288620367},\n","  {'average_q1': 130.26639,\n","   'average_q2': 130.1317,\n","   'average_q_func1_loss': 3.1645715606212614,\n","   'average_q_func2_loss': 3.1238716822862624,\n","   'n_updates': 135001,\n","   'average_entropy': -2.9932604,\n","   'temperature': 0.03280836343765259,\n","   'eval_score': 367.94035026742415},\n","  {'average_q1': 129.15472,\n","   'average_q2': 129.14294,\n","   'average_q_func1_loss': 3.2561815083026886,\n","   'average_q_func2_loss': 3.484640417098999,\n","   'n_updates': 140001,\n","   'average_entropy': -3.0013032,\n","   'temperature': 0.032060716301202774,\n","   'eval_score': 369.5250253614099},\n","  {'average_q1': 132.23712,\n","   'average_q2': 132.35825,\n","   'average_q_func1_loss': 2.5730197119712828,\n","   'average_q_func2_loss': 2.693130419254303,\n","   'n_updates': 145001,\n","   'average_entropy': -2.8541594,\n","   'temperature': 0.031910575926303864,\n","   'eval_score': 374.69441210708317},\n","  {'average_q1': 130.1832,\n","   'average_q2': 129.95709,\n","   'average_q_func1_loss': 3.406732716560364,\n","   'average_q_func2_loss': 3.4972707921266557,\n","   'n_updates': 150001,\n","   'average_entropy': -2.9848585,\n","   'temperature': 0.03128320351243019,\n","   'eval_score': 346.59841854134305},\n","  {'average_q1': 135.2872,\n","   'average_q2': 134.97041,\n","   'average_q_func1_loss': 2.451434463262558,\n","   'average_q_func2_loss': 2.424351145029068,\n","   'n_updates': 155001,\n","   'average_entropy': -2.921554,\n","   'temperature': 0.02965972200036049,\n","   'eval_score': 340.8779279030156},\n","  {'average_q1': 131.03117,\n","   'average_q2': 130.72006,\n","   'average_q_func1_loss': 2.7928471970558166,\n","   'average_q_func2_loss': 2.919542702436447,\n","   'n_updates': 160001,\n","   'average_entropy': -2.833094,\n","   'temperature': 0.02964065596461296,\n","   'eval_score': 341.85621856777743},\n","  {'average_q1': 132.90448,\n","   'average_q2': 133.00786,\n","   'average_q_func1_loss': 3.0461021292209627,\n","   'average_q_func2_loss': 3.14154527425766,\n","   'n_updates': 165001,\n","   'average_entropy': -2.9712794,\n","   'temperature': 0.02792862243950367,\n","   'eval_score': 355.97648182751135},\n","  {'average_q1': 129.3101,\n","   'average_q2': 129.35883,\n","   'average_q_func1_loss': 2.718321130871773,\n","   'average_q_func2_loss': 2.6559742420911787,\n","   'n_updates': 170001,\n","   'average_entropy': -2.941643,\n","   'temperature': 0.0265815369784832,\n","   'eval_score': 373.8210323535201},\n","  {'average_q1': 133.57545,\n","   'average_q2': 133.38843,\n","   'average_q_func1_loss': 2.571600821018219,\n","   'average_q_func2_loss': 2.505224162340164,\n","   'n_updates': 175001,\n","   'average_entropy': -2.975396,\n","   'temperature': 0.028093475848436356,\n","   'eval_score': 349.40737817395507},\n","  {'average_q1': 136.15244,\n","   'average_q2': 136.12619,\n","   'average_q_func1_loss': 3.392705979347229,\n","   'average_q_func2_loss': 3.5924385631084443,\n","   'n_updates': 180001,\n","   'average_entropy': -3.0614805,\n","   'temperature': 0.02865368127822876,\n","   'eval_score': 344.622824041543},\n","  {'average_q1': 134.1587,\n","   'average_q2': 134.25511,\n","   'average_q_func1_loss': 2.8239586663246157,\n","   'average_q_func2_loss': 3.078362276554108,\n","   'n_updates': 185001,\n","   'average_entropy': -2.9860575,\n","   'temperature': 0.029330557212233543,\n","   'eval_score': 381.2567150766769},\n","  {'average_q1': 137.18556,\n","   'average_q2': 137.17078,\n","   'average_q_func1_loss': 2.5513299143314363,\n","   'average_q_func2_loss': 2.7128352117538452,\n","   'n_updates': 190001,\n","   'average_entropy': -3.119161,\n","   'temperature': 0.02901582606136799,\n","   'eval_score': 343.7455732499136},\n","  {'average_q1': 135.97882,\n","   'average_q2': 135.82204,\n","   'average_q_func1_loss': 2.3893919438123703,\n","   'average_q_func2_loss': 2.493602575659752,\n","   'n_updates': 195001,\n","   'average_entropy': -2.8678198,\n","   'temperature': 0.029187461361289024,\n","   'eval_score': 382.84387479515226},\n","  {'average_q1': 136.35138,\n","   'average_q2': 136.349,\n","   'average_q_func1_loss': 2.618466727733612,\n","   'average_q_func2_loss': 2.6019130718708037,\n","   'n_updates': 200001,\n","   'average_entropy': -2.9197328,\n","   'temperature': 0.028825057670474052,\n","   'eval_score': 374.87014785871463},\n","  {'average_q1': 137.651,\n","   'average_q2': 137.81071,\n","   'average_q_func1_loss': 2.528417972922325,\n","   'average_q_func2_loss': 2.583783769607544,\n","   'n_updates': 205001,\n","   'average_entropy': -3.1517603,\n","   'temperature': 0.027145830914378166,\n","   'eval_score': 377.04634362357945},\n","  {'average_q1': 137.345,\n","   'average_q2': 137.19904,\n","   'average_q_func1_loss': 2.883717169761658,\n","   'average_q_func2_loss': 2.975818548798561,\n","   'n_updates': 210001,\n","   'average_entropy': -2.9587278,\n","   'temperature': 0.029021892696619034,\n","   'eval_score': 374.538827871116},\n","  {'average_q1': 136.70398,\n","   'average_q2': 136.6355,\n","   'average_q_func1_loss': 2.533043143749237,\n","   'average_q_func2_loss': 2.4976057744026186,\n","   'n_updates': 215001,\n","   'average_entropy': -2.8927398,\n","   'temperature': 0.02928290143609047,\n","   'eval_score': 420.2552705338256},\n","  {'average_q1': 133.92184,\n","   'average_q2': 133.88844,\n","   'average_q_func1_loss': 2.2990210485458373,\n","   'average_q_func2_loss': 2.4772045743465423,\n","   'n_updates': 220001,\n","   'average_entropy': -3.1088746,\n","   'temperature': 0.0285708736628294,\n","   'eval_score': 367.8267338965971},\n","  {'average_q1': 139.42328,\n","   'average_q2': 139.3344,\n","   'average_q_func1_loss': 2.38913289308548,\n","   'average_q_func2_loss': 2.5059533417224884,\n","   'n_updates': 225001,\n","   'average_entropy': -3.0265894,\n","   'temperature': 0.029247060418128967,\n","   'eval_score': 403.2838328576334},\n","  {'average_q1': 134.61128,\n","   'average_q2': 134.3978,\n","   'average_q_func1_loss': 3.2116761469841,\n","   'average_q_func2_loss': 3.183298841714859,\n","   'n_updates': 230001,\n","   'average_entropy': -3.11626,\n","   'temperature': 0.030153872445225716,\n","   'eval_score': 281.5510254312784},\n","  {'average_q1': 137.28357,\n","   'average_q2': 137.39893,\n","   'average_q_func1_loss': 2.3167739158868788,\n","   'average_q_func2_loss': 2.4073303627967833,\n","   'n_updates': 235001,\n","   'average_entropy': -3.094393,\n","   'temperature': 0.02954932302236557,\n","   'eval_score': 387.85691451547444},\n","  {'average_q1': 136.37646,\n","   'average_q2': 136.59715,\n","   'average_q_func1_loss': 2.9253100997209547,\n","   'average_q_func2_loss': 2.7066846030950544,\n","   'n_updates': 240001,\n","   'average_entropy': -3.016057,\n","   'temperature': 0.028239622712135315,\n","   'eval_score': 382.3633387911095},\n","  {'average_q1': 139.11737,\n","   'average_q2': 138.73392,\n","   'average_q_func1_loss': 2.6079494255781173,\n","   'average_q_func2_loss': 2.691927888393402,\n","   'n_updates': 245001,\n","   'average_entropy': -2.831535,\n","   'temperature': 0.028197357431054115,\n","   'eval_score': 339.0304034314087},\n","  {'average_q1': 136.44272,\n","   'average_q2': 136.43422,\n","   'average_q_func1_loss': 2.7679304033517838,\n","   'average_q_func2_loss': 2.885180978775024,\n","   'n_updates': 250001,\n","   'average_entropy': -2.9190557,\n","   'temperature': 0.02773222327232361,\n","   'eval_score': 358.9323220951544},\n","  {'average_q1': 137.26294,\n","   'average_q2': 137.2768,\n","   'average_q_func1_loss': 3.4037986838817598,\n","   'average_q_func2_loss': 3.3965057796239853,\n","   'n_updates': 255001,\n","   'average_entropy': -3.3518047,\n","   'temperature': 0.026980701833963394,\n","   'eval_score': 408.59742795545645},\n","  {'average_q1': 139.19505,\n","   'average_q2': 139.2579,\n","   'average_q_func1_loss': 2.540613917708397,\n","   'average_q_func2_loss': 2.683194839954376,\n","   'n_updates': 260001,\n","   'average_entropy': -3.0759566,\n","   'temperature': 0.026527229696512222,\n","   'eval_score': 387.21679882467714},\n","  {'average_q1': 137.46031,\n","   'average_q2': 137.54816,\n","   'average_q_func1_loss': 2.188286625146866,\n","   'average_q_func2_loss': 2.3860744082927705,\n","   'n_updates': 265001,\n","   'average_entropy': -3.170018,\n","   'temperature': 0.026774056255817413,\n","   'eval_score': 360.9266338584473},\n","  {'average_q1': 139.00638,\n","   'average_q2': 139.1618,\n","   'average_q_func1_loss': 2.0604012912511824,\n","   'average_q_func2_loss': 2.105603654384613,\n","   'n_updates': 270001,\n","   'average_entropy': -2.8769987,\n","   'temperature': 0.026208441704511642,\n","   'eval_score': 375.9208343724824},\n","  {'average_q1': 140.27287,\n","   'average_q2': 140.16498,\n","   'average_q_func1_loss': 2.127838976383209,\n","   'average_q_func2_loss': 2.1998597884178164,\n","   'n_updates': 275001,\n","   'average_entropy': -2.9111564,\n","   'temperature': 0.02546551823616028,\n","   'eval_score': 377.09517321143545},\n","  {'average_q1': 140.66884,\n","   'average_q2': 140.48068,\n","   'average_q_func1_loss': 2.7630036652088164,\n","   'average_q_func2_loss': 2.773467101454735,\n","   'n_updates': 280001,\n","   'average_entropy': -2.906002,\n","   'temperature': 0.02469233237206936,\n","   'eval_score': 376.0708031162508},\n","  {'average_q1': 140.83502,\n","   'average_q2': 140.71938,\n","   'average_q_func1_loss': 2.931262105703354,\n","   'average_q_func2_loss': 2.926825052499771,\n","   'n_updates': 285001,\n","   'average_entropy': -3.1022456,\n","   'temperature': 0.02484489232301712,\n","   'eval_score': 389.6100170919503},\n","  {'average_q1': 140.9974,\n","   'average_q2': 140.72116,\n","   'average_q_func1_loss': 2.4967803925275804,\n","   'average_q_func2_loss': 2.6419113874435425,\n","   'n_updates': 290001,\n","   'average_entropy': -3.006934,\n","   'temperature': 0.0243870597332716,\n","   'eval_score': 418.7275366342551},\n","  {'average_q1': 142.52997,\n","   'average_q2': 142.37354,\n","   'average_q_func1_loss': 2.3240578812360764,\n","   'average_q_func2_loss': 2.2537872552871705,\n","   'n_updates': 295001,\n","   'average_entropy': -2.929039,\n","   'temperature': 0.024938277900218964,\n","   'eval_score': 389.1397867584809},\n","  {'average_q1': 141.13841,\n","   'average_q2': 141.13322,\n","   'average_q_func1_loss': 2.4363531005382537,\n","   'average_q_func2_loss': 2.398443316817284,\n","   'n_updates': 300001,\n","   'average_entropy': -2.768107,\n","   'temperature': 0.023889794945716858,\n","   'eval_score': 403.54565402795043},\n","  {'average_q1': 141.92778,\n","   'average_q2': 142.10056,\n","   'average_q_func1_loss': 2.2262064057588575,\n","   'average_q_func2_loss': 2.246347873210907,\n","   'n_updates': 305001,\n","   'average_entropy': -2.8702583,\n","   'temperature': 0.024185694754123688,\n","   'eval_score': 372.8676946617388},\n","  {'average_q1': 138.335,\n","   'average_q2': 138.52596,\n","   'average_q_func1_loss': 2.022393190860748,\n","   'average_q_func2_loss': 2.040779872536659,\n","   'n_updates': 310001,\n","   'average_entropy': -2.9673214,\n","   'temperature': 0.024874527007341385,\n","   'eval_score': 340.23390112275695},\n","  {'average_q1': 139.83443,\n","   'average_q2': 139.67836,\n","   'average_q_func1_loss': 1.9580955743789672,\n","   'average_q_func2_loss': 1.9941988646984101,\n","   'n_updates': 315001,\n","   'average_entropy': -2.7915537,\n","   'temperature': 0.02374296449124813,\n","   'eval_score': 394.5981776126674},\n","  {'average_q1': 139.20528,\n","   'average_q2': 139.07634,\n","   'average_q_func1_loss': 2.12310966193676,\n","   'average_q_func2_loss': 2.1549445098638533,\n","   'n_updates': 320001,\n","   'average_entropy': -2.7391875,\n","   'temperature': 0.024410873651504517,\n","   'eval_score': 377.92209957648816},\n","  {'average_q1': 137.60922,\n","   'average_q2': 137.53404,\n","   'average_q_func1_loss': 2.0482685345411302,\n","   'average_q_func2_loss': 2.193989310860634,\n","   'n_updates': 325001,\n","   'average_entropy': -2.9399858,\n","   'temperature': 0.023426711559295654,\n","   'eval_score': 440.45344471285534},\n","  {'average_q1': 143.06647,\n","   'average_q2': 143.13914,\n","   'average_q_func1_loss': 2.359368164539337,\n","   'average_q_func2_loss': 2.4247919803857805,\n","   'n_updates': 330001,\n","   'average_entropy': -3.0388052,\n","   'temperature': 0.0238946583122015,\n","   'eval_score': 425.35286744212885},\n","  {'average_q1': 141.47731,\n","   'average_q2': 141.50201,\n","   'average_q_func1_loss': 2.3687834483385086,\n","   'average_q_func2_loss': 2.647694202065468,\n","   'n_updates': 335001,\n","   'average_entropy': -3.0891826,\n","   'temperature': 0.02396642230451107,\n","   'eval_score': 423.5408647851884},\n","  {'average_q1': 139.84825,\n","   'average_q2': 139.87683,\n","   'average_q_func1_loss': 2.450458301305771,\n","   'average_q_func2_loss': 2.5846634066104888,\n","   'n_updates': 340001,\n","   'average_entropy': -3.1522412,\n","   'temperature': 0.025347895920276642,\n","   'eval_score': 426.32117486377945},\n","  {'average_q1': 144.65047,\n","   'average_q2': 144.59944,\n","   'average_q_func1_loss': 2.904959196448326,\n","   'average_q_func2_loss': 2.9610852009057997,\n","   'n_updates': 345001,\n","   'average_entropy': -3.0199013,\n","   'temperature': 0.026269201189279556,\n","   'eval_score': 444.98665850288234},\n","  {'average_q1': 142.6719,\n","   'average_q2': 142.6363,\n","   'average_q_func1_loss': 2.5070144003629684,\n","   'average_q_func2_loss': 2.6536703836917876,\n","   'n_updates': 350001,\n","   'average_entropy': -3.05246,\n","   'temperature': 0.024561306461691856,\n","   'eval_score': 427.59186311554555},\n","  {'average_q1': 143.8889,\n","   'average_q2': 143.93729,\n","   'average_q_func1_loss': 2.0360790276527405,\n","   'average_q_func2_loss': 2.7930261993408205,\n","   'n_updates': 355001,\n","   'average_entropy': -3.020963,\n","   'temperature': 0.024648524820804596,\n","   'eval_score': 345.39103468359787},\n","  {'average_q1': 142.76047,\n","   'average_q2': 142.73059,\n","   'average_q_func1_loss': 2.030664854645729,\n","   'average_q_func2_loss': 2.067432472705841,\n","   'n_updates': 360001,\n","   'average_entropy': -2.8828228,\n","   'temperature': 0.025165554136037827,\n","   'eval_score': 414.54916815597767},\n","  {'average_q1': 146.31622,\n","   'average_q2': 146.19635,\n","   'average_q_func1_loss': 3.5915868002176286,\n","   'average_q_func2_loss': 3.5973320811986924,\n","   'n_updates': 365001,\n","   'average_entropy': -2.907532,\n","   'temperature': 0.02610924281179905,\n","   'eval_score': 372.06966058181496},\n","  {'average_q1': 143.9765,\n","   'average_q2': 144.03114,\n","   'average_q_func1_loss': 2.3601891458034516,\n","   'average_q_func2_loss': 2.385097026228905,\n","   'n_updates': 370001,\n","   'average_entropy': -3.005098,\n","   'temperature': 0.02587387152016163,\n","   'eval_score': 434.8207428763068},\n","  {'average_q1': 143.41405,\n","   'average_q2': 143.43164,\n","   'average_q_func1_loss': 2.6145404213666916,\n","   'average_q_func2_loss': 2.501053903698921,\n","   'n_updates': 375001,\n","   'average_entropy': -2.8556597,\n","   'temperature': 0.026692871004343033,\n","   'eval_score': 392.3170043895973},\n","  {'average_q1': 145.90703,\n","   'average_q2': 145.90791,\n","   'average_q_func1_loss': 2.512044351696968,\n","   'average_q_func2_loss': 2.521846371889114,\n","   'n_updates': 380001,\n","   'average_entropy': -2.8696406,\n","   'temperature': 0.02738076075911522,\n","   'eval_score': 449.146433803881},\n","  {'average_q1': 144.77197,\n","   'average_q2': 144.85281,\n","   'average_q_func1_loss': 2.9838156235218047,\n","   'average_q_func2_loss': 3.1919518184661864,\n","   'n_updates': 385001,\n","   'average_entropy': -2.9375331,\n","   'temperature': 0.027597550302743912,\n","   'eval_score': 439.2197871955716},\n","  {'average_q1': 146.78134,\n","   'average_q2': 146.74982,\n","   'average_q_func1_loss': 2.6847434276342392,\n","   'average_q_func2_loss': 2.661133737564087,\n","   'n_updates': 390001,\n","   'average_entropy': -2.8577187,\n","   'temperature': 0.027893133461475372,\n","   'eval_score': 473.4500738411548},\n","  {'average_q1': 145.53275,\n","   'average_q2': 145.64166,\n","   'average_q_func1_loss': 2.5506356024742125,\n","   'average_q_func2_loss': 2.6216224920749664,\n","   'n_updates': 395001,\n","   'average_entropy': -3.057899,\n","   'temperature': 0.028559964150190353,\n","   'eval_score': 493.3205617730942},\n","  {'average_q1': 146.27359,\n","   'average_q2': 146.28827,\n","   'average_q_func1_loss': 2.1359089142084122,\n","   'average_q_func2_loss': 2.1296300888061523,\n","   'n_updates': 400001,\n","   'average_entropy': -3.0059633,\n","   'temperature': 0.029318850487470627,\n","   'eval_score': 490.1983130839612},\n","  {'average_q1': 146.15726,\n","   'average_q2': 146.11865,\n","   'average_q_func1_loss': 2.3611513167619704,\n","   'average_q_func2_loss': 2.3906804090738296,\n","   'n_updates': 405001,\n","   'average_entropy': -2.9365623,\n","   'temperature': 0.02858375944197178,\n","   'eval_score': 464.99402396616574},\n","  {'average_q1': 145.06068,\n","   'average_q2': 145.0505,\n","   'average_q_func1_loss': 2.4826790845394133,\n","   'average_q_func2_loss': 2.4123480623960494,\n","   'n_updates': 410001,\n","   'average_entropy': -3.094623,\n","   'temperature': 0.030979793518781662,\n","   'eval_score': 446.15279845361425},\n","  {'average_q1': 146.43149,\n","   'average_q2': 146.41357,\n","   'average_q_func1_loss': 2.08096963763237,\n","   'average_q_func2_loss': 2.1151908552646637,\n","   'n_updates': 415001,\n","   'average_entropy': -2.9378242,\n","   'temperature': 0.029995055869221687,\n","   'eval_score': 444.7596801059522},\n","  {'average_q1': 147.6751,\n","   'average_q2': 147.77454,\n","   'average_q_func1_loss': 2.3660095393657685,\n","   'average_q_func2_loss': 2.536099408864975,\n","   'n_updates': 420001,\n","   'average_entropy': -2.9937925,\n","   'temperature': 0.02910364232957363,\n","   'eval_score': 417.9543216695097},\n","  {'average_q1': 149.36987,\n","   'average_q2': 149.23831,\n","   'average_q_func1_loss': 2.2258919495344163,\n","   'average_q_func2_loss': 2.405851812362671,\n","   'n_updates': 425001,\n","   'average_entropy': -2.7876728,\n","   'temperature': 0.029619794338941574,\n","   'eval_score': 431.2621141453096},\n","  {'average_q1': 147.54276,\n","   'average_q2': 147.51566,\n","   'average_q_func1_loss': 2.1656624084711074,\n","   'average_q_func2_loss': 2.1592812979221345,\n","   'n_updates': 430001,\n","   'average_entropy': -3.162527,\n","   'temperature': 0.029276682063937187,\n","   'eval_score': 428.31784235866667},\n","  {'average_q1': 146.02466,\n","   'average_q2': 146.08002,\n","   'average_q_func1_loss': 2.4964898544549943,\n","   'average_q_func2_loss': 2.6891132199764254,\n","   'n_updates': 435001,\n","   'average_entropy': -3.2970772,\n","   'temperature': 0.02963375300168991,\n","   'eval_score': 480.18819263289726},\n","  {'average_q1': 146.00107,\n","   'average_q2': 145.86707,\n","   'average_q_func1_loss': 2.177095396518707,\n","   'average_q_func2_loss': 2.4306365561485292,\n","   'n_updates': 440001,\n","   'average_entropy': -3.1501222,\n","   'temperature': 0.028840936720371246,\n","   'eval_score': 472.7544353416164},\n","  {'average_q1': 149.43965,\n","   'average_q2': 149.5829,\n","   'average_q_func1_loss': 2.703659527897835,\n","   'average_q_func2_loss': 2.8231278324127196,\n","   'n_updates': 445001,\n","   'average_entropy': -2.8837726,\n","   'temperature': 0.03050544671714306,\n","   'eval_score': 479.08661856720056},\n","  {'average_q1': 150.20139,\n","   'average_q2': 150.34428,\n","   'average_q_func1_loss': 2.8281408274173736,\n","   'average_q_func2_loss': 2.8805429059267045,\n","   'n_updates': 450001,\n","   'average_entropy': -2.8221114,\n","   'temperature': 0.02966899424791336,\n","   'eval_score': 512.7098619029714},\n","  {'average_q1': 150.10649,\n","   'average_q2': 150.13391,\n","   'average_q_func1_loss': 3.0326218891143797,\n","   'average_q_func2_loss': 3.0529048269987107,\n","   'n_updates': 455001,\n","   'average_entropy': -3.2273037,\n","   'temperature': 0.030511585995554924,\n","   'eval_score': 456.5176589039316},\n","  {'average_q1': 149.49745,\n","   'average_q2': 149.57855,\n","   'average_q_func1_loss': 2.283303450345993,\n","   'average_q_func2_loss': 2.488742324709892,\n","   'n_updates': 460001,\n","   'average_entropy': -3.0262814,\n","   'temperature': 0.0292680487036705,\n","   'eval_score': 487.2433763001641},\n","  {'average_q1': 146.8125,\n","   'average_q2': 146.67325,\n","   'average_q_func1_loss': 1.957352357506752,\n","   'average_q_func2_loss': 2.1648102647066114,\n","   'n_updates': 465001,\n","   'average_entropy': -3.0876536,\n","   'temperature': 0.030073875561356544,\n","   'eval_score': 516.9257865319926},\n","  {'average_q1': 149.33566,\n","   'average_q2': 149.42853,\n","   'average_q_func1_loss': 2.2637453019618987,\n","   'average_q_func2_loss': 2.629564046263695,\n","   'n_updates': 470001,\n","   'average_entropy': -2.8622897,\n","   'temperature': 0.029424594715237617,\n","   'eval_score': 530.7029801394411},\n","  {'average_q1': 150.15863,\n","   'average_q2': 150.01097,\n","   'average_q_func1_loss': 2.6373209500312806,\n","   'average_q_func2_loss': 3.03772125184536,\n","   'n_updates': 475001,\n","   'average_entropy': -3.0555832,\n","   'temperature': 0.029739586636424065,\n","   'eval_score': 442.8911194662157},\n","  {'average_q1': 148.40329,\n","   'average_q2': 148.1513,\n","   'average_q_func1_loss': 2.8455655753612517,\n","   'average_q_func2_loss': 3.0491160881519317,\n","   'n_updates': 480001,\n","   'average_entropy': -2.8295534,\n","   'temperature': 0.030285440385341644,\n","   'eval_score': 471.2804741326496},\n","  {'average_q1': 150.74615,\n","   'average_q2': 150.54253,\n","   'average_q_func1_loss': 2.4448703598976134,\n","   'average_q_func2_loss': 2.8448221665620803,\n","   'n_updates': 485001,\n","   'average_entropy': -2.850182,\n","   'temperature': 0.028827449306845665,\n","   'eval_score': 538.2604964935908},\n","  {'average_q1': 150.9219,\n","   'average_q2': 150.9959,\n","   'average_q_func1_loss': 2.7044841969013214,\n","   'average_q_func2_loss': 2.836162796020508,\n","   'n_updates': 490001,\n","   'average_entropy': -3.1638026,\n","   'temperature': 0.02952825278043747,\n","   'eval_score': 496.3926417908482}])"]},"metadata":{},"execution_count":51}],"source":["experiments.train_agent_batch_with_evaluation(\n","    agent=agent,\n","    env=make_batch_env(test=False),\n","    eval_env=make_batch_env(test=False),\n","    outdir=\"logs/mixed\",\n","    steps=steps,\n","    eval_n_steps=None,\n","    eval_n_episodes=30,\n","    eval_interval=5 * 10 ** 3,\n","    log_interval=10 ** 3,\n","    max_episode_len=timestep_limit,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_fNkGRJQeGQY"},"outputs":[],"source":["# agent.save(\"sac-baseline\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WI9LJpUhW5dk"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"yIr16rRYfCW_"},"source":["## online evaluation"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"xKZ7qTYofjtR","executionInfo":{"status":"ok","timestamp":1701802194582,"user_tz":300,"elapsed":5342,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["agent.load(\"sac-baseline\")"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"ACJ1HnCbcq8k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701802227221,"user_tz":300,"elapsed":15546,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}},"outputId":"86bfaa13-6a76-4e97-a4dd-92c58f82090d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","INFO:pfrl.experiments.evaluator:evaluation episode 0 length: 147 R: 427.86844092844035\n","INFO:pfrl.experiments.evaluator:evaluation episode 1 length: 148 R: 428.4222903411757\n","INFO:pfrl.experiments.evaluator:evaluation episode 2 length: 148 R: 432.0637804816967\n","INFO:pfrl.experiments.evaluator:evaluation episode 3 length: 147 R: 428.2255730740826\n","INFO:pfrl.experiments.evaluator:evaluation episode 4 length: 147 R: 428.4294474049445\n","INFO:pfrl.experiments.evaluator:evaluation episode 5 length: 148 R: 429.63887257743204\n","INFO:pfrl.experiments.evaluator:evaluation episode 6 length: 148 R: 428.99497714828766\n","INFO:pfrl.experiments.evaluator:evaluation episode 7 length: 149 R: 433.08008995403526\n","INFO:pfrl.experiments.evaluator:evaluation episode 8 length: 154 R: 452.59500951759156\n","INFO:pfrl.experiments.evaluator:evaluation episode 9 length: 154 R: 452.5261899037477\n","INFO:pfrl.experiments.evaluator:evaluation episode 10 length: 148 R: 419.73782759465627\n","INFO:pfrl.experiments.evaluator:evaluation episode 11 length: 149 R: 421.8887473005681\n","INFO:pfrl.experiments.evaluator:evaluation episode 12 length: 214 R: 544.6269435913031\n","INFO:pfrl.experiments.evaluator:evaluation episode 13 length: 163 R: 348.1722894898501\n","INFO:pfrl.experiments.evaluator:evaluation episode 14 length: 213 R: 554.1869308473399\n","INFO:pfrl.experiments.evaluator:evaluation episode 15 length: 189 R: 590.9027129527286\n","INFO:pfrl.experiments.evaluator:evaluation episode 16 length: 160 R: 487.553503414093\n","INFO:pfrl.experiments.evaluator:evaluation episode 17 length: 161 R: 497.91223152831225\n","INFO:pfrl.experiments.evaluator:evaluation episode 18 length: 207 R: 651.1842003538247\n","INFO:pfrl.experiments.evaluator:evaluation episode 19 length: 203 R: 616.154634205726\n","INFO:pfrl.experiments.evaluator:evaluation episode 20 length: 198 R: 607.0545651255995\n","INFO:pfrl.experiments.evaluator:evaluation episode 21 length: 194 R: 586.4016960568711\n","INFO:pfrl.experiments.evaluator:evaluation episode 22 length: 147 R: 430.58578137282245\n","INFO:pfrl.experiments.evaluator:evaluation episode 23 length: 162 R: 509.2942595369183\n","INFO:pfrl.experiments.evaluator:evaluation episode 24 length: 154 R: 454.57417381204897\n","INFO:pfrl.experiments.evaluator:evaluation episode 25 length: 150 R: 424.5335523610257\n","INFO:pfrl.experiments.evaluator:evaluation episode 26 length: 209 R: 534.0110053624161\n","INFO:pfrl.experiments.evaluator:evaluation episode 27 length: 199 R: 621.2496689431925\n","INFO:pfrl.experiments.evaluator:evaluation episode 28 length: 221 R: 609.0564715637945\n","INFO:pfrl.experiments.evaluator:evaluation episode 29 length: 147 R: 429.9209684019333\n"]},{"output_type":"stream","name":"stdout","text":["n_runs: 10 mean: 489.3615611715486 median: 452.56059971066963 stdev 81.01465868705594\n"]}],"source":["eval_stats = experiments.eval_performance(\n","    env=make_batch_env(test=True),\n","    agent=agent,\n","    n_steps=None,\n","    n_episodes=30,\n","    max_episode_len=timestep_limit,\n",")\n","print(\n","    \"n_runs: {} mean: {} median: {} stdev {}\".format(\n","        10,\n","        eval_stats[\"mean\"],\n","        eval_stats[\"median\"],\n","        eval_stats[\"stdev\"],\n","    )\n",")"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"-0I_hQi9W6EH","executionInfo":{"status":"ok","timestamp":1701802290035,"user_tz":300,"elapsed":406,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}}},"outputs":[],"source":["logs_baseline = {\n","    \"mean\": eval_stats[\"mean\"],\n","    \"std\": eval_stats[\"stdev\"],\n","}"]},{"cell_type":"code","source":["import pickle\n","with open(\"baseline_performances.pkl\", \"wb\") as f:\n","    pickle.dump(logs_baseline, f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jd6_2WmdUjZz","executionInfo":{"status":"ok","timestamp":1701802431624,"user_tz":300,"elapsed":542,"user":{"displayName":"Haruka Kiyohara","userId":"17824390424833208212"}},"outputId":"6bbc4b2e-6a73-4d76-c23a-1e02149d11ad"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","metadata":{"id":"vvnsN-_Kkkko"},"source":["## render (TODO)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aEE9zpZokzpX"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from IPython import display as ipythondisplay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zWMdWGs9lACY"},"outputs":[],"source":["def make_env_for_render():\n","    env = gym.make(env_name, render_mode='rgb_array')\n","    env = gym.wrappers.RecordEpisodeStatistics(env)\n","    env.seed(seed)\n","    env.action_space.seed(seed)\n","    env.observation_space.seed(seed)\n","    return env"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gV4A2mvBlLbf","colab":{"base_uri":"https://localhost:8080/","height":375},"executionInfo":{"status":"error","timestamp":1701720630633,"user_tz":300,"elapsed":11,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"aeba2840-3e36-410b-de34-a2f76a5f0b63"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:601: UserWarning: \u001b[33mWARN: Using the latest versioned environment `Hopper-v4` instead of the unversioned environment `Hopper`.\u001b[0m\n","  logger.warn(\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-63-5a4a88abfd97>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_env_for_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-62-f72aa458bbab>\u001b[0m in \u001b[0;36mmake_env_for_render\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_env_for_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hopper\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecordEpisodeStatistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/hopper_v4.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, forward_reward_weight, ctrl_cost_weight, healthy_reward, terminate_when_unhealthy, healthy_state_range, healthy_z_range, healthy_angle_range, reset_noise_scale, exclude_current_positions_from_observation, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m             )\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         MujocoEnv.__init__(\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hopper.xml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, frame_skip, observation_space, render_mode, width, height, camera_id, camera_name)\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0;34mf\"{MUJOCO_IMPORT_ERROR}. (HINT: you need to install mujoco)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             )\n\u001b[0;32m--> 368\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0mframe_skip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, frame_skip, observation_space, render_mode, width, height, camera_id, camera_name)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File {self.fullpath} does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_qpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36m_initialize_simulation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_initialize_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmujoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMjModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_xml_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfullpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmujoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMjData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: XML Error: global coordinates no longer supported. To convert existing models, load and save them in MuJoCo 2.3.3 or older\nElement 'compiler', line 2\n"]}],"source":["env_ = make_env_for_render()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cwlgrtDRkmy2","colab":{"base_uri":"https://localhost:8080/","height":782},"executionInfo":{"status":"error","timestamp":1701720549725,"user_tz":300,"elapsed":138,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"47cca083-5d77-447a-d58c-dfec15df4156"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-7aef18373a86>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mipythondisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pfrl/agent.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pfrl/agents/soft_actor_critic.py\u001b[0m in \u001b[0;36mbatch_act\u001b[0;34m(self, batch_obs)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_act_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_act_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pfrl/agents/soft_actor_critic.py\u001b[0m in \u001b[0;36m_batch_act_train\u001b[0;34m(self, batch_obs)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mbatch_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mburnin_action_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0mbatch_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_select_greedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_last_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_last_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pfrl/agents/soft_actor_critic.py\u001b[0m in \u001b[0;36mbatch_select_greedy_action\u001b[0;34m(self, batch_obs, deterministic)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpfrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mbatch_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mpolicy_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mbatch_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode_of_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["plt.axis('off')\n","done = False\n","obs = env_.reset()\n","\n","i = 0\n","while not done:\n","    i += 1\n","    if i % 20 == 0:\n","        ipythondisplay.clear_output(wait=True)\n","        # print(\"At timestep = \", i)\n","        screen = env_.render()\n","        plt.imshow(screen[0])\n","        ipythondisplay.display(plt.gcf())\n","\n","    action = agent.act(obs)\n","    obs, reward, done, info = env_.step(action)\n","\n","    if done:\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xtc6CJCoW8XU"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}