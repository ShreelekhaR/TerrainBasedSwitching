{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Installation (work on colab)"],"metadata":{"id":"4ZNZOeJhUvi5"}},{"cell_type":"code","source":["!apt-get install -y \\\n","    libgl1-mesa-dev \\\n","    libgl1-mesa-glx \\\n","    libglew-dev \\\n","    libosmesa6-dev \\\n","    software-properties-common\n","\n","!apt-get install -y patchelf\n","!pip install gym\n","\n","!pip install free-mujoco-py\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install imageio==2.4.1\n","!pip install -U colabgymrender\n","!pip install mujoco"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JKvWsG4iUn2K","executionInfo":{"status":"ok","timestamp":1701323529596,"user_tz":300,"elapsed":56389,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"bf85a4eb-767d-4ffd-f9e7-9baecc6d8cb0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","software-properties-common is already the newest version (0.99.22.8).\n","The following additional packages will be installed:\n","  libegl-dev libgl-dev libgles-dev libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev\n","  libglvnd-dev libglx-dev libopengl-dev libosmesa6\n","The following NEW packages will be installed:\n","  libegl-dev libgl-dev libgl1-mesa-dev libgl1-mesa-glx libgles-dev libgles1 libglew-dev\n","  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev libopengl-dev libosmesa6\n","  libosmesa6-dev\n","0 upgraded, 15 newly installed, 0 to remove and 15 not upgraded.\n","Need to get 3,952 kB of archives.\n","After this operation, 18.7 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5,584 B]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.0.4-0ubuntu1~22.04.1 [6,510 B]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libglew-dev amd64 2.2.0-4 [287 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6 amd64 23.0.4-0ubuntu1~22.04.1 [3,054 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6-dev amd64 23.0.4-0ubuntu1~22.04.1 [8,986 B]\n","Fetched 3,952 kB in 2s (1,663 kB/s)\n","Selecting previously unselected package libglx-dev:amd64.\n","(Reading database ... 120882 files and directories currently installed.)\n","Preparing to unpack .../00-libglx-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglx-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl-dev:amd64.\n","Preparing to unpack .../01-libgl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libegl-dev:amd64.\n","Preparing to unpack .../02-libegl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libegl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-glx:amd64.\n","Preparing to unpack .../03-libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libgles1:amd64.\n","Preparing to unpack .../04-libgles1_1.4.0-1_amd64.deb ...\n","Unpacking libgles1:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles-dev:amd64.\n","Preparing to unpack .../05-libgles-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgles-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-core-dev:amd64.\n","Preparing to unpack .../06-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libopengl-dev:amd64.\n","Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-dev:amd64.\n","Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-dev:amd64.\n","Preparing to unpack .../09-libgl1-mesa-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglu1-mesa-dev:amd64.\n","Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglew-dev:amd64.\n","Preparing to unpack .../12-libglew-dev_2.2.0-4_amd64.deb ...\n","Unpacking libglew-dev:amd64 (2.2.0-4) ...\n","Selecting previously unselected package libosmesa6:amd64.\n","Preparing to unpack .../13-libosmesa6_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libosmesa6:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libosmesa6-dev:amd64.\n","Preparing to unpack .../14-libosmesa6-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libosmesa6-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Setting up libgles1:amd64 (1.4.0-1) ...\n","Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up libglx-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up libopengl-dev:amd64 (1.4.0-1) ...\n","Setting up libgl-dev:amd64 (1.4.0-1) ...\n","Setting up libosmesa6:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up libegl-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Setting up libosmesa6-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up libgles-dev:amd64 (1.4.0-1) ...\n","Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n","Setting up libglew-dev:amd64 (2.2.0-4) ...\n","Setting up libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  patchelf\n","0 upgraded, 1 newly installed, 0 to remove and 15 not upgraded.\n","Need to get 72.1 kB of archives.\n","After this operation, 186 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 patchelf amd64 0.14.3-1 [72.1 kB]\n","Fetched 72.1 kB in 1s (55.7 kB/s)\n","Selecting previously unselected package patchelf.\n","(Reading database ... 121022 files and directories currently installed.)\n","Preparing to unpack .../patchelf_0.14.3-1_amd64.deb ...\n","Unpacking patchelf (0.14.3-1) ...\n","Setting up patchelf (0.14.3-1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n","Collecting free-mujoco-py\n","  Downloading free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Cython<0.30.0,>=0.29.24 (from free-mujoco-py)\n","  Downloading Cython-0.29.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.16.0)\n","Collecting fasteners==0.15 (from free-mujoco-py)\n","  Downloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n","Collecting glfw<2.0.0,>=1.4.0 (from free-mujoco-py)\n","  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (2.31.6)\n","Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.23.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fasteners==0.15->free-mujoco-py) (1.16.0)\n","Collecting monotonic>=0.1 (from fasteners==0.15->free-mujoco-py)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n","Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (9.4.0)\n","Installing collected packages: monotonic, glfw, fasteners, Cython, free-mujoco-py\n","  Attempting uninstall: Cython\n","    Found existing installation: Cython 3.0.5\n","    Uninstalling Cython-3.0.5:\n","      Successfully uninstalled Cython-3.0.5\n","Successfully installed Cython-0.29.36 fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 monotonic-1.6\n","Collecting imageio==2.4.1\n","  Downloading imageio-2.4.1.tar.gz (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.1) (1.23.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.1) (9.4.0)\n","Building wheels for collected packages: imageio\n","  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imageio: filename=imageio-2.4.1-py3-none-any.whl size=3303885 sha256=2720fc08a7e6eb556380e31a65de116b5093272dfba002783dc917e595c4e95c\n","  Stored in directory: /root/.cache/pip/wheels/96/5d/ce/bdbdb04744dac03906336eb0d01ff1e222061d3419c55c55f9\n","Successfully built imageio\n","Installing collected packages: imageio\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.31.6\n","    Uninstalling imageio-2.31.6:\n","      Successfully uninstalled imageio-2.31.6\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","free-mujoco-py 2.1.6 requires imageio<3.0.0,>=2.9.0, but you have imageio 2.4.1 which is incompatible.\n","moviepy 1.0.3 requires imageio<3.0,>=2.5; python_version >= \"3.4\", but you have imageio 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed imageio-2.4.1\n","Collecting colabgymrender\n","  Downloading colabgymrender-1.1.0.tar.gz (3.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from colabgymrender) (1.0.3)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (4.4.2)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (4.66.1)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (2.31.0)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (0.1.10)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (1.23.5)\n","Collecting imageio<3.0,>=2.5 (from moviepy->colabgymrender)\n","  Downloading imageio-2.33.0-py3-none-any.whl (313 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.3/313.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (0.4.9)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy->colabgymrender) (9.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy->colabgymrender) (67.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (2023.7.22)\n","Building wheels for collected packages: colabgymrender\n","  Building wheel for colabgymrender (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for colabgymrender: filename=colabgymrender-1.1.0-py3-none-any.whl size=3115 sha256=91004bd7174f4f45754d7e8f1b55be5291bcf41ebe1e78dcd9ac7aeec259ff59\n","  Stored in directory: /root/.cache/pip/wheels/13/62/63/7b3acfb684dd3d665d7fc1d213427b136205a222389767e295\n","Successfully built colabgymrender\n","Installing collected packages: imageio, colabgymrender\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.4.1\n","    Uninstalling imageio-2.4.1:\n","      Successfully uninstalled imageio-2.4.1\n","Successfully installed colabgymrender-1.1.0 imageio-2.33.0\n","Collecting mujoco\n","  Downloading mujoco-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.4.0)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.5.2)\n","Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.23.5)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco) (3.1.7)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (2023.6.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (6.1.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (4.5.0)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (3.17.0)\n","Installing collected packages: mujoco\n","Successfully installed mujoco-3.0.1\n"]}]},{"cell_type":"code","source":["!python --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BsNICgpbv6vT","executionInfo":{"status":"ok","timestamp":1701323558285,"user_tz":300,"elapsed":337,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"62b130d2-de9e-4304-e390-a4ab6616a78a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZhFKM-rDS0yE","executionInfo":{"status":"ok","timestamp":1701323919708,"user_tz":300,"elapsed":7471,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"cc2ea7cf-c7e7-4aa7-ac6f-1eb3e06c5ff8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pfrl\n","  Downloading pfrl-0.4.0.tar.gz (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/112.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pfrl) (2.1.0+cu118)\n","Requirement already satisfied: gym>=0.9.7 in /usr/local/lib/python3.10/dist-packages (from pfrl) (0.25.2)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from pfrl) (1.23.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pfrl) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from pfrl) (3.13.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.9.7->pfrl) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.9.7->pfrl) (0.0.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->pfrl) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->pfrl) (1.3.0)\n","Building wheels for collected packages: pfrl\n","  Building wheel for pfrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pfrl: filename=pfrl-0.4.0-py3-none-any.whl size=155460 sha256=5dc1024aa1d85d5de1e3b855a59a858797d502fc142964cd3c5b5a8d2f6141dd\n","  Stored in directory: /root/.cache/pip/wheels/22/4a/0f/a87cd1ae925086eb3a1b8759f620fcf48e47939fb082946c3b\n","Successfully built pfrl\n","Installing collected packages: pfrl\n","Successfully installed pfrl-0.4.0\n"]}],"source":["!pip install pfrl"]},{"cell_type":"code","source":["!pip install gymnasium"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofM7lHvOUVDc","executionInfo":{"status":"ok","timestamp":1701323928042,"user_tz":300,"elapsed":8342,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"f7a0df38-1157-4972-8079-719afb24dd34"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/953.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"]}]},{"cell_type":"code","source":["import functools\n","import numpy as np\n","\n","import gymnasium as gym\n","import gym.wrappers\n","\n","import pfrl"],"metadata":{"id":"Oh_oFqaITeKZ","executionInfo":{"status":"ok","timestamp":1701323932642,"user_tz":300,"elapsed":4604,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["pip freeze > requirements.txt"],"metadata":{"id":"5hMuefzEwZFU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701323934194,"user_tz":300,"elapsed":1558,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"8c83fc3b-ba6b-4558-d110-af6d7860c614"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["device = \"cuda:0\""],"metadata":{"id":"iUBFcB_3VDK8","executionInfo":{"status":"ok","timestamp":1701323934195,"user_tz":300,"elapsed":7,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"joyBiJ7Sedjh","executionInfo":{"status":"ok","timestamp":1701323959467,"user_tz":300,"elapsed":25278,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"6374f3c6-81af-47c9-ea5b-a07f53ee4db6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pexpect/popen_spawn.py:60: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n","  self._read_thread.setDaemon(True)\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4bA0agQ_APu","executionInfo":{"status":"ok","timestamp":1701226299356,"user_tz":300,"elapsed":7,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"d5d77ca3-c0ec-41b6-d2f7-9fca54093112"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OH5x_jG1_EjT","executionInfo":{"status":"ok","timestamp":1701226347353,"user_tz":300,"elapsed":218,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"8339dc04-2a84-4de9-86d1-56bc16eebf9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mbin\u001b[0m/    \u001b[01;36mcuda\u001b[0m@     \u001b[01;34mcuda-11.8\u001b[0m/  \u001b[01;34mgames\u001b[0m/               \u001b[01;34minclude\u001b[0m/  \u001b[01;34mlib64\u001b[0m/      \u001b[01;36mman\u001b[0m@   \u001b[01;34mshare\u001b[0m/\n","\u001b[01;34mcolab\u001b[0m/  \u001b[01;36mcuda-11\u001b[0m@  \u001b[01;34metc\u001b[0m/        \u001b[01;34m_gcs_config_ops.so\u001b[0m/  \u001b[01;34mlib\u001b[0m/      \u001b[01;34mlicensing\u001b[0m/  \u001b[01;34msbin\u001b[0m/  \u001b[01;34msrc\u001b[0m/\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rRe72RDV7pZ6","executionInfo":{"status":"ok","timestamp":1701226316235,"user_tz":300,"elapsed":231,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"b91a8118-05f0-4935-9b42-0702c5700ea9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot create regular file '/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/assets/envs/': Not a directory\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/codes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"62iQsL5jeqca","executionInfo":{"status":"ok","timestamp":1701323959467,"user_tz":300,"elapsed":9,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"f1c8119d-0f50-4ac1-e571-b265dbc346e2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1f55zVGF_dF_ofwI_JfGgeIj4XSrHgcE8/codes\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"V0gMDExYyHLW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize environment"],"metadata":{"id":"7aEG-BTXXDm9"}},{"cell_type":"code","source":["env_name = \"Hopper-v3\"\n","num_envs = 5\n","seed = 42\n","\n","monitor = False\n","render = False\n","\n","process_seeds = np.arange(num_envs) + seed * num_envs"],"metadata":{"id":"59FrC2_ziiqL","executionInfo":{"status":"ok","timestamp":1701323959467,"user_tz":300,"elapsed":6,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def make_env(process_idx, test):\n","    env = gym.make(env_name, xml_file=\"envs/sandpaper_hopper.xml\")\n","    # Unwrap TimiLimit wrapper\n","    assert isinstance(env, gym.wrappers.TimeLimit)\n","    env = env.env\n","    # Use different random seeds for train and test envs\n","    process_seed = int(process_seeds[process_idx])\n","    env_seed = 2**32 - 1 - process_seed if test else process_seed\n","    env.seed(env_seed)\n","    # Cast observations to float32 because our model uses float32\n","    env = pfrl.wrappers.CastObservationToFloat32(env)\n","    # Normalize action space to [-1, 1]^n\n","    env = pfrl.wrappers.NormalizeActionSpace(env)\n","    if monitor:\n","        env = gym.wrappers.Monitor(env, \"monitor\")\n","    if render:\n","        env = pfrl.wrappers.Render(env)\n","    return env"],"metadata":{"id":"sIrESw-ZUcLs","executionInfo":{"status":"ok","timestamp":1701323959467,"user_tz":300,"elapsed":5,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def make_batch_env(test):\n","    return pfrl.envs.MultiprocessVectorEnv(\n","        [\n","            functools.partial(make_env, idx, test)\n","            for idx, env in enumerate(range(num_envs))\n","        ]\n","    )"],"metadata":{"id":"jR-KaV8_c5HA","executionInfo":{"status":"ok","timestamp":1701323959468,"user_tz":300,"elapsed":5,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["env = make_env(0, test=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pB9V9m9IXAsR","executionInfo":{"status":"ok","timestamp":1701324154640,"user_tz":300,"elapsed":86336,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"479c8047-8613-4d1c-b3d5-e1e0fa8c10b8"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Compiling /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx because it changed.\n","[1/1] Cythonizing /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx\n"]},{"output_type":"stream","name":"stderr","text":["INFO:root:running build_ext\n","INFO:root:building 'mujoco_py.cymj' extension\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl\n","INFO:root:x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/mujoco_py -I/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.o -fopenmp -w\n","INFO:root:x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/mujoco_py -I/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c /usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.c -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.o -fopenmp -w\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310/mujoco_py\n","INFO:root:x86_64-linux-gnu-gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.o -L/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -L/usr/lib/x86_64-linux-gnu -Wl,--enable-new-dtags,-R/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -lmujoco210 -lglewosmesa -lOSMesa -lGL -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310/mujoco_py/cymj.cpython-310-x86_64-linux-gnu.so -fopenmp\n","<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["env.observation_space"],"metadata":{"id":"eTojqsUDXNpe","executionInfo":{"status":"ok","timestamp":1701324154640,"user_tz":300,"elapsed":8,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"27a22840-bf16-400f-9c7f-5bcf3bfd0f45"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(-inf, inf, (11,), float64)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["env.action_space"],"metadata":{"id":"V1db9VKdXQDL","executionInfo":{"status":"ok","timestamp":1701324154641,"user_tz":300,"elapsed":7,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0e7b7a35-8992-4146-c183-cd0f7c45d8bd"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(-1.0, 1.0, (3,), float32)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["obs_size = env.observation_space.shape[0]\n","action_size = env.action_space.shape[0]\n","timestep_limit = env.spec.max_episode_steps"],"metadata":{"id":"BNYgd4K-X1RG","executionInfo":{"status":"ok","timestamp":1701324154641,"user_tz":300,"elapsed":5,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tTDCl9klyIle"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## SAC Config (with PFRL)\n","\n","sample codes and parameters in [this file](https://github.com/pfnet/pfrl/blob/master/examples/mujoco/reproduction/soft_actor_critic/train_soft_actor_critic.py)."],"metadata":{"id":"vw5gYvbBVfEj"}},{"cell_type":"code","source":["import torch\n","from torch import distributions, nn\n","\n","from pfrl import experiments, replay_buffers, utils\n","from pfrl.nn.lmbda import Lambda"],"metadata":{"id":"CrghSSqgVmC6","executionInfo":{"status":"ok","timestamp":1701324154641,"user_tz":300,"elapsed":5,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# default params\n","policy_output_scale = 1.0\n","\n","policy_hidden_dim = 256\n","policy_lr = 3e-4\n","\n","q_func_hidden_dim = 256\n","q_func_lr = 3e-4\n","\n","temperature_optimizer_lr = 3e-4\n","\n","buffer_size = 10 ** 6\n","replay_start_size = 10 ** 4\n","batch_size = 256\n","gamma = 0.99\n","\n","# steps = 10 ** 6\n","steps = 10 * 10 ** 5"],"metadata":{"id":"YSXLsGBMYMk2","executionInfo":{"status":"ok","timestamp":1701324154641,"user_tz":300,"elapsed":5,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def burnin_action_func():\n","    \"\"\"Select random actions until model is updated one or more times.\"\"\"\n","    return np.random.uniform(env.action_space.low, env.action_space.high).astype(np.float32)"],"metadata":{"id":"3un8LjlWaQ4B","executionInfo":{"status":"ok","timestamp":1701324154641,"user_tz":300,"elapsed":5,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def squashed_diagonal_gaussian_head(x):\n","    assert x.shape[-1] == action_size * 2\n","    mean, log_scale = torch.chunk(x, 2, dim=1)\n","    log_scale = torch.clamp(log_scale, -20.0, 2.0)\n","    var = torch.exp(log_scale * 2)\n","    base_distribution = distributions.Independent(\n","        distributions.Normal(loc=mean, scale=torch.sqrt(var)), 1\n","    )\n","    # cache_size=1 is required for numerical stability\n","    return distributions.transformed_distribution.TransformedDistribution(\n","        base_distribution, [distributions.transforms.TanhTransform(cache_size=1)]\n","    )"],"metadata":{"id":"BTBY4AGWVX4q","executionInfo":{"status":"ok","timestamp":1701324154642,"user_tz":300,"elapsed":6,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def make_policy_with_optimizer():\n","    policy = nn.Sequential(\n","        nn.Linear(obs_size, policy_hidden_dim),\n","        nn.ReLU(),\n","        nn.Linear(policy_hidden_dim, policy_hidden_dim),\n","        nn.ReLU(),\n","        nn.Linear(policy_hidden_dim, action_size * 2),\n","        Lambda(squashed_diagonal_gaussian_head),\n","    )\n","    torch.nn.init.xavier_uniform_(policy[0].weight)\n","    torch.nn.init.xavier_uniform_(policy[2].weight)\n","    torch.nn.init.xavier_uniform_(policy[4].weight, gain=policy_output_scale)\n","    policy_optimizer = torch.optim.Adam(policy.parameters(), lr=policy_lr)\n","    return policy, policy_optimizer"],"metadata":{"id":"_schTPU8X9CG","executionInfo":{"status":"ok","timestamp":1701324154642,"user_tz":300,"elapsed":6,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def make_q_func_with_optimizer():\n","    q_func = nn.Sequential(\n","        pfrl.nn.ConcatObsAndAction(),\n","        nn.Linear(obs_size + action_size, q_func_hidden_dim),\n","        nn.ReLU(),\n","        nn.Linear(q_func_hidden_dim, q_func_hidden_dim),\n","        nn.ReLU(),\n","        nn.Linear(q_func_hidden_dim, 1),\n","    )\n","    torch.nn.init.xavier_uniform_(q_func[1].weight)\n","    torch.nn.init.xavier_uniform_(q_func[3].weight)\n","    torch.nn.init.xavier_uniform_(q_func[5].weight)\n","    q_func_optimizer = torch.optim.Adam(q_func.parameters(), lr=q_func_lr)\n","    return q_func, q_func_optimizer"],"metadata":{"id":"YfRiZLWzY_C6","executionInfo":{"status":"ok","timestamp":1701324154642,"user_tz":300,"elapsed":5,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["policy, policy_optimizer = make_policy_with_optimizer()\n","q_func1, q_func1_optimizer = make_q_func_with_optimizer()\n","q_func2, q_func2_optimizer = make_q_func_with_optimizer()"],"metadata":{"id":"ODiMpSZ8Ycao","executionInfo":{"status":"ok","timestamp":1701324155984,"user_tz":300,"elapsed":1347,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["rbuffer = replay_buffers.ReplayBuffer(buffer_size)"],"metadata":{"id":"UPdN1jnDZ92X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701324155984,"user_tz":300,"elapsed":5,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}},"outputId":"7e7d5a82-b0cb-4d50-c508-6cab7a3c3e5a"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"C5oTxaLoxjjA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train SAC policy online"],"metadata":{"id":"gY1IkWhJdMK8"}},{"cell_type":"code","source":["agent = pfrl.agents.SoftActorCritic(\n","    policy,\n","    q_func1,\n","    q_func2,\n","    policy_optimizer,\n","    q_func1_optimizer,\n","    q_func2_optimizer,\n","    rbuffer,\n","    gamma=gamma,\n","    replay_start_size=replay_start_size,\n","    gpu=0 if torch.cuda.is_available() else -1,\n","    minibatch_size=batch_size,\n","    burnin_action_func=burnin_action_func,\n","    entropy_target=-action_size,\n","    temperature_optimizer_lr=temperature_optimizer_lr,\n",")\n"],"metadata":{"id":"Ps-9PQuLaKPT","executionInfo":{"status":"ok","timestamp":1701324161673,"user_tz":300,"elapsed":5692,"user":{"displayName":"Lekha Revankar","userId":"16849370800548293955"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["experiments.train_agent_batch_with_evaluation(\n","    agent=agent,\n","    env=make_batch_env(test=False),\n","    eval_env=make_batch_env(test=False),\n","    outdir=\"logs/trying_something_out\",\n","    steps=steps,\n","    eval_n_steps=None,\n","    eval_n_episodes=10,\n","    eval_interval=5 * 10 ** 3,\n","    log_interval=10 ** 3,\n","    max_episode_len=timestep_limit,\n",")\n","agent.save(\"sac-default\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O1KQLV3SbeEX","outputId":"1cb6805f-2257-4f79-93ff-2320df76ab8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:1000 episode:41 last_R: 18.48498709531299 average_R:18.624072554056628\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:2000 episode:86 last_R: 9.474792623272762 average_R:17.88760234398333\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:3000 episode:132 last_R: 35.05542613866651 average_R:17.649299248512342\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:4000 episode:182 last_R: 7.977427825219676 average_R:17.098723589878684\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:5000 episode:230 last_R: 13.928722376840337 average_R:15.84263146290646\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 12 R: 10.630198692662773\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 12 R: 10.669691956487709\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 12 R: 10.689901106253377\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 12 R: 10.62511792673958\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 12 R: 10.650053859028624\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 12 R: 10.591019337636556\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 12 R: 10.596376558790134\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 12 R: 10.562501907849178\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 12 R: 10.65312855805522\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 12 R: 10.654045264050808\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated -3.4028235e+38 -> 10.632203516755396\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/trying_something_out/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:6000 episode:280 last_R: 27.28111065267444 average_R:15.928534079784036\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:7000 episode:321 last_R: 15.657090314898632 average_R:17.89371227119766\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:8000 episode:364 last_R: 7.296887245359317 average_R:17.603619244623307\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:9000 episode:414 last_R: 36.57059956144897 average_R:16.98210273820674\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', nan), ('average_q2', nan), ('average_q_func1_loss', nan), ('average_q_func2_loss', nan), ('n_updates', 0), ('average_entropy', nan), ('temperature', 1.0)]\n","/usr/local/lib/python3.10/dist-packages/pfrl/replay_buffer.py:180: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n","  \"action\": torch.as_tensor(\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:10000 episode:460 last_R: 10.901816870618005 average_R:17.313167241881853\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', -0.40864363), ('average_q2', -0.31835175), ('average_q_func1_loss', 3.5770280361175537), ('average_q_func2_loss', 3.407154083251953), ('n_updates', 1), ('average_entropy', 1.8670094), ('temperature', 0.9997000694274902)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 14 R: 12.377353125724964\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 13 R: 11.562269462286269\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 14 R: 12.510425566708488\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 14 R: 12.490971338093889\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 13 R: 11.768169758126334\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 14 R: 12.507410828599086\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 14 R: 12.499696423796198\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 13 R: 11.739439855704733\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 13 R: 11.743518954103411\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 14 R: 12.586582186309844\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 10.632203516755396 -> 12.178583749945322\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/trying_something_out/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:11000 episode:483 last_R: 221.08340186178327 average_R:22.39872067961021\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 11.253536), ('average_q2', 11.24712), ('average_q_func1_loss', 0.5904596161842346), ('average_q_func2_loss', 0.5919008205831051), ('n_updates', 1001), ('average_entropy', 1.7414479), ('temperature', 0.7565744519233704)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:12000 episode:495 last_R: 248.02829427410353 average_R:32.39234121459131\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 19.721992), ('average_q2', 19.720484), ('average_q_func1_loss', 1.2545633116364479), ('average_q_func2_loss', 1.2446636107563973), ('n_updates', 2001), ('average_entropy', 1.0776863), ('temperature', 0.5951590538024902)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:13000 episode:506 last_R: 211.5029397546574 average_R:50.96974445873791\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 28.556822), ('average_q2', 28.544165), ('average_q_func1_loss', 2.5555816167593), ('average_q_func2_loss', 2.520399731397629), ('n_updates', 3001), ('average_entropy', 0.6891195), ('temperature', 0.48098644614219666)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:14000 episode:515 last_R: 242.26225078414146 average_R:68.5696377610042\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 38.269657), ('average_q2', 38.27586), ('average_q_func1_loss', 3.875907627940178), ('average_q_func2_loss', 3.843571126461029), ('n_updates', 4001), ('average_entropy', 0.34281367), ('temperature', 0.3931860327720642)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:15000 episode:525 last_R: 310.96828608697473 average_R:91.53090822997584\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 48.495102), ('average_q2', 48.577858), ('average_q_func1_loss', 5.441526448726654), ('average_q_func2_loss', 5.383256487846374), ('n_updates', 5001), ('average_entropy', -0.13570656), ('temperature', 0.32347342371940613)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 127 R: 307.7560825539809\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 126 R: 306.13299310357723\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 126 R: 305.6431755279321\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 126 R: 306.76702126905684\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 126 R: 305.6121982688636\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 127 R: 308.23730619018795\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 127 R: 308.0816961993302\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 127 R: 308.3932619776517\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 126 R: 305.9452522640532\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 127 R: 308.10200089091376\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 12.178583749945322 -> 307.06709882455476\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/trying_something_out/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:16000 episode:534 last_R: 313.77819564049884 average_R:118.55326674972153\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 56.819393), ('average_q2', 56.747932), ('average_q_func1_loss', 7.519854925870895), ('average_q_func2_loss', 7.371223626136779), ('n_updates', 6001), ('average_entropy', -0.9949854), ('temperature', 0.269588828086853)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:17000 episode:540 last_R: 313.67103454665147 average_R:136.3215249330291\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 66.95827), ('average_q2', 67.01706), ('average_q_func1_loss', 9.314689056873322), ('average_q_func2_loss', 9.046392769813538), ('n_updates', 7001), ('average_entropy', -1.2260629), ('temperature', 0.22746190428733826)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:18000 episode:549 last_R: 313.6946266640778 average_R:163.7438022423909\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 77.1091), ('average_q2', 77.15175), ('average_q_func1_loss', 10.091459333896637), ('average_q_func2_loss', 9.965533900260926), ('n_updates', 8001), ('average_entropy', -1.2975627), ('temperature', 0.19198893010616302)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:19000 episode:555 last_R: 322.75887761944716 average_R:182.1710495595368\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 83.517555), ('average_q2', 83.41077), ('average_q_func1_loss', 10.742077116966248), ('average_q_func2_loss', 10.490492115020752), ('n_updates', 9001), ('average_entropy', -1.7476387), ('temperature', 0.1611812263727188)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:20000 episode:565 last_R: 313.1718118185678 average_R:210.93772569074193\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 90.456726), ('average_q2', 90.5105), ('average_q_func1_loss', 12.676734428405762), ('average_q_func2_loss', 12.455793361663819), ('n_updates', 10001), ('average_entropy', -1.84278), ('temperature', 0.1375969797372818)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 123 R: 312.33400076836665\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 120 R: 306.6463420056573\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 122 R: 310.76199646421065\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 123 R: 312.20048571306154\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 122 R: 310.1733407492707\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 121 R: 309.08245998375367\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 122 R: 309.5682134273817\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 120 R: 307.31239446567236\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 121 R: 309.0236925617896\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 122 R: 311.14137298333344\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 307.06709882455476 -> 309.82442991224974\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/trying_something_out/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:21000 episode:572 last_R: 301.08813948687657 average_R:231.35251577169754\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 97.162964), ('average_q2', 97.012566), ('average_q_func1_loss', 12.81640212059021), ('average_q_func2_loss', 12.620034055709839), ('n_updates', 11001), ('average_entropy', -2.2934277), ('temperature', 0.11887317895889282)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:22000 episode:580 last_R: 303.42660145334446 average_R:252.14285652912903\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 103.37377), ('average_q2', 103.3495), ('average_q_func1_loss', 15.142481527328492), ('average_q_func2_loss', 14.598786673545838), ('n_updates', 12001), ('average_entropy', -2.8046768), ('temperature', 0.1063835471868515)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:23000 episode:590 last_R: 270.83817506489765 average_R:272.15568148233933\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 110.70658), ('average_q2', 110.94813), ('average_q_func1_loss', 14.181349792480468), ('average_q_func2_loss', 13.581692271232605), ('n_updates', 13001), ('average_entropy', -2.7240903), ('temperature', 0.09600132703781128)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:24000 episode:596 last_R: 296.8557175588428 average_R:283.02865291103944\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 114.108215), ('average_q2', 114.09949), ('average_q_func1_loss', 17.027924957275392), ('average_q_func2_loss', 16.208301243782042), ('n_updates', 14001), ('average_entropy', -3.0770035), ('temperature', 0.08788514137268066)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:25000 episode:605 last_R: 306.6496993267676 average_R:294.187245287652\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 118.79449), ('average_q2', 118.85478), ('average_q_func1_loss', 16.866260242462157), ('average_q_func2_loss', 16.41508472442627), ('n_updates', 15001), ('average_entropy', -2.8947663), ('temperature', 0.08805045485496521)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 136 R: 322.71659325932154\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 132 R: 317.3859676398535\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 127 R: 313.453165402227\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 135 R: 320.79757048074555\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 131 R: 318.33807261127816\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 136 R: 321.9717087622091\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 133 R: 319.0914432498229\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 135 R: 322.4119101480958\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 129 R: 316.3014191376824\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 135 R: 322.6292743260634\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 309.82442991224974 -> 319.50971250172995\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/trying_something_out/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:26000 episode:610 last_R: 333.32335208948115 average_R:300.73212914982486\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 122.89742), ('average_q2', 122.852356), ('average_q_func1_loss', 15.573335690498352), ('average_q_func2_loss', 14.597856369018555), ('n_updates', 16001), ('average_entropy', -3.1873407), ('temperature', 0.0912756696343422)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:27000 episode:620 last_R: 342.3868016797293 average_R:311.5343325751021\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.37062), ('average_q2', 127.37844), ('average_q_func1_loss', 14.12030448436737), ('average_q_func2_loss', 13.589916784763336), ('n_updates', 17001), ('average_entropy', -2.8726215), ('temperature', 0.09521574527025223)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:28000 episode:625 last_R: 349.0970114996807 average_R:315.6386412678225\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 127.614204), ('average_q2', 127.77494), ('average_q_func1_loss', 16.693481726646425), ('average_q_func2_loss', 15.636080701351165), ('n_updates', 18001), ('average_entropy', -3.0814304), ('temperature', 0.09364257007837296)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:29000 episode:632 last_R: 311.7130562875773 average_R:316.5455677163739\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 131.78725), ('average_q2', 131.85156), ('average_q_func1_loss', 14.647498414516448), ('average_q_func2_loss', 13.96737286567688), ('n_updates', 19001), ('average_entropy', -2.9998872), ('temperature', 0.08912573754787445)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:30000 episode:640 last_R: 319.1734910039009 average_R:317.2864397038202\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 131.32872), ('average_q2', 131.18546), ('average_q_func1_loss', 15.433918843269348), ('average_q_func2_loss', 14.578337666988373), ('n_updates', 20001), ('average_entropy', -3.0286512), ('temperature', 0.08592475950717926)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 152 R: 334.4947679309741\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 153 R: 336.21671957511563\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 155 R: 338.40184158027523\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 153 R: 334.8740601251819\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 152 R: 334.00532616232647\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 155 R: 338.9551330274447\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 154 R: 336.9958648221601\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 155 R: 338.43856413781185\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 152 R: 333.9699539701215\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 155 R: 340.3209877934632\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 319.50971250172995 -> 336.66732191248747\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/trying_something_out/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:31000 episode:647 last_R: 319.21823022905943 average_R:317.64941818674674\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 135.74088), ('average_q2', 135.7509), ('average_q_func1_loss', 16.371160101890563), ('average_q_func2_loss', 15.0917214345932), ('n_updates', 21001), ('average_entropy', -2.9345112), ('temperature', 0.08010860532522202)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:32000 episode:654 last_R: 382.3094697912976 average_R:318.4833544404921\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 133.6966), ('average_q2', 133.75575), ('average_q_func1_loss', 15.265271589756011), ('average_q_func2_loss', 14.33596533536911), ('n_updates', 22001), ('average_entropy', -3.4363124), ('temperature', 0.07564666122198105)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:33000 episode:661 last_R: 322.8703001138582 average_R:320.65608387956627\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.0793), ('average_q2', 136.17622), ('average_q_func1_loss', 13.368334591388702), ('average_q_func2_loss', 12.600011026859283), ('n_updates', 23001), ('average_entropy', -3.082977), ('temperature', 0.0707317516207695)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:34000 episode:667 last_R: 349.38704281955586 average_R:322.8271303936567\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.16646), ('average_q2', 137.96394), ('average_q_func1_loss', 15.063856873512268), ('average_q_func2_loss', 13.45201908349991), ('n_updates', 24001), ('average_entropy', -2.8840847), ('temperature', 0.07130765169858932)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:35000 episode:675 last_R: 331.8614195539201 average_R:324.85618016074665\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.70439), ('average_q2', 139.21666), ('average_q_func1_loss', 11.632058734893798), ('average_q_func2_loss', 11.315068061351775), ('n_updates', 25001), ('average_entropy', -3.2327275), ('temperature', 0.07614036649465561)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 126 R: 335.6832202799229\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 126 R: 336.09023239338\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 126 R: 333.85722928197566\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 125 R: 332.73280270541846\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 126 R: 337.24413966778604\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 127 R: 337.25306847103815\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 127 R: 337.23538395963783\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 126 R: 336.34254899976105\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 127 R: 336.3227744602243\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 127 R: 336.01632117152485\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:36000 episode:682 last_R: 343.74431941361655 average_R:326.97263319774027\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.968), ('average_q2', 137.23114), ('average_q_func1_loss', 11.86678573846817), ('average_q_func2_loss', 10.786194326877593), ('n_updates', 26001), ('average_entropy', -2.888612), ('temperature', 0.0740143284201622)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:37000 episode:691 last_R: 329.5827536716563 average_R:330.63355436800174\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.78633), ('average_q2', 137.84314), ('average_q_func1_loss', 10.738645737171174), ('average_q_func2_loss', 10.05367390871048), ('n_updates', 27001), ('average_entropy', -2.7441518), ('temperature', 0.06960367411375046)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:38000 episode:698 last_R: 334.81454331096825 average_R:333.63795894057745\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.6745), ('average_q2', 139.51775), ('average_q_func1_loss', 9.783827347755432), ('average_q_func2_loss', 9.404614613056182), ('n_updates', 28001), ('average_entropy', -3.1056752), ('temperature', 0.06563124805688858)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:39000 episode:706 last_R: 322.4098705113258 average_R:334.87874592086024\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.13072), ('average_q2', 140.31172), ('average_q_func1_loss', 10.39807567834854), ('average_q_func2_loss', 9.538797764778137), ('n_updates', 29001), ('average_entropy', -3.1741674), ('temperature', 0.06696260720491409)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:40000 episode:713 last_R: 326.00608651434595 average_R:335.48163262526106\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.1706), ('average_q2', 140.04935), ('average_q_func1_loss', 10.38490049123764), ('average_q_func2_loss', 9.954439889192582), ('n_updates', 30001), ('average_entropy', -2.8998353), ('temperature', 0.06850982457399368)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 135 R: 327.7348470109462\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 135 R: 327.5087412457945\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 135 R: 326.9374792591528\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 136 R: 329.3101650871277\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 136 R: 329.2229804664162\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 135 R: 327.2581108340794\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 134 R: 325.0274605110598\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 136 R: 328.8177759743649\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 135 R: 327.4470755859033\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 135 R: 327.1719774890526\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:41000 episode:721 last_R: 323.2921858077914 average_R:334.4385752009923\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.58803), ('average_q2', 138.31552), ('average_q_func1_loss', 9.633463096618652), ('average_q_func2_loss', 9.108507170677186), ('n_updates', 31001), ('average_entropy', -2.8774765), ('temperature', 0.06443066895008087)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:42000 episode:728 last_R: 322.9667907369674 average_R:333.5415465873407\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.05147), ('average_q2', 135.91936), ('average_q_func1_loss', 9.300518553256989), ('average_q_func2_loss', 8.648835837841034), ('n_updates', 32001), ('average_entropy', -3.5274377), ('temperature', 0.06087462976574898)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:43000 episode:741 last_R: 8.901998391304552 average_R:303.68393118345693\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.16), ('average_q2', 138.0136), ('average_q_func1_loss', 10.819485573768615), ('average_q_func2_loss', 10.137317140102386), ('n_updates', 33001), ('average_entropy', -2.7327037), ('temperature', 0.06688201427459717)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:44000 episode:762 last_R: 11.95590649395947 average_R:240.9703749470253\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.1228), ('average_q2', 142.18906), ('average_q_func1_loss', 11.801494793891907), ('average_q_func2_loss', 10.973727140426636), ('n_updates', 34001), ('average_entropy', -3.2098877), ('temperature', 0.0635812059044838)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:45000 episode:774 last_R: 52.08592989806128 average_R:203.78946239692507\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.1816), ('average_q2', 138.93707), ('average_q_func1_loss', 10.251204249858857), ('average_q_func2_loss', 8.800519659519196), ('n_updates', 35001), ('average_entropy', -2.7536082), ('temperature', 0.06576887518167496)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 78 R: 37.82089619528333\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 54 R: 24.564205165728104\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 75 R: 34.402474503865214\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 81 R: 39.963328187253666\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 53 R: 23.753240170152136\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 78 R: 37.12067692098905\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 82 R: 41.22374602594944\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 78 R: 36.95827592305691\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 74 R: 34.276552883992224\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 50 R: 21.28633914338675\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:46000 episode:784 last_R: 27.48515519053753 average_R:177.99933231872737\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.90627), ('average_q2', 138.8665), ('average_q_func1_loss', 12.110419082641602), ('average_q_func2_loss', 11.205194036960602), ('n_updates', 36001), ('average_entropy', -3.0986), ('temperature', 0.06329790502786636)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:47000 episode:791 last_R: 99.36123749115916 average_R:163.26737969328977\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.32646), ('average_q2', 142.28432), ('average_q_func1_loss', 12.342876889705657), ('average_q_func2_loss', 11.186593413352966), ('n_updates', 37001), ('average_entropy', -3.2411504), ('temperature', 0.07733345031738281)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:48000 episode:797 last_R: 291.1024188712588 average_R:164.93066954608986\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.1856), ('average_q2', 140.49765), ('average_q_func1_loss', 12.19668555021286), ('average_q_func2_loss', 10.772035958766937), ('n_updates', 38001), ('average_entropy', -3.1232085), ('temperature', 0.08225894719362259)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:49000 episode:806 last_R: 298.90214289060526 average_R:162.00216798704272\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.79457), ('average_q2', 143.76125), ('average_q_func1_loss', 11.133184370994568), ('average_q_func2_loss', 9.95497644662857), ('n_updates', 39001), ('average_entropy', -2.9830062), ('temperature', 0.07788506895303726)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:50000 episode:813 last_R: 328.47466068380544 average_R:158.75933030542075\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.51996), ('average_q2', 144.3741), ('average_q_func1_loss', 11.63954151391983), ('average_q_func2_loss', 10.447328281402587), ('n_updates', 40001), ('average_entropy', -2.6921065), ('temperature', 0.07112959027290344)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 149 R: 331.2549890515512\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 150 R: 333.83115512424394\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 149 R: 331.6588253246327\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 150 R: 333.5375541959087\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 150 R: 333.66867675132863\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 149 R: 332.011714721969\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 150 R: 333.5672309336251\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 149 R: 331.2728207496429\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 149 R: 331.8032225568904\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 149 R: 331.89775880133936\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:51000 episode:820 last_R: 326.44992063588586 average_R:157.84623247505348\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 144.32394), ('average_q2', 144.30756), ('average_q_func1_loss', 11.322712676525116), ('average_q_func2_loss', 10.036910634040833), ('n_updates', 41001), ('average_entropy', -2.8727138), ('temperature', 0.06752175092697144)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:52000 episode:827 last_R: 322.79784607435 average_R:157.8696152206834\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.26256), ('average_q2', 142.05531), ('average_q_func1_loss', 9.729602944850921), ('average_q_func2_loss', 8.529123861789703), ('n_updates', 42001), ('average_entropy', -2.9061852), ('temperature', 0.06083399057388306)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:53000 episode:834 last_R: 379.35834449181294 average_R:167.2255777846685\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.5405), ('average_q2', 145.71683), ('average_q_func1_loss', 8.407283442020416), ('average_q_func2_loss', 7.34176616191864), ('n_updates', 43001), ('average_entropy', -2.8632877), ('temperature', 0.056007277220487595)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:54000 episode:841 last_R: 321.98792259551925 average_R:189.6711790706195\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.77615), ('average_q2', 143.80441), ('average_q_func1_loss', 10.504662945270539), ('average_q_func2_loss', 9.014695279598236), ('n_updates', 44001), ('average_entropy', -2.862777), ('temperature', 0.05418037623167038)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:55000 episode:848 last_R: 383.7566309986662 average_R:213.81260406082515\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.06464), ('average_q2', 143.13013), ('average_q_func1_loss', 9.193164155483245), ('average_q_func2_loss', 7.995518414974213), ('n_updates', 45001), ('average_entropy', -3.241706), ('temperature', 0.052906475961208344)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 146 R: 349.23677382154904\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 149 R: 343.60268062390145\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 146 R: 348.61686123399124\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 145 R: 346.1469238264575\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 151 R: 348.775577707132\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 144 R: 345.3897435763724\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 148 R: 342.6683871043277\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 146 R: 349.31515350221224\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 147 R: 344.4814111473111\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 148 R: 342.4450891865364\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 336.66732191248747 -> 346.0678601729791\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/trying_something_out/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:56000 episode:855 last_R: 314.6677267916155 average_R:231.54066278461897\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.06572), ('average_q2', 145.24268), ('average_q_func1_loss', 9.011267220973968), ('average_q_func2_loss', 8.297875034809113), ('n_updates', 46001), ('average_entropy', -3.0594282), ('temperature', 0.05623942241072655)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:57000 episode:861 last_R: 338.45307595575446 average_R:251.1405111946268\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.89139), ('average_q2', 138.82109), ('average_q_func1_loss', 8.350775901079178), ('average_q_func2_loss', 7.386068425178528), ('n_updates', 47001), ('average_entropy', -3.1693287), ('temperature', 0.060710400342941284)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:58000 episode:868 last_R: 320.1071936279423 average_R:273.74702900269824\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.08963), ('average_q2', 141.82431), ('average_q_func1_loss', 8.047683999538421), ('average_q_func2_loss', 7.339999425411224), ('n_updates', 48001), ('average_entropy', -3.026857), ('temperature', 0.061926744878292084)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:59000 episode:876 last_R: 367.47218788685706 average_R:297.94906606072277\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.0275), ('average_q2', 141.17882), ('average_q_func1_loss', 7.285839054584503), ('average_q_func2_loss', 6.568403205871582), ('n_updates', 49001), ('average_entropy', -2.971972), ('temperature', 0.0588521882891655)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:60000 episode:882 last_R: 372.59734510677157 average_R:311.58323672462956\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.9204), ('average_q2', 138.89638), ('average_q_func1_loss', 8.463635745048522), ('average_q_func2_loss', 6.971348451375961), ('n_updates', 50001), ('average_entropy', -3.1011887), ('temperature', 0.05627206712961197)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 138 R: 340.2258398144982\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 138 R: 340.66944701144473\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 139 R: 339.664058854243\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 138 R: 340.6351457992169\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 138 R: 339.024892762952\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 138 R: 326.9422831100785\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 137 R: 339.3957967568692\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 139 R: 339.01726439348505\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 138 R: 325.97358541387973\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 138 R: 332.64395498708734\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:61000 episode:890 last_R: 360.72966976402523 average_R:331.1498774460442\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.8119), ('average_q2', 140.04453), ('average_q_func1_loss', 6.858425033092499), ('average_q_func2_loss', 6.286964087486267), ('n_updates', 51001), ('average_entropy', -2.7772517), ('temperature', 0.05421707406640053)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:62000 episode:896 last_R: 306.71971022992636 average_R:329.52665932623074\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.90019), ('average_q2', 141.79991), ('average_q_func1_loss', 7.275447635650635), ('average_q_func2_loss', 6.1120764255523685), ('n_updates', 52001), ('average_entropy', -2.8579342), ('temperature', 0.05241316929459572)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:63000 episode:904 last_R: 365.7987007333309 average_R:332.77769696964964\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.14357), ('average_q2', 143.11948), ('average_q_func1_loss', 7.20016886472702), ('average_q_func2_loss', 6.1605855810642245), ('n_updates', 53001), ('average_entropy', -2.7605445), ('temperature', 0.05116531625390053)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:64000 episode:911 last_R: 332.50986996989855 average_R:337.356890333998\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 143.14806), ('average_q2', 143.20047), ('average_q_func1_loss', 6.725780615806579), ('average_q_func2_loss', 5.530961287021637), ('n_updates', 54001), ('average_entropy', -2.936053), ('temperature', 0.05115566402673721)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:65000 episode:918 last_R: 381.4512043873443 average_R:341.3338410889241\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.83536), ('average_q2', 138.65794), ('average_q_func1_loss', 6.926830668449401), ('average_q_func2_loss', 5.643179242610931), ('n_updates', 55001), ('average_entropy', -2.9092386), ('temperature', 0.049090027809143066)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 136 R: 321.5090664294291\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 135 R: 319.3662089189877\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 141 R: 345.62784940190454\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 137 R: 327.81829983691193\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 138 R: 330.56835109939846\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 137 R: 326.1192866709656\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 138 R: 330.3428057485797\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 141 R: 346.52639918769324\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 140 R: 337.83986628493795\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 133 R: 316.9478477432232\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:66000 episode:926 last_R: 368.0042359923572 average_R:345.24480871594903\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.6641), ('average_q2', 137.92926), ('average_q_func1_loss', 6.141609951257705), ('average_q_func2_loss', 4.829242721796036), ('n_updates', 56001), ('average_entropy', -3.160205), ('temperature', 0.049585286527872086)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:67000 episode:932 last_R: 383.0462192073381 average_R:346.3676810177327\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.45033), ('average_q2', 140.3174), ('average_q_func1_loss', 6.50614563703537), ('average_q_func2_loss', 5.848977128267288), ('n_updates', 57001), ('average_entropy', -2.6602788), ('temperature', 0.04943552613258362)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:68000 episode:938 last_R: 383.6079784352958 average_R:348.0666908526328\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.5742), ('average_q2', 139.72772), ('average_q_func1_loss', 5.662601945400238), ('average_q_func2_loss', 4.3769461846351625), ('n_updates', 58001), ('average_entropy', -3.1303017), ('temperature', 0.048051923513412476)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:69000 episode:946 last_R: 380.20135302551773 average_R:348.403553168941\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.54666), ('average_q2', 140.60928), ('average_q_func1_loss', 4.763843376636505), ('average_q_func2_loss', 4.168293752670288), ('n_updates', 59001), ('average_entropy', -2.961922), ('temperature', 0.048107508569955826)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:70000 episode:953 last_R: 376.8420316166486 average_R:348.5133539060366\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.10837), ('average_q2', 140.00368), ('average_q_func1_loss', 5.142037695646286), ('average_q_func2_loss', 4.341610614061356), ('n_updates', 60001), ('average_entropy', -3.0052104), ('temperature', 0.04553212597966194)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 163 R: 358.5023039122285\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 193 R: 375.56085680493567\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 160 R: 354.2495900797775\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 162 R: 358.468443299067\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 162 R: 353.05024384477895\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 190 R: 372.7195661034814\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 160 R: 353.69387185903565\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 160 R: 345.8343160189118\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 168 R: 363.5644082162072\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 161 R: 344.9640001806404\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 346.0678601729791 -> 358.0607600319064\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/trying_something_out/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:71000 episode:960 last_R: 378.0619431632942 average_R:350.48007132287216\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 136.28981), ('average_q2', 136.31006), ('average_q_func1_loss', 5.406954512596131), ('average_q_func2_loss', 4.296256960630417), ('n_updates', 61001), ('average_entropy', -2.9455547), ('temperature', 0.04615071415901184)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:72000 episode:966 last_R: 319.2746463601828 average_R:351.84341015467857\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.45787), ('average_q2', 139.41606), ('average_q_func1_loss', 5.731360788345337), ('average_q_func2_loss', 4.702223318815231), ('n_updates', 62001), ('average_entropy', -3.1662176), ('temperature', 0.04588690027594566)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:73000 episode:973 last_R: 391.6804594996455 average_R:353.76099152231006\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.28238), ('average_q2', 138.07484), ('average_q_func1_loss', 4.250938014984131), ('average_q_func2_loss', 3.469392077922821), ('n_updates', 63001), ('average_entropy', -3.0263393), ('temperature', 0.04794258624315262)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:74000 episode:980 last_R: 410.01106278917615 average_R:358.1799416780263\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.36397), ('average_q2', 139.21788), ('average_q_func1_loss', 4.5883088999986645), ('average_q_func2_loss', 3.540499817728996), ('n_updates', 64001), ('average_entropy', -3.0824366), ('temperature', 0.05007586255669594)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:75000 episode:986 last_R: 414.4104151831498 average_R:360.51241990638397\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.59546), ('average_q2', 137.31198), ('average_q_func1_loss', 4.473126370906829), ('average_q_func2_loss', 3.7726410675048827), ('n_updates', 65001), ('average_entropy', -3.4107842), ('temperature', 0.05601881444454193)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 149 R: 412.4191945307301\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 148 R: 412.3630982255548\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 148 R: 411.08143338312993\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 148 R: 409.9073556264533\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 148 R: 411.1684482516677\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 148 R: 409.90824307679486\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 148 R: 411.63000744655113\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 147 R: 411.677903793534\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 148 R: 413.09233137960894\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 150 R: 417.21730497612566\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 358.0607600319064 -> 412.04653206901503\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/trying_something_out/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:76000 episode:993 last_R: 397.32558548772977 average_R:364.9304729466021\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.73262), ('average_q2', 137.76907), ('average_q_func1_loss', 5.083905739784241), ('average_q_func2_loss', 3.875617275238037), ('n_updates', 66001), ('average_entropy', -2.9743965), ('temperature', 0.0582633912563324)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:77000 episode:1000 last_R: 401.8403227137082 average_R:369.49325832177203\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.39209), ('average_q2', 137.42955), ('average_q_func1_loss', 4.864974520206451), ('average_q_func2_loss', 3.8501882833242416), ('n_updates', 67001), ('average_entropy', -2.849388), ('temperature', 0.053179603070020676)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:78000 episode:1006 last_R: 301.1159219774413 average_R:375.2130946361355\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.60599), ('average_q2', 140.31847), ('average_q_func1_loss', 4.587724115848541), ('average_q_func2_loss', 3.6377417701482773), ('n_updates', 68001), ('average_entropy', -2.8638248), ('temperature', 0.04660911113023758)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:79000 episode:1013 last_R: 286.5207824549138 average_R:377.4158791831993\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 139.75005), ('average_q2', 139.65823), ('average_q_func1_loss', 5.292860091924667), ('average_q_func2_loss', 4.2741979885101316), ('n_updates', 69001), ('average_entropy', -3.078904), ('temperature', 0.045823268592357635)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:80000 episode:1018 last_R: 591.897155788225 average_R:384.92751728644254\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 137.90471), ('average_q2', 137.9839), ('average_q_func1_loss', 4.155413080453872), ('average_q_func2_loss', 3.3297926115989687), ('n_updates', 70001), ('average_entropy', -2.7886791), ('temperature', 0.04530900716781616)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 149 R: 413.73966076885733\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 148 R: 413.3051955967206\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 147 R: 414.13786102135424\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 150 R: 415.01220641132113\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 148 R: 413.40335116221945\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 148 R: 413.063246347234\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 147 R: 410.42312291669805\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 147 R: 411.343897099413\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 147 R: 409.7443280458291\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 147 R: 409.46273486035744\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 412.04653206901503 -> 412.3635604230004\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/trying_something_out/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:81000 episode:1025 last_R: 421.0639168843066 average_R:389.1482605786268\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 140.89493), ('average_q2', 141.16478), ('average_q_func1_loss', 5.427644975185395), ('average_q_func2_loss', 4.045669983625412), ('n_updates', 71001), ('average_entropy', -2.9580152), ('temperature', 0.04584749788045883)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:82000 episode:1032 last_R: 426.25620099511036 average_R:395.9711550025736\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 142.04834), ('average_q2', 141.8913), ('average_q_func1_loss', 4.542074496746063), ('average_q_func2_loss', 3.8135988330841064), ('n_updates', 72001), ('average_entropy', -3.020772), ('temperature', 0.047736044973134995)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:83000 episode:1038 last_R: 451.35820599996583 average_R:402.2697401641124\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.3094), ('average_q2', 141.35179), ('average_q_func1_loss', 4.22448539674282), ('average_q_func2_loss', 3.4491927337646486), ('n_updates', 73001), ('average_entropy', -3.0136127), ('temperature', 0.04905154928565025)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:84000 episode:1042 last_R: 439.40568377427115 average_R:408.1362168386172\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.97789), ('average_q2', 142.27953), ('average_q_func1_loss', 4.100272988080978), ('average_q_func2_loss', 3.2836903619766233), ('n_updates', 74001), ('average_entropy', -3.1302476), ('temperature', 0.0514509491622448)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:85000 episode:1048 last_R: 483.7705067059562 average_R:421.3706883563495\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 138.96371), ('average_q2', 139.0719), ('average_q_func1_loss', 4.926841659545898), ('average_q_func2_loss', 4.2528559076786046), ('n_updates', 75001), ('average_entropy', -3.2740238), ('temperature', 0.05389481037855148)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 209 R: 607.162264833896\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 208 R: 673.5665778652731\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 209 R: 678.3220680900913\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 209 R: 658.1336381929672\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 207 R: 678.8988593067996\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 210 R: 661.8152638921647\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 198 R: 549.9883592096093\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 209 R: 681.9856597209123\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 209 R: 598.8483982128641\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 208 R: 670.3677205133716\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 412.3635604230004 -> 645.9088809837949\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/trying_something_out/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:86000 episode:1053 last_R: 462.7711833088284 average_R:430.5330246388435\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.71051), ('average_q2', 145.67746), ('average_q_func1_loss', 4.9587326741218565), ('average_q_func2_loss', 4.026176147460937), ('n_updates', 76001), ('average_entropy', -3.08267), ('temperature', 0.054809704422950745)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:87000 episode:1058 last_R: 563.6753725647173 average_R:438.3563654410947\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.6129), ('average_q2', 145.4956), ('average_q_func1_loss', 6.2967978286743165), ('average_q_func2_loss', 4.6803865814208985), ('n_updates', 77001), ('average_entropy', -2.8105462), ('temperature', 0.05540603771805763)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:88000 episode:1064 last_R: 538.0987929540603 average_R:450.48060123129363\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 141.11012), ('average_q2', 141.20795), ('average_q_func1_loss', 4.739796742200851), ('average_q_func2_loss', 4.204398362636566), ('n_updates', 78001), ('average_entropy', -2.9951572), ('temperature', 0.05738786980509758)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:89000 episode:1069 last_R: 553.2332914523374 average_R:463.8709979292802\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 147.78506), ('average_q2', 147.75787), ('average_q_func1_loss', 3.8552785754203795), ('average_q_func2_loss', 3.3316855692863463), ('n_updates', 79001), ('average_entropy', -3.0964966), ('temperature', 0.060558680444955826)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:90000 episode:1074 last_R: 499.93093176906893 average_R:471.251765243583\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 149.32185), ('average_q2', 149.26599), ('average_q_func1_loss', 4.25999094247818), ('average_q_func2_loss', 3.7107573997974397), ('n_updates', 80001), ('average_entropy', -3.0286667), ('temperature', 0.06326358765363693)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 172 R: 529.5227164373853\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 171 R: 526.1060701214818\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 170 R: 522.388005237598\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 171 R: 525.0677982127462\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 170 R: 520.8516464070547\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 171 R: 527.3203026154739\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 168 R: 513.208419276367\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 165 R: 504.1733117022991\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 170 R: 522.8584248167462\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 171 R: 526.1417988717391\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:91000 episode:1081 last_R: 504.3313593471985 average_R:480.1172946308954\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 145.689), ('average_q2', 145.85562), ('average_q_func1_loss', 4.755161371231079), ('average_q_func2_loss', 4.05402601480484), ('n_updates', 81001), ('average_entropy', -3.0114648), ('temperature', 0.06519994884729385)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:92000 episode:1086 last_R: 613.4217205810841 average_R:489.1183674081909\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 157.19101), ('average_q2', 157.29584), ('average_q_func1_loss', 5.0715578281879425), ('average_q_func2_loss', 4.368080396652221), ('n_updates', 82001), ('average_entropy', -3.1035044), ('temperature', 0.06533674895763397)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:93000 episode:1091 last_R: 639.9446934363423 average_R:501.67084759712185\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 153.5857), ('average_q2', 153.45757), ('average_q_func1_loss', 5.398634071350098), ('average_q_func2_loss', 4.314033591747284), ('n_updates', 83001), ('average_entropy', -2.9918232), ('temperature', 0.06789401918649673)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:94000 episode:1096 last_R: 683.5816367101803 average_R:515.2516108703812\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 156.59503), ('average_q2', 156.96129), ('average_q_func1_loss', 4.542334339618683), ('average_q_func2_loss', 3.990569896697998), ('n_updates', 84001), ('average_entropy', -3.0135581), ('temperature', 0.06726367026567459)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:95000 episode:1101 last_R: 669.5700437784595 average_R:529.1161657258317\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 159.9739), ('average_q2', 159.98892), ('average_q_func1_loss', 4.654986492395401), ('average_q_func2_loss', 4.30868644952774), ('n_updates', 85001), ('average_entropy', -3.0903478), ('temperature', 0.06732718646526337)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 212 R: 712.0594058893685\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 210 R: 708.2709151547425\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 210 R: 706.5282804401337\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 212 R: 714.7811080341564\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 213 R: 721.9393168253818\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 215 R: 727.2342121660527\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 209 R: 703.4245691212352\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 211 R: 709.3535423923274\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 210 R: 708.5713353245445\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 212 R: 714.1015958090486\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 645.9088809837949 -> 712.6264281156991\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/trying_something_out/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:96000 episode:1106 last_R: 698.6805985359694 average_R:541.6825868284691\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 163.28331), ('average_q2', 163.65436), ('average_q_func1_loss', 5.613488270044327), ('average_q_func2_loss', 5.437516462802887), ('n_updates', 86001), ('average_entropy', -3.0988677), ('temperature', 0.06875422596931458)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:97000 episode:1111 last_R: 672.1946503047498 average_R:555.5576969589205\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 161.70732), ('average_q2', 161.6588), ('average_q_func1_loss', 5.823758518695831), ('average_q_func2_loss', 4.691662304401397), ('n_updates', 87001), ('average_entropy', -2.851426), ('temperature', 0.06956055760383606)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:98000 episode:1116 last_R: 660.6395842388373 average_R:566.8594711955233\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 163.54144), ('average_q2', 163.68617), ('average_q_func1_loss', 4.812764309644699), ('average_q_func2_loss', 4.018831428289413), ('n_updates', 88001), ('average_entropy', -3.1291602), ('temperature', 0.07010349631309509)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:99000 episode:1121 last_R: 676.2019838452574 average_R:573.4256997305889\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 168.37265), ('average_q2', 168.25716), ('average_q_func1_loss', 5.280950700044632), ('average_q_func2_loss', 4.6500040769577025), ('n_updates', 89001), ('average_entropy', -3.1268318), ('temperature', 0.07203413546085358)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:100000 episode:1127 last_R: 660.354372025305 average_R:584.8368116018526\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 170.54236), ('average_q2', 170.59396), ('average_q_func1_loss', 4.201286023855209), ('average_q_func2_loss', 3.6802542293071747), ('n_updates', 90001), ('average_entropy', -2.8439527), ('temperature', 0.07327505946159363)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 190 R: 653.7340291781082\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 189 R: 648.3467866742639\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 191 R: 658.5773093172627\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 190 R: 652.9626802755921\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 189 R: 649.4856633755329\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 190 R: 652.9713958183346\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 191 R: 659.7848386913154\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 190 R: 651.2619957849964\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 190 R: 653.2496865352255\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 189 R: 647.5235177310886\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:101000 episode:1132 last_R: 669.7366141377814 average_R:594.7982320101418\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 171.745), ('average_q2', 171.7808), ('average_q_func1_loss', 5.1692305850982665), ('average_q_func2_loss', 4.3838811004161835), ('n_updates', 91001), ('average_entropy', -3.1119268), ('temperature', 0.07277747988700867)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:102000 episode:1137 last_R: 659.2728347061711 average_R:602.2939760380965\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 171.95992), ('average_q2', 171.80382), ('average_q_func1_loss', 4.759797570705413), ('average_q_func2_loss', 3.9456427550315856), ('n_updates', 92001), ('average_entropy', -3.0262551), ('temperature', 0.06990471482276917)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:103000 episode:1144 last_R: 456.3365440006647 average_R:602.7795219314023\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 175.21446), ('average_q2', 175.16235), ('average_q_func1_loss', 5.067509964704514), ('average_q_func2_loss', 4.50488752245903), ('n_updates', 93001), ('average_entropy', -3.080355), ('temperature', 0.07105043530464172)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:104000 episode:1149 last_R: 633.9541098429524 average_R:600.8145475478881\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 175.59172), ('average_q2', 175.55453), ('average_q_func1_loss', 5.553252300024033), ('average_q_func2_loss', 4.955305049419403), ('n_updates', 94001), ('average_entropy', -3.0239906), ('temperature', 0.07303473353385925)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:105000 episode:1155 last_R: 690.478466056367 average_R:604.1992321334566\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 181.1809), ('average_q2', 181.06888), ('average_q_func1_loss', 4.470796741247177), ('average_q_func2_loss', 3.7834910786151887), ('n_updates', 95001), ('average_entropy', -2.965125), ('temperature', 0.07301422208547592)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 196 R: 667.8456198937156\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 190 R: 635.5742034276569\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 198 R: 672.145614770484\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 195 R: 662.2987974024857\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 191 R: 640.9396564457629\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 194 R: 657.6353300177263\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 190 R: 633.0284017490883\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 190 R: 636.236835112898\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 194 R: 655.3254915014518\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 192 R: 644.1069572882735\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:106000 episode:1159 last_R: 696.0310795152885 average_R:608.7035086075273\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 179.608), ('average_q2', 179.5298), ('average_q_func1_loss', 5.722121229171753), ('average_q_func2_loss', 4.884733228683472), ('n_updates', 96001), ('average_entropy', -3.0764322), ('temperature', 0.07188785821199417)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:107000 episode:1164 last_R: 586.3704946823916 average_R:612.4170322026971\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 182.38533), ('average_q2', 182.32663), ('average_q_func1_loss', 5.0867115902900695), ('average_q_func2_loss', 4.134742747545243), ('n_updates', 97001), ('average_entropy', -2.8848984), ('temperature', 0.06996142119169235)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:108000 episode:1169 last_R: 664.0731416996726 average_R:614.1925636856355\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 182.3506), ('average_q2', 182.18694), ('average_q_func1_loss', 5.49942949295044), ('average_q_func2_loss', 4.233894823789597), ('n_updates', 98001), ('average_entropy', -2.8730354), ('temperature', 0.0753018707036972)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:109000 episode:1174 last_R: 659.3727112863193 average_R:621.2232343201104\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 182.60013), ('average_q2', 182.59767), ('average_q_func1_loss', 6.024491052627564), ('average_q_func2_loss', 5.760867311954498), ('n_updates', 99001), ('average_entropy', -3.0696113), ('temperature', 0.07373141497373581)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:110000 episode:1179 last_R: 617.424738037897 average_R:628.5321958693535\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 184.39845), ('average_q2', 183.99007), ('average_q_func1_loss', 5.964657788276672), ('average_q_func2_loss', 5.284959144592285), ('n_updates', 100001), ('average_entropy', -3.2263494), ('temperature', 0.07307570427656174)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 154 R: 481.17974270344746\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 155 R: 488.66527806308136\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 154 R: 483.42722549419256\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 155 R: 486.689644263746\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 154 R: 482.0223375562873\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 155 R: 486.1666355935859\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 169 R: 558.9592243258243\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 168 R: 555.7933644041257\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 153 R: 476.741408924921\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 166 R: 542.4242736786648\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:111000 episode:1184 last_R: 614.348995896594 average_R:632.952868774265\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 188.59325), ('average_q2', 188.56213), ('average_q_func1_loss', 6.100538932085037), ('average_q_func2_loss', 5.584378232955933), ('n_updates', 101001), ('average_entropy', -2.916069), ('temperature', 0.07413062453269958)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:112000 episode:1191 last_R: 621.4793742743503 average_R:632.0966260934129\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 188.01979), ('average_q2', 188.01852), ('average_q_func1_loss', 5.473924744129181), ('average_q_func2_loss', 4.340343344211578), ('n_updates', 102001), ('average_entropy', -3.0548549), ('temperature', 0.07382098585367203)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:113000 episode:1196 last_R: 654.3716053930032 average_R:631.0603133353188\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 189.53137), ('average_q2', 189.35027), ('average_q_func1_loss', 5.475767344236374), ('average_q_func2_loss', 4.099166676998139), ('n_updates', 103001), ('average_entropy', -3.1396246), ('temperature', 0.07320743054151535)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:114000 episode:1201 last_R: 677.588498513638 average_R:630.3405991663967\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 190.78629), ('average_q2', 190.85893), ('average_q_func1_loss', 6.70794365644455), ('average_q_func2_loss', 5.388838350772858), ('n_updates', 104001), ('average_entropy', -3.1237962), ('temperature', 0.07495870441198349)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:115000 episode:1206 last_R: 705.3467366009165 average_R:629.501853237342\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 190.88463), ('average_q2', 191.09555), ('average_q_func1_loss', 5.185177023410797), ('average_q_func2_loss', 4.418886816501617), ('n_updates', 105001), ('average_entropy', -3.0136597), ('temperature', 0.07773639261722565)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 196 R: 676.6737328598381\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 194 R: 662.9043753647742\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 194 R: 660.4221878024466\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 192 R: 652.7082553659442\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 195 R: 667.3039858244884\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 195 R: 669.4249242497237\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 194 R: 661.3912292119168\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 193 R: 656.2625551068629\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 195 R: 666.7092648070824\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 197 R: 676.3647180394313\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:116000 episode:1211 last_R: 711.5656759051093 average_R:630.3691343228151\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 192.3965), ('average_q2', 192.39383), ('average_q_func1_loss', 6.184059625864029), ('average_q_func2_loss', 5.318731387853623), ('n_updates', 106001), ('average_entropy', -3.0132992), ('temperature', 0.0778016597032547)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:117000 episode:1216 last_R: 701.6401286653634 average_R:633.5088003307674\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 190.35559), ('average_q2', 190.24329), ('average_q_func1_loss', 5.978030415773392), ('average_q_func2_loss', 4.7748029577732085), ('n_updates', 107001), ('average_entropy', -3.062475), ('temperature', 0.07633998990058899)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:118000 episode:1220 last_R: 712.577352200614 average_R:636.2330052242633\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 196.37126), ('average_q2', 196.55109), ('average_q_func1_loss', 5.539057073593139), ('average_q_func2_loss', 4.843308544158935), ('n_updates', 108001), ('average_entropy', -2.9758031), ('temperature', 0.0744284987449646)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:119000 episode:1224 last_R: 721.9965045416812 average_R:640.5921480847156\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 192.23358), ('average_q2', 192.04366), ('average_q_func1_loss', 6.0407502508163455), ('average_q_func2_loss', 5.363268262147903), ('n_updates', 109001), ('average_entropy', -2.9503586), ('temperature', 0.0718454122543335)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:120000 episode:1229 last_R: 704.7901851530618 average_R:645.1510308555926\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 195.38705), ('average_q2', 195.79059), ('average_q_func1_loss', 5.639093374013901), ('average_q_func2_loss', 5.121938889026642), ('n_updates', 110001), ('average_entropy', -3.0715888), ('temperature', 0.06742952018976212)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 211 R: 721.435951639696\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 207 R: 713.35472305044\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 212 R: 716.1342385870396\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 211 R: 723.1130454434514\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 195 R: 665.886337064287\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 212 R: 718.1021561262837\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 211 R: 722.4599525895202\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 206 R: 711.5107065022688\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 209 R: 719.7836042876695\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 209 R: 717.140433667494\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 712.6264281156991 -> 712.892114895815\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/trying_something_out/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:121000 episode:1234 last_R: 716.8557307305603 average_R:647.4425853144908\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 193.00513), ('average_q2', 193.0319), ('average_q_func1_loss', 5.125120854377746), ('average_q_func2_loss', 4.087509180307388), ('n_updates', 111001), ('average_entropy', -3.0275578), ('temperature', 0.06827975809574127)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:122000 episode:1239 last_R: 753.4359487096006 average_R:656.5620664316197\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 196.11694), ('average_q2', 196.15848), ('average_q_func1_loss', 4.640300112962723), ('average_q_func2_loss', 3.738479460477829), ('n_updates', 112001), ('average_entropy', -3.0594683), ('temperature', 0.06817011535167694)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:123000 episode:1244 last_R: 697.2520607829446 average_R:667.4498275521812\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 198.91327), ('average_q2', 199.00981), ('average_q_func1_loss', 5.2030546772480015), ('average_q_func2_loss', 4.440121643543243), ('n_updates', 113001), ('average_entropy', -2.9738247), ('temperature', 0.06860291957855225)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:124000 episode:1248 last_R: 697.2233889279214 average_R:672.7323386525597\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 196.71288), ('average_q2', 196.97307), ('average_q_func1_loss', 5.040976775884628), ('average_q_func2_loss', 4.59534110546112), ('n_updates', 114001), ('average_entropy', -3.0404463), ('temperature', 0.07131537050008774)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:125000 episode:1253 last_R: 768.2602100969256 average_R:678.7318935955527\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 197.91823), ('average_q2', 198.03615), ('average_q_func1_loss', 4.836208801269532), ('average_q_func2_loss', 3.905760961771011), ('n_updates', 115001), ('average_entropy', -3.1289058), ('temperature', 0.07052917033433914)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 231 R: 769.0922603480152\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 235 R: 789.0777968375178\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 234 R: 785.6918319272964\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 231 R: 764.7180282573411\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 232 R: 769.6505019263135\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 232 R: 772.2278218024992\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 231 R: 764.6607866547585\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 232 R: 772.6142691579578\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 231 R: 764.2219572517823\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 232 R: 763.9223385261415\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 712.892114895815 -> 771.5877592689624\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/trying_something_out/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:126000 episode:1258 last_R: 631.0605028193593 average_R:683.5016204237904\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 195.7203), ('average_q2', 195.89267), ('average_q_func1_loss', 5.015353916883469), ('average_q_func2_loss', 4.040185146331787), ('n_updates', 116001), ('average_entropy', -2.9043565), ('temperature', 0.07177897542715073)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:127000 episode:1263 last_R: 698.5283983473283 average_R:684.8728698752961\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 201.69167), ('average_q2', 202.00868), ('average_q_func1_loss', 4.461476066112518), ('average_q_func2_loss', 3.675657181739807), ('n_updates', 117001), ('average_entropy', -2.9654565), ('temperature', 0.07464500516653061)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:128000 episode:1266 last_R: 827.3047406108377 average_R:689.1006505890687\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 195.97327), ('average_q2', 196.02429), ('average_q_func1_loss', 4.731563906669617), ('average_q_func2_loss', 4.407452722787857), ('n_updates', 118001), ('average_entropy', -3.0877843), ('temperature', 0.07294026017189026)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:129000 episode:1269 last_R: 903.6689950712167 average_R:694.1636065284895\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 195.96828), ('average_q2', 196.09552), ('average_q_func1_loss', 4.9409720730781554), ('average_q_func2_loss', 4.242286415100097), ('n_updates', 119001), ('average_entropy', -2.9458692), ('temperature', 0.07369431853294373)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:130000 episode:1270 last_R: 635.1272670300002 average_R:693.9276100344895\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 194.00888), ('average_q2', 194.18034), ('average_q_func1_loss', 6.820102783441544), ('average_q_func2_loss', 6.219924434423446), ('n_updates', 120001), ('average_entropy', -3.0459354), ('temperature', 0.07042882591485977)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 216 R: 740.8074953036349\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 215 R: 740.056090488311\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 217 R: 745.5913135171108\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 216 R: 741.4608255916895\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 217 R: 745.1555581112588\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 217 R: 745.505712371468\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 218 R: 747.3793563320282\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 215 R: 740.3233109653079\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 216 R: 743.5211555637429\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 216 R: 742.1827386707594\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:131000 episode:1272 last_R: 922.2305151802868 average_R:696.8375623743824\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 200.14748), ('average_q2', 200.1878), ('average_q_func1_loss', 4.850125811100006), ('average_q_func2_loss', 3.7738872456550596), ('n_updates', 121001), ('average_entropy', -2.8163216), ('temperature', 0.06897628307342529)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:132000 episode:1277 last_R: 1494.671530064942 average_R:717.5998384584087\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 195.48344), ('average_q2', 195.5344), ('average_q_func1_loss', 4.584980261325836), ('average_q_func2_loss', 4.001732094287872), ('n_updates', 122001), ('average_entropy', -2.8897572), ('temperature', 0.06702010333538055)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:133000 episode:1281 last_R: 745.7760958429125 average_R:721.3288655415829\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 198.56503), ('average_q2', 198.42763), ('average_q_func1_loss', 5.923702887296677), ('average_q_func2_loss', 4.866231865882874), ('n_updates', 123001), ('average_entropy', -3.0385892), ('temperature', 0.06721850484609604)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:134000 episode:1284 last_R: 747.677896568756 average_R:726.1846655549825\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 196.57225), ('average_q2', 196.47733), ('average_q_func1_loss', 4.12554145693779), ('average_q_func2_loss', 3.26283265709877), ('n_updates', 124001), ('average_entropy', -3.0935018), ('temperature', 0.06739179790019989)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:135000 episode:1289 last_R: 602.9535428645016 average_R:729.6762558050676\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 198.91185), ('average_q2', 199.0521), ('average_q_func1_loss', 4.214598233699799), ('average_q_func2_loss', 3.562164866924286), ('n_updates', 125001), ('average_entropy', -2.900932), ('temperature', 0.06648693233728409)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 193 R: 577.3605530456447\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 194 R: 578.3741188732496\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 193 R: 576.9295698118339\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 192 R: 573.2687056030635\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 193 R: 576.5127730865248\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 194 R: 576.3450442820017\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 194 R: 578.9270434021513\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 193 R: 573.8484242335318\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 193 R: 572.2842858798264\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 194 R: 578.1743020361407\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:136000 episode:1293 last_R: 710.7928632835877 average_R:732.9180206230257\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 201.23512), ('average_q2', 200.9962), ('average_q_func1_loss', 4.674336265325547), ('average_q_func2_loss', 4.055811002254486), ('n_updates', 126001), ('average_entropy', -2.91517), ('temperature', 0.06598684191703796)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:137000 episode:1299 last_R: 232.8379363343274 average_R:733.7734576029463\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 199.21626), ('average_q2', 199.2059), ('average_q_func1_loss', 6.629421752691269), ('average_q_func2_loss', 6.014607571363449), ('n_updates', 127001), ('average_entropy', -2.9632642), ('temperature', 0.06420949101448059)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:138000 episode:1303 last_R: 743.516282038611 average_R:736.7389791628937\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 200.6833), ('average_q2', 200.62029), ('average_q_func1_loss', 4.159292721748352), ('average_q_func2_loss', 3.6998329854011534), ('n_updates', 128001), ('average_entropy', -2.8818848), ('temperature', 0.0632096379995346)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:139000 episode:1308 last_R: 752.9059975795359 average_R:738.3064401196045\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 200.79318), ('average_q2', 200.86594), ('average_q_func1_loss', 5.896225193738937), ('average_q_func2_loss', 5.243963397741318), ('n_updates', 129001), ('average_entropy', -2.8141973), ('temperature', 0.06301209330558777)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:140000 episode:1313 last_R: 676.720900605212 average_R:738.1716815811403\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 201.07059), ('average_q2', 201.27002), ('average_q_func1_loss', 5.343909410238266), ('average_q_func2_loss', 5.021410009860992), ('n_updates', 130001), ('average_entropy', -2.9544976), ('temperature', 0.06208626925945282)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 209 R: 712.1884180354258\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 208 R: 706.3258060749088\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 207 R: 699.5101800781871\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 208 R: 704.5328847388959\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 209 R: 713.4829476408115\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 208 R: 710.7822022068148\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 209 R: 714.3625121892248\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 208 R: 710.0704287270654\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 209 R: 713.0385502493617\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 208 R: 704.0596638700558\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:141000 episode:1318 last_R: 747.1078821842134 average_R:738.9256578314452\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 200.3421), ('average_q2', 200.23135), ('average_q_func1_loss', 4.119664875268936), ('average_q_func2_loss', 4.020201524496079), ('n_updates', 131001), ('average_entropy', -2.9287884), ('temperature', 0.06171436607837677)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:142000 episode:1323 last_R: 691.321956040217 average_R:738.8816534248131\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 193.48787), ('average_q2', 193.57117), ('average_q_func1_loss', 5.0940947699546815), ('average_q_func2_loss', 4.013564814329147), ('n_updates', 132001), ('average_entropy', -3.243398), ('temperature', 0.0638149306178093)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:143000 episode:1328 last_R: 745.765832142315 average_R:738.2430405073875\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 201.21736), ('average_q2', 201.23763), ('average_q_func1_loss', 9.457165992259979), ('average_q_func2_loss', 13.016747952699662), ('n_updates', 133001), ('average_entropy', -3.1449277), ('temperature', 0.064962238073349)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:144000 episode:1333 last_R: 753.2795215972188 average_R:740.2160779425784\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 200.33432), ('average_q2', 200.36162), ('average_q_func1_loss', 4.827494951486588), ('average_q_func2_loss', 4.26279150724411), ('n_updates', 134001), ('average_entropy', -2.9667895), ('temperature', 0.06392906606197357)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:145000 episode:1336 last_R: 752.3652822420207 average_R:740.8953671985225\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 199.02144), ('average_q2', 198.80637), ('average_q_func1_loss', 4.841827640533447), ('average_q_func2_loss', 4.4637617433071135), ('n_updates', 135001), ('average_entropy', -2.8368897), ('temperature', 0.06399857252836227)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 202 R: 702.050196067535\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 222 R: 744.2249193008516\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 224 R: 735.597913166391\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 229 R: 732.1423395232307\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 223 R: 748.3639225891278\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 224 R: 741.7395458430723\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 203 R: 704.8205298201468\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 229 R: 730.309789998165\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 227 R: 727.8944183867595\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 222 R: 736.7312341718612\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:146000 episode:1341 last_R: 752.7716066378882 average_R:741.674009883008\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 203.10487), ('average_q2', 202.95068), ('average_q_func1_loss', 5.65990637421608), ('average_q_func2_loss', 5.083115648031235), ('n_updates', 136001), ('average_entropy', -2.9971063), ('temperature', 0.06300715357065201)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:147000 episode:1346 last_R: 715.0988438032919 average_R:743.1115801380273\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 202.02513), ('average_q2', 201.95587), ('average_q_func1_loss', 3.9476882362365724), ('average_q_func2_loss', 4.133420155048371), ('n_updates', 137001), ('average_entropy', -3.0011766), ('temperature', 0.06226065382361412)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:148000 episode:1352 last_R: 559.5584073491634 average_R:738.8398663300734\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 197.6482), ('average_q2', 197.65778), ('average_q_func1_loss', 5.221669393777847), ('average_q_func2_loss', 5.020444196462631), ('n_updates', 138001), ('average_entropy', -3.0217886), ('temperature', 0.06332723796367645)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:149000 episode:1357 last_R: 686.5911431086273 average_R:731.2108722870648\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 197.65607), ('average_q2', 197.4446), ('average_q_func1_loss', 5.824738037586212), ('average_q_func2_loss', 5.66877238869667), ('n_updates', 139001), ('average_entropy', -3.040565), ('temperature', 0.06333605945110321)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:150000 episode:1362 last_R: 697.2898705900578 average_R:732.1219200144701\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 204.02843), ('average_q2', 203.97862), ('average_q_func1_loss', 3.8505191576480864), ('average_q_func2_loss', 3.5925274753570555), ('n_updates', 140001), ('average_entropy', -3.036666), ('temperature', 0.062836192548275)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 205 R: 717.6367046692484\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 206 R: 720.881843332316\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 205 R: 718.3399032934836\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 204 R: 713.9954820740929\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 205 R: 719.2897912851481\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 208 R: 730.4436354402161\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 208 R: 729.2997873235278\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 208 R: 728.7872701352474\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 206 R: 721.8350046602698\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 205 R: 716.6302665235314\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:151000 episode:1367 last_R: 633.901881105351 average_R:724.2036633651999\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 203.69333), ('average_q2', 203.5222), ('average_q_func1_loss', 4.43387482047081), ('average_q_func2_loss', 3.949442719221115), ('n_updates', 141001), ('average_entropy', -2.809999), ('temperature', 0.06521037966012955)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:152000 episode:1372 last_R: 716.3525120067131 average_R:720.1669640705106\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 202.49535), ('average_q2', 202.47076), ('average_q_func1_loss', 4.370623452663422), ('average_q_func2_loss', 3.671625964641571), ('n_updates', 142001), ('average_entropy', -3.0576713), ('temperature', 0.06485795974731445)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:153000 episode:1376 last_R: 708.4553407128735 average_R:709.3178705299343\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 204.76216), ('average_q2', 205.13622), ('average_q_func1_loss', 4.720901407003403), ('average_q_func2_loss', 4.7145491945743565), ('n_updates', 143001), ('average_entropy', -2.8991375), ('temperature', 0.06488335132598877)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:154000 episode:1381 last_R: 737.0115440509586 average_R:701.0397798499232\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 204.17097), ('average_q2', 204.2452), ('average_q_func1_loss', 3.805822328329086), ('average_q_func2_loss', 3.548777021765709), ('n_updates', 144001), ('average_entropy', -3.0395927), ('temperature', 0.06665991246700287)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:155000 episode:1385 last_R: 738.8164883431806 average_R:700.3491930403204\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 208.28703), ('average_q2', 208.29312), ('average_q_func1_loss', 3.6299243998527526), ('average_q_func2_loss', 2.9826103329658507), ('n_updates', 145001), ('average_entropy', -2.9816368), ('temperature', 0.06535885483026505)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 218 R: 722.573128647464\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 221 R: 733.1238090140542\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 220 R: 728.4208635231356\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 224 R: 743.9958334968576\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 224 R: 743.894300152698\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 223 R: 737.7496970528846\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 221 R: 730.1313339570448\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 218 R: 720.8871107026217\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 223 R: 737.2254971347598\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 218 R: 719.5639701198968\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:156000 episode:1390 last_R: 738.788532308092 average_R:703.134012467652\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 206.33139), ('average_q2', 206.29828), ('average_q_func1_loss', 3.548710024356842), ('average_q_func2_loss', 3.18377431511879), ('n_updates', 146001), ('average_entropy', -3.1146758), ('temperature', 0.06506817042827606)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:157000 episode:1395 last_R: 747.0053904345265 average_R:703.4734787592633\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 205.97809), ('average_q2', 205.9734), ('average_q_func1_loss', 3.36293172955513), ('average_q_func2_loss', 3.0493378341197968), ('n_updates', 147001), ('average_entropy', -2.8861194), ('temperature', 0.06451115012168884)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:158000 episode:1400 last_R: 739.4061542560643 average_R:708.2700980915265\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 204.92097), ('average_q2', 205.04453), ('average_q_func1_loss', 5.824842712879181), ('average_q_func2_loss', 4.964285483360291), ('n_updates', 148001), ('average_entropy', -3.0959463), ('temperature', 0.06494463980197906)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:159000 episode:1405 last_R: 753.9792515961855 average_R:709.7247873541096\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 203.9131), ('average_q2', 203.82272), ('average_q_func1_loss', 3.7183056449890137), ('average_q_func2_loss', 3.526773874759674), ('n_updates', 149001), ('average_entropy', -3.0419664), ('temperature', 0.06472595781087875)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:160000 episode:1410 last_R: 757.2830008146271 average_R:711.1294236638214\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 205.9014), ('average_q2', 205.91556), ('average_q_func1_loss', 3.9963543105125425), ('average_q_func2_loss', 3.675782744884491), ('n_updates', 150001), ('average_entropy', -3.111293), ('temperature', 0.06288421154022217)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 221 R: 734.7048773144539\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 213 R: 752.6550105301089\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 219 R: 727.3126955511568\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 212 R: 747.6608749750313\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 225 R: 749.9154579116916\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 213 R: 743.7723366709043\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 212 R: 739.8997223292386\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 219 R: 737.495791160392\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 213 R: 758.0991924327976\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 213 R: 749.7931855663217\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:161000 episode:1414 last_R: 725.8383176382604 average_R:713.3668027995585\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 203.40526), ('average_q2', 203.65742), ('average_q_func1_loss', 3.942089865207672), ('average_q_func2_loss', 3.4733100998401643), ('n_updates', 151001), ('average_entropy', -2.8154082), ('temperature', 0.06406762450933456)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:162000 episode:1417 last_R: 691.9608905286757 average_R:713.1893455315394\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 206.61435), ('average_q2', 206.80208), ('average_q_func1_loss', 3.4524372243881225), ('average_q_func2_loss', 3.4799215710163116), ('n_updates', 152001), ('average_entropy', -2.956878), ('temperature', 0.06184084340929985)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:163000 episode:1421 last_R: 690.9707362014536 average_R:713.6623251597161\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 204.53802), ('average_q2', 204.74374), ('average_q_func1_loss', 5.714685512781143), ('average_q_func2_loss', 4.966046909093857), ('n_updates', 153001), ('average_entropy', -3.1279745), ('temperature', 0.062253307551145554)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:164000 episode:1426 last_R: 752.1187612184073 average_R:714.306954504868\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 205.25398), ('average_q2', 205.35284), ('average_q_func1_loss', 3.903179211616516), ('average_q_func2_loss', 3.2849305963516233), ('n_updates', 154001), ('average_entropy', -3.110519), ('temperature', 0.06380431354045868)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:165000 episode:1430 last_R: 724.4292084717036 average_R:714.3970224779964\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 208.2937), ('average_q2', 208.23637), ('average_q_func1_loss', 3.5100010299682616), ('average_q_func2_loss', 3.035265315771103), ('n_updates', 155001), ('average_entropy', -2.9362426), ('temperature', 0.0627419576048851)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 215 R: 767.8994017298502\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 217 R: 750.3861459903752\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 215 R: 765.9983945897811\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 219 R: 752.485380654876\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 215 R: 765.5836223453106\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 216 R: 746.0741259828952\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 215 R: 765.5001101657749\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 216 R: 766.6319068007687\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 214 R: 758.1180665439607\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 215 R: 762.6992569905575\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:166000 episode:1435 last_R: 731.8802063276413 average_R:713.6227662944137\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 205.74937), ('average_q2', 205.55634), ('average_q_func1_loss', 4.368412536382675), ('average_q_func2_loss', 3.666266914606094), ('n_updates', 156001), ('average_entropy', -2.8317237), ('temperature', 0.060976866632699966)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:167000 episode:1440 last_R: 766.691405293405 average_R:713.3143436727282\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 212.08026), ('average_q2', 212.36278), ('average_q_func1_loss', 3.976201277971268), ('average_q_func2_loss', 3.417416310310364), ('n_updates', 157001), ('average_entropy', -2.8443258), ('temperature', 0.06117332726716995)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:168000 episode:1445 last_R: 717.69051988993 average_R:712.2831488291154\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 206.039), ('average_q2', 206.0599), ('average_q_func1_loss', 4.631065826416016), ('average_q_func2_loss', 4.950297970771789), ('n_updates', 158001), ('average_entropy', -2.9438062), ('temperature', 0.060972053557634354)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:169000 episode:1450 last_R: 733.5780179854564 average_R:716.1681323561379\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 208.03363), ('average_q2', 207.92966), ('average_q_func1_loss', 4.616587293148041), ('average_q_func2_loss', 4.585287384986877), ('n_updates', 159001), ('average_entropy', -2.9405606), ('temperature', 0.06258860230445862)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:170000 episode:1455 last_R: 759.5992325008358 average_R:726.3809836014688\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 210.51088), ('average_q2', 210.41995), ('average_q_func1_loss', 5.591113156080246), ('average_q_func2_loss', 5.470567973852158), ('n_updates', 160001), ('average_entropy', -3.155484), ('temperature', 0.06244932860136032)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 217 R: 712.2721839220395\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 218 R: 727.305855316699\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 219 R: 727.0638805660908\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 218 R: 720.8114618669475\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 218 R: 720.1359807011164\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 216 R: 710.7297112774943\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 218 R: 726.0963967071324\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 217 R: 709.8875849562803\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 218 R: 711.376891364768\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 217 R: 708.6747201790245\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:171000 episode:1460 last_R: 746.9145096321888 average_R:729.2679620088347\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 208.66855), ('average_q2', 208.63089), ('average_q_func1_loss', 3.4621600198745726), ('average_q_func2_loss', 3.1367576253414153), ('n_updates', 161001), ('average_entropy', -3.0101206), ('temperature', 0.0637013167142868)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:172000 episode:1465 last_R: 675.8480743122349 average_R:734.6325185613819\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 208.0429), ('average_q2', 208.03366), ('average_q_func1_loss', 5.710093948841095), ('average_q_func2_loss', 5.218807053565979), ('n_updates', 162001), ('average_entropy', -2.8262703), ('temperature', 0.06327524036169052)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:173000 episode:1470 last_R: 764.6303552131635 average_R:738.9730853661956\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 210.39293), ('average_q2', 210.58119), ('average_q_func1_loss', 4.211772640943527), ('average_q_func2_loss', 3.782283842563629), ('n_updates', 163001), ('average_entropy', -3.1098897), ('temperature', 0.0628909319639206)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:174000 episode:1475 last_R: 753.6765514625995 average_R:739.8792951366687\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 213.76604), ('average_q2', 213.91869), ('average_q_func1_loss', 4.134958838224411), ('average_q_func2_loss', 3.7199715328216554), ('n_updates', 164001), ('average_entropy', -3.0328746), ('temperature', 0.06527991592884064)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:175000 episode:1479 last_R: 754.5909966599626 average_R:741.1204416129495\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 209.12102), ('average_q2', 209.14038), ('average_q_func1_loss', 4.23509343624115), ('average_q_func2_loss', 4.007648862600327), ('n_updates', 165001), ('average_entropy', -2.9510722), ('temperature', 0.06407205760478973)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 216 R: 740.0537782618575\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 215 R: 735.1854330239855\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 216 R: 737.5185058848353\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 217 R: 734.9146250700909\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 216 R: 735.1639257175435\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 216 R: 737.0711100365372\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 216 R: 738.1470283841379\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 217 R: 743.0836390186562\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 221 R: 731.8863240304084\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 216 R: 736.8539643505218\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:176000 episode:1483 last_R: 740.7470435331305 average_R:741.4881115288498\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 212.36188), ('average_q2', 212.19443), ('average_q_func1_loss', 3.8518667709827423), ('average_q_func2_loss', 3.215980863571167), ('n_updates', 166001), ('average_entropy', -3.2217846), ('temperature', 0.06383425742387772)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:177000 episode:1486 last_R: 752.6908054708249 average_R:742.1146835595299\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 215.89423), ('average_q2', 216.06544), ('average_q_func1_loss', 4.46562326669693), ('average_q_func2_loss', 4.135141289234161), ('n_updates', 167001), ('average_entropy', -2.9320738), ('temperature', 0.06306696683168411)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:178000 episode:1490 last_R: 749.4689525491896 average_R:742.9125040412363\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 211.01068), ('average_q2', 210.79526), ('average_q_func1_loss', 3.514313838481903), ('average_q_func2_loss', 3.30049782037735), ('n_updates', 168001), ('average_entropy', -2.9521964), ('temperature', 0.06160429120063782)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:179000 episode:1495 last_R: 726.1194372987349 average_R:742.085327951298\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 211.63464), ('average_q2', 211.5436), ('average_q_func1_loss', 4.37297768831253), ('average_q_func2_loss', 4.410737764835358), ('n_updates', 169001), ('average_entropy', -2.9501143), ('temperature', 0.06280747801065445)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:180000 episode:1500 last_R: 763.928924347067 average_R:742.8528640662687\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 213.5229), ('average_q2', 213.48178), ('average_q_func1_loss', 4.007405078411102), ('average_q_func2_loss', 3.5417052006721494), ('n_updates', 170001), ('average_entropy', -3.025542), ('temperature', 0.061844516545534134)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 213 R: 730.8794423528161\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 212 R: 726.850847954952\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 213 R: 735.3602592401605\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 216 R: 732.7824261676287\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 217 R: 735.9070269939276\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 212 R: 726.5185622860591\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 217 R: 737.4716863152695\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 213 R: 727.933452217887\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 216 R: 733.7891623275043\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 217 R: 736.270343607671\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:181000 episode:1505 last_R: 767.3731167851834 average_R:742.3360229380236\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 215.73343), ('average_q2', 215.81644), ('average_q_func1_loss', 4.122598323822022), ('average_q_func2_loss', 3.58647441983223), ('n_updates', 171001), ('average_entropy', -3.0012643), ('temperature', 0.06079376861453056)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:182000 episode:1510 last_R: 735.6182113736226 average_R:741.8173660566903\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 214.13095), ('average_q2', 213.88565), ('average_q_func1_loss', 5.61238337635994), ('average_q_func2_loss', 5.388772509098053), ('n_updates', 172001), ('average_entropy', -2.883366), ('temperature', 0.061793915927410126)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:183000 episode:1515 last_R: 720.5605384774607 average_R:741.2302447824455\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 214.92987), ('average_q2', 215.06288), ('average_q_func1_loss', 3.8112334561347962), ('average_q_func2_loss', 3.182520306110382), ('n_updates', 173001), ('average_entropy', -3.1493578), ('temperature', 0.06115854158997536)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:184000 episode:1520 last_R: 750.6905192753378 average_R:741.2356929290813\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 215.35896), ('average_q2', 215.32487), ('average_q_func1_loss', 4.315127265453339), ('average_q_func2_loss', 4.0004409825801845), ('n_updates', 174001), ('average_entropy', -3.2103472), ('temperature', 0.05947582796216011)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:185000 episode:1525 last_R: 684.2738465376863 average_R:736.4564603388848\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 216.30649), ('average_q2', 216.26352), ('average_q_func1_loss', 4.728682754039764), ('average_q_func2_loss', 4.04939957857132), ('n_updates', 175001), ('average_entropy', -2.7777305), ('temperature', 0.060087550431489944)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 196 R: 679.5543428902988\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 186 R: 638.5483287609053\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 184 R: 630.6653394325\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 192 R: 663.9123202207076\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 185 R: 634.0577075852518\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 192 R: 664.8504612187992\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 191 R: 658.8920649241252\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 197 R: 684.9045734134966\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 187 R: 642.996956853492\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 191 R: 658.2658410865869\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:186000 episode:1530 last_R: 688.5341765818229 average_R:732.8276253178838\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 211.7851), ('average_q2', 211.84828), ('average_q_func1_loss', 4.407696845531464), ('average_q_func2_loss', 4.14012025475502), ('n_updates', 176001), ('average_entropy', -3.0270317), ('temperature', 0.061192598193883896)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:187000 episode:1536 last_R: 518.1938763621147 average_R:723.9204837975417\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 215.22389), ('average_q2', 215.1181), ('average_q_func1_loss', 3.2364758610725404), ('average_q_func2_loss', 2.8033166015148163), ('n_updates', 177001), ('average_entropy', -2.8198938), ('temperature', 0.060561567544937134)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:188000 episode:1540 last_R: 727.6288702189977 average_R:723.616451577532\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 212.25371), ('average_q2', 212.2454), ('average_q_func1_loss', 3.995845853090286), ('average_q_func2_loss', 3.4773098957538604), ('n_updates', 178001), ('average_entropy', -3.1558945), ('temperature', 0.06117653474211693)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:189000 episode:1545 last_R: 745.5592591940713 average_R:721.7106264603922\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 218.67227), ('average_q2', 218.41376), ('average_q_func1_loss', 3.7288980758190156), ('average_q_func2_loss', 3.217663525342941), ('n_updates', 179001), ('average_entropy', -2.9552026), ('temperature', 0.06149474158883095)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:190000 episode:1550 last_R: 656.8043987741647 average_R:719.1236176794149\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 214.05002), ('average_q2', 213.99573), ('average_q_func1_loss', 5.5778229367733), ('average_q_func2_loss', 5.184920769929886), ('n_updates', 180001), ('average_entropy', -3.0726528), ('temperature', 0.06312365084886551)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 204 R: 727.2889887124215\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 200 R: 708.6423750570117\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 206 R: 725.4364536926648\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 204 R: 726.925209302047\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 201 R: 709.3566353717472\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 196 R: 685.3503267358432\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 204 R: 722.5428072255233\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 204 R: 726.8884142386496\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 205 R: 727.7936484866927\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 196 R: 688.6703805619194\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:191000 episode:1555 last_R: 761.4888627400727 average_R:717.8579706200327\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 213.94847), ('average_q2', 213.86441), ('average_q_func1_loss', 4.33584649682045), ('average_q_func2_loss', 4.06079988360405), ('n_updates', 181001), ('average_entropy', -2.9381537), ('temperature', 0.06203679367899895)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:192000 episode:1560 last_R: 747.5877966870896 average_R:714.1270293316109\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 218.55498), ('average_q2', 218.57169), ('average_q_func1_loss', 3.65609757065773), ('average_q_func2_loss', 3.1602540576457976), ('n_updates', 182001), ('average_entropy', -2.8574271), ('temperature', 0.05959322676062584)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:193000 episode:1565 last_R: 714.7961504399517 average_R:713.2359590133025\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 212.63994), ('average_q2', 212.70657), ('average_q_func1_loss', 5.6344191634655), ('average_q_func2_loss', 5.2008368247747425), ('n_updates', 183001), ('average_entropy', -3.1682286), ('temperature', 0.05836954712867737)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:194000 episode:1570 last_R: 764.4772232976243 average_R:713.6111465935251\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 214.63239), ('average_q2', 214.46873), ('average_q_func1_loss', 3.8901950514316557), ('average_q_func2_loss', 3.5195523524284362), ('n_updates', 184001), ('average_entropy', -3.1077635), ('temperature', 0.06040516495704651)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:195000 episode:1575 last_R: 736.6518601525684 average_R:712.9103920707381\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 217.64159), ('average_q2', 217.89218), ('average_q_func1_loss', 3.0742970955371858), ('average_q_func2_loss', 2.6628925025463106), ('n_updates', 185001), ('average_entropy', -2.9505424), ('temperature', 0.060957908630371094)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 217 R: 730.8481864446453\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 219 R: 748.9506525912562\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 213 R: 760.2132947841831\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 219 R: 762.9786367071138\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 214 R: 764.1015567751541\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 215 R: 765.784550827657\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 218 R: 773.9729776261723\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 220 R: 764.5962946373031\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 213 R: 760.8424881875396\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 212 R: 757.7165234937231\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:196000 episode:1580 last_R: 760.5401814296778 average_R:712.0661385558208\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 212.25713), ('average_q2', 211.97871), ('average_q_func1_loss', 3.5775983345508577), ('average_q_func2_loss', 3.0824088084697725), ('n_updates', 186001), ('average_entropy', -2.8321354), ('temperature', 0.060236770659685135)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:197000 episode:1584 last_R: 751.6648881952374 average_R:708.4195529136719\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 215.66606), ('average_q2', 215.61804), ('average_q_func1_loss', 3.969229760169983), ('average_q_func2_loss', 3.7189907884597777), ('n_updates', 187001), ('average_entropy', -2.939519), ('temperature', 0.06097135320305824)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:198000 episode:1589 last_R: 751.0515288287182 average_R:707.8320602925938\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 218.04315), ('average_q2', 218.11798), ('average_q_func1_loss', 3.287394572496414), ('average_q_func2_loss', 2.954918817281723), ('n_updates', 188001), ('average_entropy', -2.995912), ('temperature', 0.06103184074163437)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:199000 episode:1594 last_R: 737.8155393058656 average_R:708.711459819411\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 216.46227), ('average_q2', 216.42278), ('average_q_func1_loss', 3.6902460896968843), ('average_q_func2_loss', 3.1594111907482145), ('n_updates', 189001), ('average_entropy', -2.9495635), ('temperature', 0.060619696974754333)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:200000 episode:1599 last_R: 737.5433073998311 average_R:708.2640308711547\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 216.71458), ('average_q2', 216.73732), ('average_q_func1_loss', 3.4901921439170835), ('average_q_func2_loss', 3.562592053413391), ('n_updates', 190001), ('average_entropy', -3.0254412), ('temperature', 0.0604238286614418)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 221 R: 763.6976564063652\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 220 R: 765.4486925793094\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 222 R: 757.8350931734268\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 218 R: 763.850078449082\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 222 R: 748.0833382345063\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 214 R: 759.0318852777374\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 219 R: 737.866299461098\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 218 R: 766.6885470492693\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 221 R: 754.8059989112691\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 219 R: 764.7610877379507\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:201000 episode:1604 last_R: 741.0389760580886 average_R:707.6551857155007\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 216.94849), ('average_q2', 216.93097), ('average_q_func1_loss', 2.7647060334682463), ('average_q_func2_loss', 2.4251124262809753), ('n_updates', 191001), ('average_entropy', -2.9008174), ('temperature', 0.05959117040038109)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:202000 episode:1608 last_R: 748.7424520139292 average_R:708.7252801995589\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 216.55031), ('average_q2', 216.67355), ('average_q_func1_loss', 4.444313912391663), ('average_q_func2_loss', 3.429051526784897), ('n_updates', 192001), ('average_entropy', -3.003044), ('temperature', 0.05907794088125229)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:203000 episode:1613 last_R: 757.7961850403157 average_R:707.1132377621886\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 210.92026), ('average_q2', 211.19534), ('average_q_func1_loss', 3.4465932524204255), ('average_q_func2_loss', 3.1232269179821013), ('n_updates', 193001), ('average_entropy', -3.0376132), ('temperature', 0.05820652097463608)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:204000 episode:1619 last_R: 715.0118485716989 average_R:700.0035762745276\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 211.2319), ('average_q2', 211.0184), ('average_q_func1_loss', 5.640673449039459), ('average_q_func2_loss', 5.190922820568085), ('n_updates', 194001), ('average_entropy', -3.0565882), ('temperature', 0.0573178306221962)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:205000 episode:1622 last_R: 760.4773122871844 average_R:703.4789688360632\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 218.57387), ('average_q2', 218.49069), ('average_q_func1_loss', 3.895082116127014), ('average_q_func2_loss', 3.2833432161808016), ('n_updates', 195001), ('average_entropy', -2.95423), ('temperature', 0.057860810309648514)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 232 R: 764.5689124468744\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 229 R: 744.2612021891591\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 233 R: 764.7393469165921\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 229 R: 752.1722703668166\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 230 R: 764.4358674525363\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 232 R: 756.671506661516\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 230 R: 762.5125813128385\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 231 R: 760.4586960672333\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 230 R: 756.0337499289745\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 230 R: 747.5676851243694\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:206000 episode:1626 last_R: 769.5804924760358 average_R:706.8790005528602\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 213.38168), ('average_q2', 213.29121), ('average_q_func1_loss', 3.34513613820076), ('average_q_func2_loss', 2.973576737046242), ('n_updates', 196001), ('average_entropy', -3.1305094), ('temperature', 0.05583304911851883)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:207000 episode:1630 last_R: 773.1180231379562 average_R:710.5206767495805\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 214.17429), ('average_q2', 214.13284), ('average_q_func1_loss', 5.105705605745316), ('average_q_func2_loss', 4.75088693857193), ('n_updates', 197001), ('average_entropy', -3.140382), ('temperature', 0.05706166848540306)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:208000 episode:1635 last_R: 739.1902023377108 average_R:718.28209524984\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 214.30414), ('average_q2', 214.52534), ('average_q_func1_loss', 2.684684851169586), ('average_q_func2_loss', 2.5494765138626097), ('n_updates', 198001), ('average_entropy', -3.0064375), ('temperature', 0.056310005486011505)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:209000 episode:1640 last_R: 771.0176990276832 average_R:721.914812444757\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 215.95544), ('average_q2', 216.0311), ('average_q_func1_loss', 3.212099643945694), ('average_q_func2_loss', 2.745588490962982), ('n_updates', 199001), ('average_entropy', -3.1524258), ('temperature', 0.056812990456819534)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:210000 episode:1645 last_R: 760.6363002723971 average_R:724.3795725790437\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 216.94887), ('average_q2', 216.70592), ('average_q_func1_loss', 3.011552219390869), ('average_q_func2_loss', 2.6791835021972656), ('n_updates', 200001), ('average_entropy', -3.008977), ('temperature', 0.05624167621135712)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 219 R: 751.5450769239353\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 219 R: 754.6743076745181\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 219 R: 757.4893601890758\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 219 R: 762.5951502582179\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 219 R: 754.2168886321559\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 218 R: 750.0573318936599\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 217 R: 748.2825182004804\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 220 R: 761.6678891959131\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 220 R: 759.8525770560695\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 220 R: 762.8602271356775\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:211000 episode:1649 last_R: 725.056317362913 average_R:724.3283716069079\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 215.80707), ('average_q2', 215.61784), ('average_q_func1_loss', 4.883663268089294), ('average_q_func2_loss', 4.377314846515656), ('n_updates', 201001), ('average_entropy', -3.061596), ('temperature', 0.05667215958237648)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:212000 episode:1654 last_R: 754.1973763373196 average_R:727.6139502873478\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 212.12962), ('average_q2', 212.07959), ('average_q_func1_loss', 4.489769710302353), ('average_q_func2_loss', 4.4324094939231875), ('n_updates', 202001), ('average_entropy', -2.9538052), ('temperature', 0.05582239106297493)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:213000 episode:1659 last_R: 770.9869248835261 average_R:732.2310612451566\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 215.86179), ('average_q2', 215.6871), ('average_q_func1_loss', 4.572043628692627), ('average_q_func2_loss', 4.000957003831863), ('n_updates', 203001), ('average_entropy', -2.9888878), ('temperature', 0.05637606978416443)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:214000 episode:1664 last_R: 746.0418567569363 average_R:733.4987572198498\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 214.90051), ('average_q2', 214.75531), ('average_q_func1_loss', 3.0285972583293916), ('average_q_func2_loss', 2.9054572129249574), ('n_updates', 204001), ('average_entropy', -3.180781), ('temperature', 0.055389609187841415)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:215000 episode:1669 last_R: 773.1443187948922 average_R:733.3201100238059\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 215.5586), ('average_q2', 215.73824), ('average_q_func1_loss', 3.434701532125473), ('average_q_func2_loss', 2.901361688375473), ('n_updates', 205001), ('average_entropy', -2.9816024), ('temperature', 0.054137226194143295)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 224 R: 775.999489656841\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 224 R: 772.2752897891974\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 225 R: 774.5077318447582\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 224 R: 773.7776788199402\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 225 R: 773.3015079144483\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 223 R: 771.5522763470652\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 225 R: 774.9676807518196\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 224 R: 771.6194258054985\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 225 R: 775.7847132013799\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 225 R: 775.2581831783202\n","INFO:pfrl.experiments.train_agent_batch:The best score is updated 771.5877592689624 -> 773.9043977309268\n","INFO:pfrl.experiments.train_agent_batch:Saved the agent to logs/trying_something_out/best\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:216000 episode:1673 last_R: 761.1622435379757 average_R:734.1354254291836\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 213.49492), ('average_q2', 213.48372), ('average_q_func1_loss', 2.4404279911518096), ('average_q_func2_loss', 2.290946484208107), ('n_updates', 206001), ('average_entropy', -3.1791494), ('temperature', 0.05597986653447151)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:217000 episode:1678 last_R: 731.8546186611777 average_R:734.020445270236\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 219.015), ('average_q2', 218.92686), ('average_q_func1_loss', 3.651829843521118), ('average_q_func2_loss', 3.565046557188034), ('n_updates', 207001), ('average_entropy', -2.8988736), ('temperature', 0.05481741949915886)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:218000 episode:1681 last_R: 748.7033953625623 average_R:736.0343077691384\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 213.61047), ('average_q2', 213.72809), ('average_q_func1_loss', 3.357938494682312), ('average_q_func2_loss', 2.922910134792328), ('n_updates', 208001), ('average_entropy', -2.9746683), ('temperature', 0.05523156374692917)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:219000 episode:1685 last_R: 801.8286116751918 average_R:738.9269925517157\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 214.666), ('average_q2', 214.66844), ('average_q_func1_loss', 6.040026118755341), ('average_q_func2_loss', 5.499159578084946), ('n_updates', 209001), ('average_entropy', -3.1256132), ('temperature', 0.0546732172369957)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:220000 episode:1690 last_R: 680.8871526925572 average_R:736.729778128777\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 217.33957), ('average_q2', 217.57335), ('average_q_func1_loss', 5.830561314821243), ('average_q_func2_loss', 5.649180583953857), ('n_updates', 210001), ('average_entropy', -2.9150083), ('temperature', 0.05527123808860779)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 199 R: 697.2334294506896\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 200 R: 701.947760415299\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 199 R: 698.1151993064115\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 199 R: 697.8164844095348\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 200 R: 701.448232563313\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 199 R: 697.8148673382203\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 201 R: 703.3962446627467\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 201 R: 703.8968121087117\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 198 R: 695.0571931613224\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 199 R: 697.4179463847723\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:221000 episode:1696 last_R: 673.9442300580606 average_R:732.934808755466\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 217.33612), ('average_q2', 217.22336), ('average_q_func1_loss', 3.023381439447403), ('average_q_func2_loss', 2.8802281332015993), ('n_updates', 211001), ('average_entropy', -2.9198837), ('temperature', 0.05494999513030052)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:222000 episode:1701 last_R: 672.342854747323 average_R:730.063483829232\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 214.54411), ('average_q2', 214.6087), ('average_q_func1_loss', 2.957911118268967), ('average_q_func2_loss', 2.4854801964759825), ('n_updates', 212001), ('average_entropy', -3.2404234), ('temperature', 0.055388446897268295)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:223000 episode:1706 last_R: 697.4352778266036 average_R:728.324076554998\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 218.87024), ('average_q2', 218.67078), ('average_q_func1_loss', 3.0469374561309817), ('average_q_func2_loss', 2.7607303953170774), ('n_updates', 213001), ('average_entropy', -3.130036), ('temperature', 0.054565537720918655)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:224000 episode:1711 last_R: 716.9341681075705 average_R:726.3953041940662\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 216.70488), ('average_q2', 216.64015), ('average_q_func1_loss', 3.1932317960262298), ('average_q_func2_loss', 3.4029120165109634), ('n_updates', 214001), ('average_entropy', -2.974683), ('temperature', 0.055768780410289764)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:225000 episode:1716 last_R: 705.0432552559387 average_R:728.3859931097427\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 215.18161), ('average_q2', 214.87482), ('average_q_func1_loss', 3.182787786722183), ('average_q_func2_loss', 2.5801339983940124), ('n_updates', 215001), ('average_entropy', -3.1164463), ('temperature', 0.0543341338634491)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 209 R: 742.9035292813746\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 169 R: 534.1687266869267\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 210 R: 747.3247655989242\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 208 R: 739.7822534764207\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 210 R: 743.01770850785\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 170 R: 540.5653318841004\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 170 R: 536.4073740917794\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 208 R: 740.6823816896623\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 170 R: 536.1988552538226\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 170 R: 540.0896692514331\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:226000 episode:1720 last_R: 693.7303805504745 average_R:733.8638182173521\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 217.509), ('average_q2', 217.54791), ('average_q_func1_loss', 3.5696671628952026), ('average_q_func2_loss', 3.230775890350342), ('n_updates', 216001), ('average_entropy', -2.840886), ('temperature', 0.05558675527572632)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:227000 episode:1725 last_R: 762.4123124986788 average_R:733.1534707592858\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 212.9403), ('average_q2', 212.88165), ('average_q_func1_loss', 2.996783502101898), ('average_q_func2_loss', 3.107948291301727), ('n_updates', 217001), ('average_entropy', -3.003145), ('temperature', 0.055168986320495605)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:228000 episode:1730 last_R: 747.7666430543599 average_R:732.2503338064223\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 216.26028), ('average_q2', 216.08057), ('average_q_func1_loss', 3.408356646299362), ('average_q_func2_loss', 2.9887498235702514), ('n_updates', 218001), ('average_entropy', -3.037089), ('temperature', 0.05526267737150192)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:229000 episode:1735 last_R: 741.9938490928029 average_R:731.9212914222845\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 220.11263), ('average_q2', 220.29854), ('average_q_func1_loss', 2.994108899831772), ('average_q_func2_loss', 3.226519702672958), ('n_updates', 219001), ('average_entropy', -2.889625), ('temperature', 0.05628231540322304)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:230000 episode:1740 last_R: 740.8147566629746 average_R:728.6030393721608\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 219.89684), ('average_q2', 219.94672), ('average_q_func1_loss', 3.683812770843506), ('average_q_func2_loss', 3.4783797490596773), ('n_updates', 220001), ('average_entropy', -3.054787), ('temperature', 0.054633304476737976)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 216 R: 744.6758441858417\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 215 R: 740.8023632790782\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 215 R: 742.7624011119724\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 215 R: 746.2089520671808\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 215 R: 740.7266262325428\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 215 R: 742.5845081621934\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 216 R: 744.9590529205367\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 214 R: 741.6616534150619\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 215 R: 747.7211269198954\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 215 R: 743.2778796302371\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:231000 episode:1743 last_R: 733.8402521935109 average_R:730.1484679136664\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 223.19617), ('average_q2', 223.1175), ('average_q_func1_loss', 2.9825574445724485), ('average_q_func2_loss', 3.0427521157264708), ('n_updates', 221001), ('average_entropy', -2.954402), ('temperature', 0.05391235649585724)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:232000 episode:1748 last_R: 695.4939568882277 average_R:728.854535991958\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 219.27893), ('average_q2', 219.40147), ('average_q_func1_loss', 3.311875283718109), ('average_q_func2_loss', 2.957038896083832), ('n_updates', 222001), ('average_entropy', -3.096077), ('temperature', 0.055108409374952316)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:233000 episode:1753 last_R: 746.8023609024112 average_R:725.5156960019258\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 220.5326), ('average_q2', 220.54437), ('average_q_func1_loss', 3.3455708122253416), ('average_q_func2_loss', 2.925594253540039), ('n_updates', 223001), ('average_entropy', -2.8721573), ('temperature', 0.05460957810282707)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:234000 episode:1757 last_R: 728.2067760371448 average_R:721.7397133890623\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 218.32272), ('average_q2', 218.42908), ('average_q_func1_loss', 3.1804102385044097), ('average_q_func2_loss', 2.927315891981125), ('n_updates', 224001), ('average_entropy', -3.0557814), ('temperature', 0.05401651933789253)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:235000 episode:1762 last_R: 767.1135399667978 average_R:717.06021804955\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 220.79193), ('average_q2', 220.7499), ('average_q_func1_loss', 3.6953804910182955), ('average_q_func2_loss', 3.943308128118515), ('n_updates', 225001), ('average_entropy', -3.366552), ('temperature', 0.054119694977998734)]\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 251 R: 752.9520090045199\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 248 R: 740.3717099218229\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 255 R: 764.5582137639502\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 253 R: 759.3914707710676\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 253 R: 758.2570091128114\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 246 R: 736.1585454679398\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 254 R: 763.2038553127021\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 258 R: 777.7362226204818\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 259 R: 777.1574741157838\n","INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 257 R: 773.543206113894\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:236000 episode:1767 last_R: 685.7777015160347 average_R:716.922393479746\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 224.97913), ('average_q2', 224.93097), ('average_q_func1_loss', 3.845926390886307), ('average_q_func2_loss', 4.011072021722794), ('n_updates', 226001), ('average_entropy', -2.8651547), ('temperature', 0.053241170942783356)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:237000 episode:1772 last_R: 736.0965783738491 average_R:715.9387706804644\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 216.00972), ('average_q2', 216.10341), ('average_q_func1_loss', 3.39095134973526), ('average_q_func2_loss', 2.8732021403312684), ('n_updates', 227001), ('average_entropy', -3.2094362), ('temperature', 0.05198975279927254)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:238000 episode:1776 last_R: 730.5851086448464 average_R:716.081772811812\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 217.70134), ('average_q2', 217.75623), ('average_q_func1_loss', 4.122618842124939), ('average_q_func2_loss', 3.7317458361387255), ('n_updates', 228001), ('average_entropy', -3.1188076), ('temperature', 0.053861480206251144)]\n","INFO:pfrl.experiments.train_agent_batch:outdir:logs/trying_something_out step:239000 episode:1780 last_R: 784.0565862146314 average_R:717.0448208819605\n","INFO:pfrl.experiments.train_agent_batch:statistics: [('average_q1', 219.83916), ('average_q2', 219.87885), ('average_q_func1_loss', 2.709577738046646), ('average_q_func2_loss', 2.6000146436691285), ('n_updates', 229001), ('average_entropy', -2.9654748), ('temperature', 0.05309689790010452)]\n"]}]},{"cell_type":"code","source":["agent.save(\"sac-default\")"],"metadata":{"id":"_fNkGRJQeGQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"N9h11tqPyKvr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## online evaluation"],"metadata":{"id":"yIr16rRYfCW_"}},{"cell_type":"code","source":["agent.load(\"sac-default\")"],"metadata":{"id":"xKZ7qTYofjtR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_stats = experiments.eval_performance(\n","    env=make_batch_env(test=True),\n","    agent=agent,\n","    n_steps=None,\n","    n_episodes=10,\n","    max_episode_len=timestep_limit,\n",")\n","print(\n","    \"n_runs: {} mean: {} median: {} stdev {}\".format(\n","        10,\n","        eval_stats[\"mean\"],\n","        eval_stats[\"median\"],\n","        eval_stats[\"stdev\"],\n","    )\n",")"],"metadata":{"id":"ACJ1HnCbcq8k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3JsAD9BiyLng"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## render (TODO)"],"metadata":{"id":"vvnsN-_Kkkko"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from IPython import display as ipythondisplay"],"metadata":{"id":"aEE9zpZokzpX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_env_for_render():\n","    env = gym.make(env_name, render_mode='rgb_array')\n","    env = gym.wrappers.RecordEpisodeStatistics(env)\n","    env.seed(seed)\n","    env.action_space.seed(seed)\n","    env.observation_space.seed(seed)\n","    return env"],"metadata":{"id":"zWMdWGs9lACY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env_ = make_env_for_render()"],"metadata":{"id":"gV4A2mvBlLbf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.axis('off')\n","done = False\n","obs = env_.reset()\n","\n","i = 0\n","while not done:\n","    i += 1\n","    if i % 20 == 0:\n","        ipythondisplay.clear_output(wait=True)\n","        # print(\"At timestep = \", i)\n","        screen = env_.render()\n","        plt.imshow(screen[0])\n","        ipythondisplay.display(plt.gcf())\n","\n","    action = agent.act(obs)\n","    obs, reward, done, info = env_.step(action)\n","\n","    if done:\n","        break"],"metadata":{"id":"cwlgrtDRkmy2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pReZTYC1yMSg"},"execution_count":null,"outputs":[]}]}